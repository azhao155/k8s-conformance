I1125 06:45:24.441894      23 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-894937859
I1125 06:45:24.441924      23 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I1125 06:45:24.442015      23 e2e.go:129] Starting e2e run "7ef5a412-9337-4820-96c8-33bcf80b8254" on Ginkgo node 1
{"msg":"Test Suite starting","total":303,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1606286722 - Will randomize all specs
Will run 303 of 5234 specs

Nov 25 06:45:24.500: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 06:45:24.502: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Nov 25 06:45:24.513: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 25 06:45:24.537: INFO: 10 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 25 06:45:24.537: INFO: expected 1 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Nov 25 06:45:24.537: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 25 06:45:24.544: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kindnet' (0 seconds elapsed)
Nov 25 06:45:24.544: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Nov 25 06:45:24.544: INFO: e2e test version: v1.19.4
Nov 25 06:45:24.545: INFO: kube-apiserver version: v1.19.4
Nov 25 06:45:24.545: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 06:45:24.549: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:45:24.549: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename replicaset
Nov 25 06:45:24.586: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 06:45:24.588: INFO: Creating ReplicaSet my-hostname-basic-6ad14987-61ab-4b59-b7e0-1cb5fd9de508
Nov 25 06:45:24.593: INFO: Pod name my-hostname-basic-6ad14987-61ab-4b59-b7e0-1cb5fd9de508: Found 0 pods out of 1
Nov 25 06:45:29.596: INFO: Pod name my-hostname-basic-6ad14987-61ab-4b59-b7e0-1cb5fd9de508: Found 1 pods out of 1
Nov 25 06:45:29.596: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6ad14987-61ab-4b59-b7e0-1cb5fd9de508" is running
Nov 25 06:45:31.601: INFO: Pod "my-hostname-basic-6ad14987-61ab-4b59-b7e0-1cb5fd9de508-dxhkp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-25 06:45:24 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-25 06:45:24 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-6ad14987-61ab-4b59-b7e0-1cb5fd9de508]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-25 06:45:24 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-6ad14987-61ab-4b59-b7e0-1cb5fd9de508]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-25 06:45:24 +0000 UTC Reason: Message:}])
Nov 25 06:45:31.601: INFO: Trying to dial the pod
Nov 25 06:45:36.609: INFO: Controller my-hostname-basic-6ad14987-61ab-4b59-b7e0-1cb5fd9de508: Got expected result from replica 1 [my-hostname-basic-6ad14987-61ab-4b59-b7e0-1cb5fd9de508-dxhkp]: "my-hostname-basic-6ad14987-61ab-4b59-b7e0-1cb5fd9de508-dxhkp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:45:36.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6176" for this suite.

• [SLOW TEST:12.065 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":303,"completed":1,"skipped":26,"failed":0}
SSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:45:36.614: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-1644
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1644
STEP: Deleting pre-stop pod
Nov 25 06:45:47.683: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:45:47.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1644" for this suite.

• [SLOW TEST:11.081 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":303,"completed":2,"skipped":33,"failed":0}
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:45:47.695: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2411
Nov 25 06:45:49.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2411 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 25 06:45:50.982: INFO: rc: 7
Nov 25 06:45:50.986: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 25 06:45:50.988: INFO: Pod kube-proxy-mode-detector still exists
Nov 25 06:45:52.988: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 25 06:45:52.991: INFO: Pod kube-proxy-mode-detector still exists
Nov 25 06:45:54.988: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 25 06:45:54.991: INFO: Pod kube-proxy-mode-detector no longer exists
Nov 25 06:45:54.991: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2411 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-2411
STEP: creating replication controller affinity-clusterip-timeout in namespace services-2411
I1125 06:45:55.028032      23 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-2411, replica count: 3
I1125 06:45:58.109207      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1125 06:46:01.109413      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 25 06:46:01.113: INFO: Creating new exec pod
Nov 25 06:46:04.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2411 execpod-affinitylctkt -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Nov 25 06:46:04.292: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Nov 25 06:46:04.292: INFO: stdout: ""
Nov 25 06:46:04.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2411 execpod-affinitylctkt -- /bin/sh -x -c nc -zv -t -w 2 10.107.77.4 80'
Nov 25 06:46:04.433: INFO: stderr: "+ nc -zv -t -w 2 10.107.77.4 80\nConnection to 10.107.77.4 80 port [tcp/http] succeeded!\n"
Nov 25 06:46:04.433: INFO: stdout: ""
Nov 25 06:46:04.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2411 execpod-affinitylctkt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.77.4:80/ ; done'
Nov 25 06:46:04.656: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n"
Nov 25 06:46:04.656: INFO: stdout: "\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8\naffinity-clusterip-timeout-ccfq8"
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Received response from host: affinity-clusterip-timeout-ccfq8
Nov 25 06:46:04.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2411 execpod-affinitylctkt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.107.77.4:80/'
Nov 25 06:46:04.817: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n"
Nov 25 06:46:04.817: INFO: stdout: "affinity-clusterip-timeout-ccfq8"
Nov 25 06:46:19.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2411 execpod-affinitylctkt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.107.77.4:80/'
Nov 25 06:46:19.962: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n"
Nov 25 06:46:19.962: INFO: stdout: "affinity-clusterip-timeout-ccfq8"
Nov 25 06:46:34.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2411 execpod-affinitylctkt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.107.77.4:80/'
Nov 25 06:46:35.117: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.107.77.4:80/\n"
Nov 25 06:46:35.117: INFO: stdout: "affinity-clusterip-timeout-hn8bc"
Nov 25 06:46:35.117: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-2411, will wait for the garbage collector to delete the pods
Nov 25 06:46:35.183: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 4.816686ms
Nov 25 06:46:35.583: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 400.307322ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:46:50.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2411" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:63.031 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":3,"skipped":33,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:46:50.726: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:163
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:46:50.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8538" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":303,"completed":4,"skipped":37,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:46:50.788: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-c2f0e0a2-5af3-4cc4-a63d-e0f194c21787
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:46:50.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1191" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":303,"completed":5,"skipped":41,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:46:50.878: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Nov 25 06:46:50.930: INFO: Waiting up to 5m0s for pod "var-expansion-fffb326a-1571-46d0-bd90-c91dea8cf266" in namespace "var-expansion-8722" to be "Succeeded or Failed"
Nov 25 06:46:50.938: INFO: Pod "var-expansion-fffb326a-1571-46d0-bd90-c91dea8cf266": Phase="Pending", Reason="", readiness=false. Elapsed: 8.699439ms
Nov 25 06:46:52.941: INFO: Pod "var-expansion-fffb326a-1571-46d0-bd90-c91dea8cf266": Phase="Running", Reason="", readiness=true. Elapsed: 2.011425901s
Nov 25 06:46:54.944: INFO: Pod "var-expansion-fffb326a-1571-46d0-bd90-c91dea8cf266": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014393088s
STEP: Saw pod success
Nov 25 06:46:54.944: INFO: Pod "var-expansion-fffb326a-1571-46d0-bd90-c91dea8cf266" satisfied condition "Succeeded or Failed"
Nov 25 06:46:54.946: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-fffb326a-1571-46d0-bd90-c91dea8cf266 container dapi-container: <nil>
STEP: delete the pod
Nov 25 06:46:54.969: INFO: Waiting for pod var-expansion-fffb326a-1571-46d0-bd90-c91dea8cf266 to disappear
Nov 25 06:46:54.970: INFO: Pod var-expansion-fffb326a-1571-46d0-bd90-c91dea8cf266 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:46:54.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8722" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":303,"completed":6,"skipped":56,"failed":0}
SSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:46:54.975: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 06:46:55.003: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-65d2f41c-c1f2-4527-b999-811e25ee8ebd" in namespace "security-context-test-6037" to be "Succeeded or Failed"
Nov 25 06:46:55.005: INFO: Pod "busybox-readonly-false-65d2f41c-c1f2-4527-b999-811e25ee8ebd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.490347ms
Nov 25 06:46:57.008: INFO: Pod "busybox-readonly-false-65d2f41c-c1f2-4527-b999-811e25ee8ebd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004619428s
Nov 25 06:46:57.008: INFO: Pod "busybox-readonly-false-65d2f41c-c1f2-4527-b999-811e25ee8ebd" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:46:57.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6037" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":303,"completed":7,"skipped":61,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:46:57.014: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Nov 25 06:46:57.036: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Nov 25 06:46:57.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 create -f - --namespace=kubectl-3538'
Nov 25 06:46:57.315: INFO: stderr: ""
Nov 25 06:46:57.315: INFO: stdout: "service/agnhost-replica created\n"
Nov 25 06:46:57.315: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Nov 25 06:46:57.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 create -f - --namespace=kubectl-3538'
Nov 25 06:46:57.605: INFO: stderr: ""
Nov 25 06:46:57.605: INFO: stdout: "service/agnhost-primary created\n"
Nov 25 06:46:57.605: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 25 06:46:57.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 create -f - --namespace=kubectl-3538'
Nov 25 06:46:57.892: INFO: stderr: ""
Nov 25 06:46:57.892: INFO: stdout: "service/frontend created\n"
Nov 25 06:46:57.892: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Nov 25 06:46:57.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 create -f - --namespace=kubectl-3538'
Nov 25 06:46:58.100: INFO: stderr: ""
Nov 25 06:46:58.101: INFO: stdout: "deployment.apps/frontend created\n"
Nov 25 06:46:58.101: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 25 06:46:58.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 create -f - --namespace=kubectl-3538'
Nov 25 06:46:58.410: INFO: stderr: ""
Nov 25 06:46:58.410: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Nov 25 06:46:58.410: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 25 06:46:58.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 create -f - --namespace=kubectl-3538'
Nov 25 06:46:58.687: INFO: stderr: ""
Nov 25 06:46:58.687: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Nov 25 06:46:58.687: INFO: Waiting for all frontend pods to be Running.
Nov 25 06:47:03.739: INFO: Waiting for frontend to serve content.
Nov 25 06:47:03.746: INFO: Trying to add a new entry to the guestbook.
Nov 25 06:47:03.756: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Nov 25 06:47:03.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 delete --grace-period=0 --force -f - --namespace=kubectl-3538'
Nov 25 06:47:03.864: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 25 06:47:03.864: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Nov 25 06:47:03.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 delete --grace-period=0 --force -f - --namespace=kubectl-3538'
Nov 25 06:47:04.012: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 25 06:47:04.012: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov 25 06:47:04.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 delete --grace-period=0 --force -f - --namespace=kubectl-3538'
Nov 25 06:47:04.119: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 25 06:47:04.119: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 25 06:47:04.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 delete --grace-period=0 --force -f - --namespace=kubectl-3538'
Nov 25 06:47:04.192: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 25 06:47:04.192: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 25 06:47:04.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 delete --grace-period=0 --force -f - --namespace=kubectl-3538'
Nov 25 06:47:04.362: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 25 06:47:04.362: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov 25 06:47:04.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 delete --grace-period=0 --force -f - --namespace=kubectl-3538'
Nov 25 06:47:04.439: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 25 06:47:04.439: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:47:04.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3538" for this suite.

• [SLOW TEST:7.434 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":303,"completed":8,"skipped":86,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:47:04.448: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 06:47:04.527: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a584c355-4755-4adc-841d-8af712330ab7" in namespace "projected-3102" to be "Succeeded or Failed"
Nov 25 06:47:04.531: INFO: Pod "downwardapi-volume-a584c355-4755-4adc-841d-8af712330ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.540748ms
Nov 25 06:47:06.534: INFO: Pod "downwardapi-volume-a584c355-4755-4adc-841d-8af712330ab7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007491563s
STEP: Saw pod success
Nov 25 06:47:06.534: INFO: Pod "downwardapi-volume-a584c355-4755-4adc-841d-8af712330ab7" satisfied condition "Succeeded or Failed"
Nov 25 06:47:06.536: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-a584c355-4755-4adc-841d-8af712330ab7 container client-container: <nil>
STEP: delete the pod
Nov 25 06:47:06.552: INFO: Waiting for pod downwardapi-volume-a584c355-4755-4adc-841d-8af712330ab7 to disappear
Nov 25 06:47:06.554: INFO: Pod downwardapi-volume-a584c355-4755-4adc-841d-8af712330ab7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:47:06.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3102" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":9,"skipped":115,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:47:06.560: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-tvs7
STEP: Creating a pod to test atomic-volume-subpath
Nov 25 06:47:06.602: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-tvs7" in namespace "subpath-1920" to be "Succeeded or Failed"
Nov 25 06:47:06.604: INFO: Pod "pod-subpath-test-secret-tvs7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.812088ms
Nov 25 06:47:08.607: INFO: Pod "pod-subpath-test-secret-tvs7": Phase="Running", Reason="", readiness=true. Elapsed: 2.004860423s
Nov 25 06:47:10.610: INFO: Pod "pod-subpath-test-secret-tvs7": Phase="Running", Reason="", readiness=true. Elapsed: 4.0078304s
Nov 25 06:47:12.614: INFO: Pod "pod-subpath-test-secret-tvs7": Phase="Running", Reason="", readiness=true. Elapsed: 6.011623576s
Nov 25 06:47:14.617: INFO: Pod "pod-subpath-test-secret-tvs7": Phase="Running", Reason="", readiness=true. Elapsed: 8.015053978s
Nov 25 06:47:16.620: INFO: Pod "pod-subpath-test-secret-tvs7": Phase="Running", Reason="", readiness=true. Elapsed: 10.018344572s
Nov 25 06:47:18.623: INFO: Pod "pod-subpath-test-secret-tvs7": Phase="Running", Reason="", readiness=true. Elapsed: 12.021303733s
Nov 25 06:47:20.626: INFO: Pod "pod-subpath-test-secret-tvs7": Phase="Running", Reason="", readiness=true. Elapsed: 14.024203487s
Nov 25 06:47:22.629: INFO: Pod "pod-subpath-test-secret-tvs7": Phase="Running", Reason="", readiness=true. Elapsed: 16.027085617s
Nov 25 06:47:24.632: INFO: Pod "pod-subpath-test-secret-tvs7": Phase="Running", Reason="", readiness=true. Elapsed: 18.030171149s
Nov 25 06:47:26.635: INFO: Pod "pod-subpath-test-secret-tvs7": Phase="Running", Reason="", readiness=true. Elapsed: 20.033084196s
Nov 25 06:47:28.638: INFO: Pod "pod-subpath-test-secret-tvs7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.035907587s
STEP: Saw pod success
Nov 25 06:47:28.638: INFO: Pod "pod-subpath-test-secret-tvs7" satisfied condition "Succeeded or Failed"
Nov 25 06:47:28.640: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-secret-tvs7 container test-container-subpath-secret-tvs7: <nil>
STEP: delete the pod
Nov 25 06:47:28.650: INFO: Waiting for pod pod-subpath-test-secret-tvs7 to disappear
Nov 25 06:47:28.651: INFO: Pod pod-subpath-test-secret-tvs7 no longer exists
STEP: Deleting pod pod-subpath-test-secret-tvs7
Nov 25 06:47:28.651: INFO: Deleting pod "pod-subpath-test-secret-tvs7" in namespace "subpath-1920"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:47:28.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1920" for this suite.

• [SLOW TEST:22.098 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":303,"completed":10,"skipped":128,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:47:28.658: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 06:47:28.688: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov 25 06:47:28.692: INFO: Number of nodes with available pods: 0
Nov 25 06:47:28.692: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Nov 25 06:47:28.705: INFO: Number of nodes with available pods: 0
Nov 25 06:47:28.705: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:29.708: INFO: Number of nodes with available pods: 0
Nov 25 06:47:29.708: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:30.708: INFO: Number of nodes with available pods: 0
Nov 25 06:47:30.708: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:31.707: INFO: Number of nodes with available pods: 0
Nov 25 06:47:31.707: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:32.708: INFO: Number of nodes with available pods: 0
Nov 25 06:47:32.708: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:33.736: INFO: Number of nodes with available pods: 0
Nov 25 06:47:33.736: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:34.708: INFO: Number of nodes with available pods: 0
Nov 25 06:47:34.708: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:35.707: INFO: Number of nodes with available pods: 0
Nov 25 06:47:35.707: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:36.708: INFO: Number of nodes with available pods: 1
Nov 25 06:47:36.708: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov 25 06:47:36.720: INFO: Number of nodes with available pods: 1
Nov 25 06:47:36.720: INFO: Number of running nodes: 0, number of available pods: 1
Nov 25 06:47:37.723: INFO: Number of nodes with available pods: 0
Nov 25 06:47:37.723: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov 25 06:47:37.730: INFO: Number of nodes with available pods: 0
Nov 25 06:47:37.730: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:38.733: INFO: Number of nodes with available pods: 0
Nov 25 06:47:38.733: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:39.733: INFO: Number of nodes with available pods: 0
Nov 25 06:47:39.733: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:40.733: INFO: Number of nodes with available pods: 0
Nov 25 06:47:40.733: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:41.732: INFO: Number of nodes with available pods: 0
Nov 25 06:47:41.732: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:42.733: INFO: Number of nodes with available pods: 0
Nov 25 06:47:42.733: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:43.733: INFO: Number of nodes with available pods: 0
Nov 25 06:47:43.733: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:44.733: INFO: Number of nodes with available pods: 0
Nov 25 06:47:44.733: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:45.732: INFO: Number of nodes with available pods: 0
Nov 25 06:47:45.732: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:46.733: INFO: Number of nodes with available pods: 0
Nov 25 06:47:46.733: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:47.733: INFO: Number of nodes with available pods: 0
Nov 25 06:47:47.733: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:48.733: INFO: Number of nodes with available pods: 0
Nov 25 06:47:48.733: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:49.733: INFO: Number of nodes with available pods: 0
Nov 25 06:47:49.733: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:50.733: INFO: Number of nodes with available pods: 0
Nov 25 06:47:50.733: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:51.732: INFO: Number of nodes with available pods: 0
Nov 25 06:47:51.732: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 06:47:52.733: INFO: Number of nodes with available pods: 1
Nov 25 06:47:52.733: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1295, will wait for the garbage collector to delete the pods
Nov 25 06:47:52.793: INFO: Deleting DaemonSet.extensions daemon-set took: 4.544782ms
Nov 25 06:47:53.193: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.212028ms
Nov 25 06:48:00.695: INFO: Number of nodes with available pods: 0
Nov 25 06:48:00.695: INFO: Number of running nodes: 0, number of available pods: 0
Nov 25 06:48:00.699: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1295/daemonsets","resourceVersion":"1492"},"items":null}

Nov 25 06:48:00.701: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1295/pods","resourceVersion":"1492"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:48:00.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1295" for this suite.

• [SLOW TEST:32.067 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":303,"completed":11,"skipped":140,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:48:00.725: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Nov 25 06:48:00.748: INFO: Waiting up to 5m0s for pod "pod-21ea93e6-e8cf-46de-bcdd-79ce23a269f6" in namespace "emptydir-3590" to be "Succeeded or Failed"
Nov 25 06:48:00.753: INFO: Pod "pod-21ea93e6-e8cf-46de-bcdd-79ce23a269f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.321089ms
Nov 25 06:48:02.756: INFO: Pod "pod-21ea93e6-e8cf-46de-bcdd-79ce23a269f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007676359s
STEP: Saw pod success
Nov 25 06:48:02.756: INFO: Pod "pod-21ea93e6-e8cf-46de-bcdd-79ce23a269f6" satisfied condition "Succeeded or Failed"
Nov 25 06:48:02.758: INFO: Trying to get logs from node k8sconformance-m02 pod pod-21ea93e6-e8cf-46de-bcdd-79ce23a269f6 container test-container: <nil>
STEP: delete the pod
Nov 25 06:48:02.775: INFO: Waiting for pod pod-21ea93e6-e8cf-46de-bcdd-79ce23a269f6 to disappear
Nov 25 06:48:02.776: INFO: Pod pod-21ea93e6-e8cf-46de-bcdd-79ce23a269f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:48:02.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3590" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":12,"skipped":149,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:48:02.781: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 06:48:02.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cfeef8bd-2e90-4f14-b16c-06a07df4aa9d" in namespace "downward-api-2898" to be "Succeeded or Failed"
Nov 25 06:48:02.812: INFO: Pod "downwardapi-volume-cfeef8bd-2e90-4f14-b16c-06a07df4aa9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.824189ms
Nov 25 06:48:04.815: INFO: Pod "downwardapi-volume-cfeef8bd-2e90-4f14-b16c-06a07df4aa9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007959273s
STEP: Saw pod success
Nov 25 06:48:04.815: INFO: Pod "downwardapi-volume-cfeef8bd-2e90-4f14-b16c-06a07df4aa9d" satisfied condition "Succeeded or Failed"
Nov 25 06:48:04.817: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-cfeef8bd-2e90-4f14-b16c-06a07df4aa9d container client-container: <nil>
STEP: delete the pod
Nov 25 06:48:04.832: INFO: Waiting for pod downwardapi-volume-cfeef8bd-2e90-4f14-b16c-06a07df4aa9d to disappear
Nov 25 06:48:04.839: INFO: Pod downwardapi-volume-cfeef8bd-2e90-4f14-b16c-06a07df4aa9d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:48:04.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2898" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":303,"completed":13,"skipped":152,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:48:04.848: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-fc67e47e-7357-43e4-9572-b2d364f5efb1
STEP: Creating a pod to test consume secrets
Nov 25 06:48:04.873: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9bfc77e0-d5a9-4acf-a144-ac3b4a290aa8" in namespace "projected-6107" to be "Succeeded or Failed"
Nov 25 06:48:04.877: INFO: Pod "pod-projected-secrets-9bfc77e0-d5a9-4acf-a144-ac3b4a290aa8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.988513ms
Nov 25 06:48:06.881: INFO: Pod "pod-projected-secrets-9bfc77e0-d5a9-4acf-a144-ac3b4a290aa8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007189928s
STEP: Saw pod success
Nov 25 06:48:06.881: INFO: Pod "pod-projected-secrets-9bfc77e0-d5a9-4acf-a144-ac3b4a290aa8" satisfied condition "Succeeded or Failed"
Nov 25 06:48:06.882: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-9bfc77e0-d5a9-4acf-a144-ac3b4a290aa8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 25 06:48:06.898: INFO: Waiting for pod pod-projected-secrets-9bfc77e0-d5a9-4acf-a144-ac3b4a290aa8 to disappear
Nov 25 06:48:06.900: INFO: Pod pod-projected-secrets-9bfc77e0-d5a9-4acf-a144-ac3b4a290aa8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:48:06.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6107" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":14,"skipped":153,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:48:06.906: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Nov 25 06:48:06.931: INFO: Waiting up to 5m0s for pod "client-containers-aeec19fd-6b69-45b0-8c91-afdeeac81d0d" in namespace "containers-4831" to be "Succeeded or Failed"
Nov 25 06:48:06.933: INFO: Pod "client-containers-aeec19fd-6b69-45b0-8c91-afdeeac81d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.853568ms
Nov 25 06:48:08.936: INFO: Pod "client-containers-aeec19fd-6b69-45b0-8c91-afdeeac81d0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004730117s
STEP: Saw pod success
Nov 25 06:48:08.936: INFO: Pod "client-containers-aeec19fd-6b69-45b0-8c91-afdeeac81d0d" satisfied condition "Succeeded or Failed"
Nov 25 06:48:08.938: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-aeec19fd-6b69-45b0-8c91-afdeeac81d0d container test-container: <nil>
STEP: delete the pod
Nov 25 06:48:08.992: INFO: Waiting for pod client-containers-aeec19fd-6b69-45b0-8c91-afdeeac81d0d to disappear
Nov 25 06:48:08.993: INFO: Pod client-containers-aeec19fd-6b69-45b0-8c91-afdeeac81d0d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:48:08.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4831" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":303,"completed":15,"skipped":161,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:48:08.998: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 25 06:48:13.045: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 25 06:48:13.100: INFO: Pod pod-with-poststart-http-hook still exists
Nov 25 06:48:15.100: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 25 06:48:15.103: INFO: Pod pod-with-poststart-http-hook still exists
Nov 25 06:48:17.100: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 25 06:48:17.103: INFO: Pod pod-with-poststart-http-hook still exists
Nov 25 06:48:19.100: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 25 06:48:19.103: INFO: Pod pod-with-poststart-http-hook still exists
Nov 25 06:48:21.100: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 25 06:48:21.103: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:48:21.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4446" for this suite.

• [SLOW TEST:12.112 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":303,"completed":16,"skipped":171,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:48:21.111: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Nov 25 06:48:21.132: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-894937859 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:48:21.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5508" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":303,"completed":17,"skipped":174,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:48:21.209: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 25 06:48:21.233: INFO: Waiting up to 5m0s for pod "pod-5e4943fc-e62f-45a4-a4d9-e43bbce7224e" in namespace "emptydir-8647" to be "Succeeded or Failed"
Nov 25 06:48:21.235: INFO: Pod "pod-5e4943fc-e62f-45a4-a4d9-e43bbce7224e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.822988ms
Nov 25 06:48:23.237: INFO: Pod "pod-5e4943fc-e62f-45a4-a4d9-e43bbce7224e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004342869s
Nov 25 06:48:25.240: INFO: Pod "pod-5e4943fc-e62f-45a4-a4d9-e43bbce7224e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007056626s
STEP: Saw pod success
Nov 25 06:48:25.240: INFO: Pod "pod-5e4943fc-e62f-45a4-a4d9-e43bbce7224e" satisfied condition "Succeeded or Failed"
Nov 25 06:48:25.242: INFO: Trying to get logs from node k8sconformance-m02 pod pod-5e4943fc-e62f-45a4-a4d9-e43bbce7224e container test-container: <nil>
STEP: delete the pod
Nov 25 06:48:25.260: INFO: Waiting for pod pod-5e4943fc-e62f-45a4-a4d9-e43bbce7224e to disappear
Nov 25 06:48:25.262: INFO: Pod pod-5e4943fc-e62f-45a4-a4d9-e43bbce7224e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:48:25.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8647" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":18,"skipped":186,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:48:25.272: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 06:48:25.856: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 06:48:28.874: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 06:48:28.877: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4015-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:48:30.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9399" for this suite.
STEP: Destroying namespace "webhook-9399-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":303,"completed":19,"skipped":190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:48:30.102: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Nov 25 06:48:30.176: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Nov 25 06:48:31.401: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov 25 06:48:33.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741883711, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741883711, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741883711, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741883711, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 25 06:48:35.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741883711, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741883711, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741883711, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741883711, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 25 06:48:37.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741883711, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741883711, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741883711, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741883711, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 25 06:48:40.187: INFO: Waited 718.169166ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:48:40.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2648" for this suite.

• [SLOW TEST:10.762 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":303,"completed":20,"skipped":228,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:48:40.865: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 25 06:48:40.926: INFO: Waiting up to 5m0s for pod "pod-dd0fdc15-3dec-432f-a224-5372c47fe504" in namespace "emptydir-6331" to be "Succeeded or Failed"
Nov 25 06:48:40.930: INFO: Pod "pod-dd0fdc15-3dec-432f-a224-5372c47fe504": Phase="Pending", Reason="", readiness=false. Elapsed: 3.723484ms
Nov 25 06:48:42.933: INFO: Pod "pod-dd0fdc15-3dec-432f-a224-5372c47fe504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006639817s
STEP: Saw pod success
Nov 25 06:48:42.933: INFO: Pod "pod-dd0fdc15-3dec-432f-a224-5372c47fe504" satisfied condition "Succeeded or Failed"
Nov 25 06:48:42.935: INFO: Trying to get logs from node k8sconformance-m02 pod pod-dd0fdc15-3dec-432f-a224-5372c47fe504 container test-container: <nil>
STEP: delete the pod
Nov 25 06:48:42.960: INFO: Waiting for pod pod-dd0fdc15-3dec-432f-a224-5372c47fe504 to disappear
Nov 25 06:48:42.962: INFO: Pod pod-dd0fdc15-3dec-432f-a224-5372c47fe504 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:48:42.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6331" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":21,"skipped":228,"failed":0}
SSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:48:42.967: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:48:43.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4765" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":303,"completed":22,"skipped":231,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:48:43.032: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Nov 25 06:48:43.055: INFO: Waiting up to 5m0s for pod "var-expansion-4e6b0273-4dc6-48f1-b279-cde565e66b31" in namespace "var-expansion-205" to be "Succeeded or Failed"
Nov 25 06:48:43.059: INFO: Pod "var-expansion-4e6b0273-4dc6-48f1-b279-cde565e66b31": Phase="Pending", Reason="", readiness=false. Elapsed: 3.95888ms
Nov 25 06:48:45.063: INFO: Pod "var-expansion-4e6b0273-4dc6-48f1-b279-cde565e66b31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007249155s
STEP: Saw pod success
Nov 25 06:48:45.063: INFO: Pod "var-expansion-4e6b0273-4dc6-48f1-b279-cde565e66b31" satisfied condition "Succeeded or Failed"
Nov 25 06:48:45.065: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-4e6b0273-4dc6-48f1-b279-cde565e66b31 container dapi-container: <nil>
STEP: delete the pod
Nov 25 06:48:45.097: INFO: Waiting for pod var-expansion-4e6b0273-4dc6-48f1-b279-cde565e66b31 to disappear
Nov 25 06:48:45.100: INFO: Pod var-expansion-4e6b0273-4dc6-48f1-b279-cde565e66b31 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:48:45.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-205" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":303,"completed":23,"skipped":243,"failed":0}
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:48:45.106: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Nov 25 06:48:47.144: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9251 PodName:var-expansion-709e1781-e79f-4571-be1a-4fbf2614b061 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 06:48:47.144: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: test for file in mounted path
Nov 25 06:48:47.213: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9251 PodName:var-expansion-709e1781-e79f-4571-be1a-4fbf2614b061 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 06:48:47.214: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: updating the annotation value
Nov 25 06:48:47.783: INFO: Successfully updated pod "var-expansion-709e1781-e79f-4571-be1a-4fbf2614b061"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Nov 25 06:48:47.789: INFO: Deleting pod "var-expansion-709e1781-e79f-4571-be1a-4fbf2614b061" in namespace "var-expansion-9251"
Nov 25 06:48:47.792: INFO: Wait up to 5m0s for pod "var-expansion-709e1781-e79f-4571-be1a-4fbf2614b061" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:49:21.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9251" for this suite.

• [SLOW TEST:36.700 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":303,"completed":24,"skipped":249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:49:21.806: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 06:49:21.829: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 25 06:49:23.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-173 create -f -'
Nov 25 06:49:24.901: INFO: stderr: ""
Nov 25 06:49:24.901: INFO: stdout: "e2e-test-crd-publish-openapi-3722-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 25 06:49:24.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-173 delete e2e-test-crd-publish-openapi-3722-crds test-cr'
Nov 25 06:49:24.983: INFO: stderr: ""
Nov 25 06:49:24.983: INFO: stdout: "e2e-test-crd-publish-openapi-3722-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov 25 06:49:24.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-173 apply -f -'
Nov 25 06:49:25.194: INFO: stderr: ""
Nov 25 06:49:25.194: INFO: stdout: "e2e-test-crd-publish-openapi-3722-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 25 06:49:25.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-173 delete e2e-test-crd-publish-openapi-3722-crds test-cr'
Nov 25 06:49:25.272: INFO: stderr: ""
Nov 25 06:49:25.272: INFO: stdout: "e2e-test-crd-publish-openapi-3722-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 25 06:49:25.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 explain e2e-test-crd-publish-openapi-3722-crds'
Nov 25 06:49:25.446: INFO: stderr: ""
Nov 25 06:49:25.446: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3722-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:49:28.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-173" for this suite.

• [SLOW TEST:6.470 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":303,"completed":25,"skipped":273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:49:28.276: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:49:45.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3036" for this suite.

• [SLOW TEST:17.119 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":303,"completed":26,"skipped":306,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:49:45.395: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:49:45.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-402" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":303,"completed":27,"skipped":310,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:49:45.492: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1125 06:49:55.550274      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 25 06:50:57.562: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:50:57.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2863" for this suite.

• [SLOW TEST:72.077 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":303,"completed":28,"skipped":366,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:50:57.569: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2429.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2429.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2429.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2429.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2429.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2429.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 25 06:51:07.619: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:07.621: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:07.624: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:07.626: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:07.632: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:07.634: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:07.636: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:07.638: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:07.642: INFO: Lookups using dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2429.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2429.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local jessie_udp@dns-test-service-2.dns-2429.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2429.svc.cluster.local]

Nov 25 06:51:12.645: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:12.648: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:12.650: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:12.653: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:12.659: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:12.661: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:12.663: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:12.666: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:12.670: INFO: Lookups using dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2429.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2429.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local jessie_udp@dns-test-service-2.dns-2429.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2429.svc.cluster.local]

Nov 25 06:51:17.645: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:17.648: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:17.650: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:17.653: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:17.659: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:17.661: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:17.663: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:17.665: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:17.669: INFO: Lookups using dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2429.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2429.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local jessie_udp@dns-test-service-2.dns-2429.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2429.svc.cluster.local]

Nov 25 06:51:22.645: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:22.650: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:22.652: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:22.654: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:22.660: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:22.662: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:22.664: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:22.667: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:22.673: INFO: Lookups using dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2429.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2429.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local jessie_udp@dns-test-service-2.dns-2429.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2429.svc.cluster.local]

Nov 25 06:51:27.645: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:27.648: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:27.650: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:27.652: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:27.659: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:27.661: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:27.663: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:27.665: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2429.svc.cluster.local from pod dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d: the server could not find the requested resource (get pods dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d)
Nov 25 06:51:27.669: INFO: Lookups using dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2429.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2429.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2429.svc.cluster.local jessie_udp@dns-test-service-2.dns-2429.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2429.svc.cluster.local]

Nov 25 06:51:32.669: INFO: DNS probes using dns-2429/dns-test-0f4099c5-de5e-4aa9-867f-c7c20f47aa2d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:51:32.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2429" for this suite.

• [SLOW TEST:35.576 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":303,"completed":29,"skipped":378,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:51:33.145: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 06:51:33.491: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84ec6c81-20de-4f94-be22-285a14eae9ce" in namespace "downward-api-5962" to be "Succeeded or Failed"
Nov 25 06:51:33.495: INFO: Pod "downwardapi-volume-84ec6c81-20de-4f94-be22-285a14eae9ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.313172ms
Nov 25 06:51:35.498: INFO: Pod "downwardapi-volume-84ec6c81-20de-4f94-be22-285a14eae9ce": Phase="Running", Reason="", readiness=true. Elapsed: 2.007141413s
Nov 25 06:51:37.501: INFO: Pod "downwardapi-volume-84ec6c81-20de-4f94-be22-285a14eae9ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010577358s
STEP: Saw pod success
Nov 25 06:51:37.501: INFO: Pod "downwardapi-volume-84ec6c81-20de-4f94-be22-285a14eae9ce" satisfied condition "Succeeded or Failed"
Nov 25 06:51:37.503: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-84ec6c81-20de-4f94-be22-285a14eae9ce container client-container: <nil>
STEP: delete the pod
Nov 25 06:51:37.519: INFO: Waiting for pod downwardapi-volume-84ec6c81-20de-4f94-be22-285a14eae9ce to disappear
Nov 25 06:51:37.521: INFO: Pod downwardapi-volume-84ec6c81-20de-4f94-be22-285a14eae9ce no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:51:37.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5962" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":30,"skipped":385,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:51:37.531: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5363.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5363.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5363.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5363.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5363.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5363.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5363.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5363.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5363.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5363.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 85.138.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.138.85_udp@PTR;check="$$(dig +tcp +noall +answer +search 85.138.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.138.85_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5363.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5363.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5363.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5363.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5363.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5363.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5363.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5363.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5363.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5363.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5363.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 85.138.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.138.85_udp@PTR;check="$$(dig +tcp +noall +answer +search 85.138.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.138.85_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 25 06:51:39.645: INFO: Unable to read wheezy_udp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:39.647: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:39.649: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:39.652: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:39.702: INFO: Unable to read jessie_udp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:39.705: INFO: Unable to read jessie_tcp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:39.707: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:39.710: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:39.717: INFO: Unable to read jessie_udp@PodARecord from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:39.721: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:39.726: INFO: Lookups using dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00 failed for: [wheezy_udp@dns-test-service.dns-5363.svc.cluster.local wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local jessie_udp@dns-test-service.dns-5363.svc.cluster.local jessie_tcp@dns-test-service.dns-5363.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Nov 25 06:51:44.729: INFO: Unable to read wheezy_udp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:44.731: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:44.733: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:44.736: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:44.749: INFO: Unable to read jessie_udp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:44.751: INFO: Unable to read jessie_tcp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:44.753: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:44.755: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:44.766: INFO: Lookups using dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00 failed for: [wheezy_udp@dns-test-service.dns-5363.svc.cluster.local wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local jessie_udp@dns-test-service.dns-5363.svc.cluster.local jessie_tcp@dns-test-service.dns-5363.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local]

Nov 25 06:51:49.729: INFO: Unable to read wheezy_udp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:49.732: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:49.734: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:49.736: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:49.751: INFO: Unable to read jessie_udp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:49.753: INFO: Unable to read jessie_tcp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:49.755: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:49.757: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:49.769: INFO: Lookups using dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00 failed for: [wheezy_udp@dns-test-service.dns-5363.svc.cluster.local wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local jessie_udp@dns-test-service.dns-5363.svc.cluster.local jessie_tcp@dns-test-service.dns-5363.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local]

Nov 25 06:51:54.729: INFO: Unable to read wheezy_udp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:54.732: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:54.735: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:54.737: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:54.753: INFO: Unable to read jessie_udp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:54.755: INFO: Unable to read jessie_tcp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:54.757: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:54.759: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:54.772: INFO: Lookups using dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00 failed for: [wheezy_udp@dns-test-service.dns-5363.svc.cluster.local wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local jessie_udp@dns-test-service.dns-5363.svc.cluster.local jessie_tcp@dns-test-service.dns-5363.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local]

Nov 25 06:51:59.729: INFO: Unable to read wheezy_udp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:59.732: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:59.736: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:59.739: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:59.768: INFO: Unable to read jessie_udp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:59.771: INFO: Unable to read jessie_tcp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:59.774: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:59.776: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:51:59.790: INFO: Lookups using dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00 failed for: [wheezy_udp@dns-test-service.dns-5363.svc.cluster.local wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local jessie_udp@dns-test-service.dns-5363.svc.cluster.local jessie_tcp@dns-test-service.dns-5363.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local]

Nov 25 06:52:04.729: INFO: Unable to read wheezy_udp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:52:04.731: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:52:04.734: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:52:04.736: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:52:04.752: INFO: Unable to read jessie_udp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:52:04.754: INFO: Unable to read jessie_tcp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:52:04.756: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:52:04.759: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:52:04.773: INFO: Lookups using dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00 failed for: [wheezy_udp@dns-test-service.dns-5363.svc.cluster.local wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local jessie_udp@dns-test-service.dns-5363.svc.cluster.local jessie_tcp@dns-test-service.dns-5363.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local]

Nov 25 06:52:09.729: INFO: Unable to read wheezy_udp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:52:09.732: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:52:09.735: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:52:09.738: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local from pod dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00: the server could not find the requested resource (get pods dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00)
Nov 25 06:52:09.774: INFO: Lookups using dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00 failed for: [wheezy_udp@dns-test-service.dns-5363.svc.cluster.local wheezy_tcp@dns-test-service.dns-5363.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5363.svc.cluster.local]

Nov 25 06:52:14.772: INFO: DNS probes using dns-5363/dns-test-2a9744fa-1484-4541-92c3-7437c0b38c00 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:52:14.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5363" for this suite.

• [SLOW TEST:37.383 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":303,"completed":31,"skipped":403,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:52:14.916: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-9562/configmap-test-ae6c24d5-0814-4f9b-9bba-d0b8e92fa544
STEP: Creating a pod to test consume configMaps
Nov 25 06:52:14.984: INFO: Waiting up to 5m0s for pod "pod-configmaps-f82ecc87-0a0e-4de5-8bc7-ed4b4d882119" in namespace "configmap-9562" to be "Succeeded or Failed"
Nov 25 06:52:14.986: INFO: Pod "pod-configmaps-f82ecc87-0a0e-4de5-8bc7-ed4b4d882119": Phase="Pending", Reason="", readiness=false. Elapsed: 1.880393ms
Nov 25 06:52:16.989: INFO: Pod "pod-configmaps-f82ecc87-0a0e-4de5-8bc7-ed4b4d882119": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004672654s
STEP: Saw pod success
Nov 25 06:52:16.989: INFO: Pod "pod-configmaps-f82ecc87-0a0e-4de5-8bc7-ed4b4d882119" satisfied condition "Succeeded or Failed"
Nov 25 06:52:16.990: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-f82ecc87-0a0e-4de5-8bc7-ed4b4d882119 container env-test: <nil>
STEP: delete the pod
Nov 25 06:52:17.057: INFO: Waiting for pod pod-configmaps-f82ecc87-0a0e-4de5-8bc7-ed4b4d882119 to disappear
Nov 25 06:52:17.059: INFO: Pod pod-configmaps-f82ecc87-0a0e-4de5-8bc7-ed4b4d882119 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:52:17.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9562" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":303,"completed":32,"skipped":420,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:52:17.064: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:52:33.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8798" for this suite.

• [SLOW TEST:16.187 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":303,"completed":33,"skipped":422,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:52:33.251: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Nov 25 06:52:33.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 create -f - --namespace=kubectl-279'
Nov 25 06:52:33.584: INFO: stderr: ""
Nov 25 06:52:33.584: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 25 06:52:33.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-279'
Nov 25 06:52:33.664: INFO: stderr: ""
Nov 25 06:52:33.664: INFO: stdout: "update-demo-nautilus-g5xjp update-demo-nautilus-x6zl9 "
Nov 25 06:52:33.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-g5xjp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-279'
Nov 25 06:52:33.810: INFO: stderr: ""
Nov 25 06:52:33.810: INFO: stdout: ""
Nov 25 06:52:33.810: INFO: update-demo-nautilus-g5xjp is created but not running
Nov 25 06:52:38.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-279'
Nov 25 06:52:38.882: INFO: stderr: ""
Nov 25 06:52:38.882: INFO: stdout: "update-demo-nautilus-g5xjp update-demo-nautilus-x6zl9 "
Nov 25 06:52:38.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-g5xjp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-279'
Nov 25 06:52:38.952: INFO: stderr: ""
Nov 25 06:52:38.952: INFO: stdout: "true"
Nov 25 06:52:38.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-g5xjp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-279'
Nov 25 06:52:39.021: INFO: stderr: ""
Nov 25 06:52:39.021: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 25 06:52:39.021: INFO: validating pod update-demo-nautilus-g5xjp
Nov 25 06:52:39.024: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 25 06:52:39.024: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 25 06:52:39.024: INFO: update-demo-nautilus-g5xjp is verified up and running
Nov 25 06:52:39.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-x6zl9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-279'
Nov 25 06:52:39.093: INFO: stderr: ""
Nov 25 06:52:39.093: INFO: stdout: "true"
Nov 25 06:52:39.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-x6zl9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-279'
Nov 25 06:52:39.162: INFO: stderr: ""
Nov 25 06:52:39.162: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 25 06:52:39.162: INFO: validating pod update-demo-nautilus-x6zl9
Nov 25 06:52:39.165: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 25 06:52:39.165: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 25 06:52:39.165: INFO: update-demo-nautilus-x6zl9 is verified up and running
STEP: using delete to clean up resources
Nov 25 06:52:39.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 delete --grace-period=0 --force -f - --namespace=kubectl-279'
Nov 25 06:52:39.236: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 25 06:52:39.236: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 25 06:52:39.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-279'
Nov 25 06:52:39.379: INFO: stderr: "No resources found in kubectl-279 namespace.\n"
Nov 25 06:52:39.379: INFO: stdout: ""
Nov 25 06:52:39.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods -l name=update-demo --namespace=kubectl-279 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 25 06:52:39.452: INFO: stderr: ""
Nov 25 06:52:39.452: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:52:39.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-279" for this suite.

• [SLOW TEST:6.207 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":303,"completed":34,"skipped":450,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:52:39.458: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-50f66def-52ee-479c-98c5-36f48858c041
STEP: Creating a pod to test consume configMaps
Nov 25 06:52:39.512: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5c9087ac-c666-4484-8f05-32f8c1d11b5d" in namespace "projected-1792" to be "Succeeded or Failed"
Nov 25 06:52:39.514: INFO: Pod "pod-projected-configmaps-5c9087ac-c666-4484-8f05-32f8c1d11b5d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.905012ms
Nov 25 06:52:41.517: INFO: Pod "pod-projected-configmaps-5c9087ac-c666-4484-8f05-32f8c1d11b5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004794417s
STEP: Saw pod success
Nov 25 06:52:41.517: INFO: Pod "pod-projected-configmaps-5c9087ac-c666-4484-8f05-32f8c1d11b5d" satisfied condition "Succeeded or Failed"
Nov 25 06:52:41.519: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-5c9087ac-c666-4484-8f05-32f8c1d11b5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 25 06:52:41.532: INFO: Waiting for pod pod-projected-configmaps-5c9087ac-c666-4484-8f05-32f8c1d11b5d to disappear
Nov 25 06:52:41.533: INFO: Pod pod-projected-configmaps-5c9087ac-c666-4484-8f05-32f8c1d11b5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:52:41.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1792" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":303,"completed":35,"skipped":477,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:52:41.539: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 06:52:41.565: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb5e0fcb-7b56-4831-b1bc-0ca6fe2dc83b" in namespace "projected-3251" to be "Succeeded or Failed"
Nov 25 06:52:41.567: INFO: Pod "downwardapi-volume-eb5e0fcb-7b56-4831-b1bc-0ca6fe2dc83b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.894502ms
Nov 25 06:52:43.570: INFO: Pod "downwardapi-volume-eb5e0fcb-7b56-4831-b1bc-0ca6fe2dc83b": Phase="Running", Reason="", readiness=true. Elapsed: 2.004872064s
Nov 25 06:52:45.573: INFO: Pod "downwardapi-volume-eb5e0fcb-7b56-4831-b1bc-0ca6fe2dc83b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008122936s
STEP: Saw pod success
Nov 25 06:52:45.573: INFO: Pod "downwardapi-volume-eb5e0fcb-7b56-4831-b1bc-0ca6fe2dc83b" satisfied condition "Succeeded or Failed"
Nov 25 06:52:45.575: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-eb5e0fcb-7b56-4831-b1bc-0ca6fe2dc83b container client-container: <nil>
STEP: delete the pod
Nov 25 06:52:45.590: INFO: Waiting for pod downwardapi-volume-eb5e0fcb-7b56-4831-b1bc-0ca6fe2dc83b to disappear
Nov 25 06:52:45.592: INFO: Pod downwardapi-volume-eb5e0fcb-7b56-4831-b1bc-0ca6fe2dc83b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:52:45.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3251" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":36,"skipped":488,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:52:45.597: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:52:45.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-49" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":303,"completed":37,"skipped":506,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:52:45.636: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:52:45.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2181" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":303,"completed":38,"skipped":511,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:52:45.691: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-47331244-fdf2-4477-81f8-631484d50459
STEP: Creating configMap with name cm-test-opt-upd-796c7c47-8591-4410-9a8e-b16f784da970
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-47331244-fdf2-4477-81f8-631484d50459
STEP: Updating configmap cm-test-opt-upd-796c7c47-8591-4410-9a8e-b16f784da970
STEP: Creating configMap with name cm-test-opt-create-03587794-a824-42b4-8c24-33f75f5879dd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:53:57.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-553" for this suite.

• [SLOW TEST:72.282 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":39,"skipped":522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:53:57.975: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Nov 25 06:53:58.028: INFO: Waiting up to 5m0s for pod "var-expansion-a93ba693-6712-41d4-bc34-77d6ced72f61" in namespace "var-expansion-5281" to be "Succeeded or Failed"
Nov 25 06:53:58.033: INFO: Pod "var-expansion-a93ba693-6712-41d4-bc34-77d6ced72f61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.891753ms
Nov 25 06:54:00.036: INFO: Pod "var-expansion-a93ba693-6712-41d4-bc34-77d6ced72f61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007931972s
STEP: Saw pod success
Nov 25 06:54:00.036: INFO: Pod "var-expansion-a93ba693-6712-41d4-bc34-77d6ced72f61" satisfied condition "Succeeded or Failed"
Nov 25 06:54:00.038: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-a93ba693-6712-41d4-bc34-77d6ced72f61 container dapi-container: <nil>
STEP: delete the pod
Nov 25 06:54:00.053: INFO: Waiting for pod var-expansion-a93ba693-6712-41d4-bc34-77d6ced72f61 to disappear
Nov 25 06:54:00.055: INFO: Pod var-expansion-a93ba693-6712-41d4-bc34-77d6ced72f61 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:54:00.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5281" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":303,"completed":40,"skipped":550,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:54:00.062: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 25 06:54:02.617: INFO: Successfully updated pod "annotationupdate5e5c9ad4-7003-4ab4-b784-ac26c3b7306c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:54:04.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8324" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":303,"completed":41,"skipped":579,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:54:04.652: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 06:54:04.725: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6695eac6-4be0-4494-a78e-4a36cae3f61a" in namespace "downward-api-6390" to be "Succeeded or Failed"
Nov 25 06:54:04.730: INFO: Pod "downwardapi-volume-6695eac6-4be0-4494-a78e-4a36cae3f61a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.478145ms
Nov 25 06:54:06.733: INFO: Pod "downwardapi-volume-6695eac6-4be0-4494-a78e-4a36cae3f61a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007606615s
STEP: Saw pod success
Nov 25 06:54:06.733: INFO: Pod "downwardapi-volume-6695eac6-4be0-4494-a78e-4a36cae3f61a" satisfied condition "Succeeded or Failed"
Nov 25 06:54:06.735: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-6695eac6-4be0-4494-a78e-4a36cae3f61a container client-container: <nil>
STEP: delete the pod
Nov 25 06:54:06.750: INFO: Waiting for pod downwardapi-volume-6695eac6-4be0-4494-a78e-4a36cae3f61a to disappear
Nov 25 06:54:06.751: INFO: Pod downwardapi-volume-6695eac6-4be0-4494-a78e-4a36cae3f61a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:54:06.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6390" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":42,"skipped":600,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:54:06.757: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 25 06:54:06.780: INFO: Waiting up to 5m0s for pod "pod-e1447b48-5e21-481d-9e3d-9665b2ecc683" in namespace "emptydir-8105" to be "Succeeded or Failed"
Nov 25 06:54:06.784: INFO: Pod "pod-e1447b48-5e21-481d-9e3d-9665b2ecc683": Phase="Pending", Reason="", readiness=false. Elapsed: 4.117073ms
Nov 25 06:54:08.787: INFO: Pod "pod-e1447b48-5e21-481d-9e3d-9665b2ecc683": Phase="Running", Reason="", readiness=true. Elapsed: 2.007055962s
Nov 25 06:54:10.790: INFO: Pod "pod-e1447b48-5e21-481d-9e3d-9665b2ecc683": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010112075s
STEP: Saw pod success
Nov 25 06:54:10.790: INFO: Pod "pod-e1447b48-5e21-481d-9e3d-9665b2ecc683" satisfied condition "Succeeded or Failed"
Nov 25 06:54:10.792: INFO: Trying to get logs from node k8sconformance-m02 pod pod-e1447b48-5e21-481d-9e3d-9665b2ecc683 container test-container: <nil>
STEP: delete the pod
Nov 25 06:54:10.807: INFO: Waiting for pod pod-e1447b48-5e21-481d-9e3d-9665b2ecc683 to disappear
Nov 25 06:54:10.809: INFO: Pod pod-e1447b48-5e21-481d-9e3d-9665b2ecc683 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:54:10.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8105" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":43,"skipped":614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:54:10.814: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-288f277b-0c9f-4806-ad16-a518bcaaff62
STEP: Creating configMap with name cm-test-opt-upd-6d3f2b24-336b-428f-acbf-3c15535adb1d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-288f277b-0c9f-4806-ad16-a518bcaaff62
STEP: Updating configmap cm-test-opt-upd-6d3f2b24-336b-428f-acbf-3c15535adb1d
STEP: Creating configMap with name cm-test-opt-create-aca2e2f3-7602-41f4-9443-b92178f22100
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:55:33.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6785" for this suite.

• [SLOW TEST:82.305 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":44,"skipped":705,"failed":0}
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:55:33.120: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Nov 25 06:55:33.145: INFO: Waiting up to 5m0s for pod "client-containers-1053287f-6d94-4851-bac2-3eafc6ed52ac" in namespace "containers-1469" to be "Succeeded or Failed"
Nov 25 06:55:33.146: INFO: Pod "client-containers-1053287f-6d94-4851-bac2-3eafc6ed52ac": Phase="Pending", Reason="", readiness=false. Elapsed: 1.346214ms
Nov 25 06:55:35.148: INFO: Pod "client-containers-1053287f-6d94-4851-bac2-3eafc6ed52ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003702491s
STEP: Saw pod success
Nov 25 06:55:35.148: INFO: Pod "client-containers-1053287f-6d94-4851-bac2-3eafc6ed52ac" satisfied condition "Succeeded or Failed"
Nov 25 06:55:35.150: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-1053287f-6d94-4851-bac2-3eafc6ed52ac container test-container: <nil>
STEP: delete the pod
Nov 25 06:55:35.160: INFO: Waiting for pod client-containers-1053287f-6d94-4851-bac2-3eafc6ed52ac to disappear
Nov 25 06:55:35.162: INFO: Pod client-containers-1053287f-6d94-4851-bac2-3eafc6ed52ac no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:55:35.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1469" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":303,"completed":45,"skipped":709,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:55:35.167: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3097
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Nov 25 06:55:35.230: INFO: Found 0 stateful pods, waiting for 3
Nov 25 06:55:45.234: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 25 06:55:45.234: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 25 06:55:45.234: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 25 06:55:55.233: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 25 06:55:55.234: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 25 06:55:55.234: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 25 06:55:55.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-3097 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 25 06:55:55.461: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 25 06:55:55.461: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 25 06:55:55.461: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov 25 06:56:05.487: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov 25 06:56:15.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-3097 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 06:56:15.662: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 25 06:56:15.662: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 25 06:56:15.662: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 25 06:56:25.676: INFO: Waiting for StatefulSet statefulset-3097/ss2 to complete update
Nov 25 06:56:25.676: INFO: Waiting for Pod statefulset-3097/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 25 06:56:25.676: INFO: Waiting for Pod statefulset-3097/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 25 06:56:35.681: INFO: Waiting for StatefulSet statefulset-3097/ss2 to complete update
Nov 25 06:56:35.681: INFO: Waiting for Pod statefulset-3097/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 25 06:56:35.681: INFO: Waiting for Pod statefulset-3097/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 25 06:56:45.684: INFO: Waiting for StatefulSet statefulset-3097/ss2 to complete update
Nov 25 06:56:45.684: INFO: Waiting for Pod statefulset-3097/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 25 06:56:55.681: INFO: Waiting for StatefulSet statefulset-3097/ss2 to complete update
Nov 25 06:56:55.681: INFO: Waiting for Pod statefulset-3097/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Nov 25 06:57:05.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-3097 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 25 06:57:05.835: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 25 06:57:05.835: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 25 06:57:05.835: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 25 06:57:15.862: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov 25 06:57:25.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-3097 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 06:57:26.038: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 25 06:57:26.038: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 25 06:57:26.038: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 25 06:57:46.053: INFO: Waiting for StatefulSet statefulset-3097/ss2 to complete update
Nov 25 06:57:46.053: INFO: Waiting for Pod statefulset-3097/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 25 06:57:56.058: INFO: Deleting all statefulset in ns statefulset-3097
Nov 25 06:57:56.060: INFO: Scaling statefulset ss2 to 0
Nov 25 06:58:16.074: INFO: Waiting for statefulset status.replicas updated to 0
Nov 25 06:58:16.076: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:58:16.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3097" for this suite.

• [SLOW TEST:160.922 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":303,"completed":46,"skipped":713,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:58:16.089: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Nov 25 06:58:16.110: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 06:58:18.944: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:58:30.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6757" for this suite.

• [SLOW TEST:13.926 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":303,"completed":47,"skipped":725,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:58:30.015: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1125 06:58:31.074960      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 25 06:59:33.087: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 06:59:33.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8280" for this suite.

• [SLOW TEST:63.078 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":303,"completed":48,"skipped":747,"failed":0}
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 06:59:33.093: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-129a2326-3a7d-4f12-bc5f-475924c1c28e in namespace container-probe-9321
Nov 25 06:59:35.125: INFO: Started pod busybox-129a2326-3a7d-4f12-bc5f-475924c1c28e in namespace container-probe-9321
STEP: checking the pod's current state and verifying that restartCount is present
Nov 25 06:59:35.127: INFO: Initial restart count of pod busybox-129a2326-3a7d-4f12-bc5f-475924c1c28e is 0
Nov 25 07:00:23.190: INFO: Restart count of pod container-probe-9321/busybox-129a2326-3a7d-4f12-bc5f-475924c1c28e is now 1 (48.062988418s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:00:23.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9321" for this suite.

• [SLOW TEST:50.138 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":303,"completed":49,"skipped":747,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:00:23.232: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 25 07:00:25.785: INFO: Successfully updated pod "annotationupdatecb7dbc3f-0a2d-4496-a534-5bb6e2ccff9d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:00:29.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4105" for this suite.

• [SLOW TEST:6.577 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":303,"completed":50,"skipped":802,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:00:29.809: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-qdq9
STEP: Creating a pod to test atomic-volume-subpath
Nov 25 07:00:29.841: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qdq9" in namespace "subpath-3440" to be "Succeeded or Failed"
Nov 25 07:00:29.845: INFO: Pod "pod-subpath-test-configmap-qdq9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.318125ms
Nov 25 07:00:31.848: INFO: Pod "pod-subpath-test-configmap-qdq9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006859147s
Nov 25 07:00:33.850: INFO: Pod "pod-subpath-test-configmap-qdq9": Phase="Running", Reason="", readiness=true. Elapsed: 4.009388884s
Nov 25 07:00:35.853: INFO: Pod "pod-subpath-test-configmap-qdq9": Phase="Running", Reason="", readiness=true. Elapsed: 6.012209063s
Nov 25 07:00:37.856: INFO: Pod "pod-subpath-test-configmap-qdq9": Phase="Running", Reason="", readiness=true. Elapsed: 8.015504466s
Nov 25 07:00:39.859: INFO: Pod "pod-subpath-test-configmap-qdq9": Phase="Running", Reason="", readiness=true. Elapsed: 10.018794931s
Nov 25 07:00:41.862: INFO: Pod "pod-subpath-test-configmap-qdq9": Phase="Running", Reason="", readiness=true. Elapsed: 12.021393918s
Nov 25 07:00:43.865: INFO: Pod "pod-subpath-test-configmap-qdq9": Phase="Running", Reason="", readiness=true. Elapsed: 14.024067744s
Nov 25 07:00:45.868: INFO: Pod "pod-subpath-test-configmap-qdq9": Phase="Running", Reason="", readiness=true. Elapsed: 16.027215421s
Nov 25 07:00:47.871: INFO: Pod "pod-subpath-test-configmap-qdq9": Phase="Running", Reason="", readiness=true. Elapsed: 18.030422228s
Nov 25 07:00:49.874: INFO: Pod "pod-subpath-test-configmap-qdq9": Phase="Running", Reason="", readiness=true. Elapsed: 20.033509288s
Nov 25 07:00:51.877: INFO: Pod "pod-subpath-test-configmap-qdq9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.03639138s
STEP: Saw pod success
Nov 25 07:00:51.877: INFO: Pod "pod-subpath-test-configmap-qdq9" satisfied condition "Succeeded or Failed"
Nov 25 07:00:51.879: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-configmap-qdq9 container test-container-subpath-configmap-qdq9: <nil>
STEP: delete the pod
Nov 25 07:00:51.892: INFO: Waiting for pod pod-subpath-test-configmap-qdq9 to disappear
Nov 25 07:00:51.895: INFO: Pod pod-subpath-test-configmap-qdq9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qdq9
Nov 25 07:00:51.895: INFO: Deleting pod "pod-subpath-test-configmap-qdq9" in namespace "subpath-3440"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:00:51.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3440" for this suite.

• [SLOW TEST:22.093 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":303,"completed":51,"skipped":805,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:00:51.902: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov 25 07:00:51.936: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3987 /api/v1/namespaces/watch-3987/configmaps/e2e-watch-test-resource-version ed72484d-28bd-4f38-b14f-02d0ce9bcca8 4063 0 2020-11-25 07:00:51 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-11-25 07:00:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 25 07:00:51.937: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3987 /api/v1/namespaces/watch-3987/configmaps/e2e-watch-test-resource-version ed72484d-28bd-4f38-b14f-02d0ce9bcca8 4064 0 2020-11-25 07:00:51 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-11-25 07:00:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:00:51.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3987" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":303,"completed":52,"skipped":820,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:00:51.941: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 25 07:00:51.964: INFO: Waiting up to 5m0s for pod "pod-2ff3af85-5966-4c88-861d-6ef9f18ce7c8" in namespace "emptydir-8683" to be "Succeeded or Failed"
Nov 25 07:00:51.969: INFO: Pod "pod-2ff3af85-5966-4c88-861d-6ef9f18ce7c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.981264ms
Nov 25 07:00:53.972: INFO: Pod "pod-2ff3af85-5966-4c88-861d-6ef9f18ce7c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007845509s
STEP: Saw pod success
Nov 25 07:00:53.972: INFO: Pod "pod-2ff3af85-5966-4c88-861d-6ef9f18ce7c8" satisfied condition "Succeeded or Failed"
Nov 25 07:00:53.974: INFO: Trying to get logs from node k8sconformance-m02 pod pod-2ff3af85-5966-4c88-861d-6ef9f18ce7c8 container test-container: <nil>
STEP: delete the pod
Nov 25 07:00:53.991: INFO: Waiting for pod pod-2ff3af85-5966-4c88-861d-6ef9f18ce7c8 to disappear
Nov 25 07:00:53.993: INFO: Pod pod-2ff3af85-5966-4c88-861d-6ef9f18ce7c8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:00:53.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8683" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":53,"skipped":821,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:00:53.998: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:00:56.052: INFO: Waiting up to 5m0s for pod "client-envvars-a670b2c3-2be4-400c-9cb8-434b8701a450" in namespace "pods-739" to be "Succeeded or Failed"
Nov 25 07:00:56.055: INFO: Pod "client-envvars-a670b2c3-2be4-400c-9cb8-434b8701a450": Phase="Pending", Reason="", readiness=false. Elapsed: 3.153933ms
Nov 25 07:00:58.058: INFO: Pod "client-envvars-a670b2c3-2be4-400c-9cb8-434b8701a450": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006064177s
STEP: Saw pod success
Nov 25 07:00:58.058: INFO: Pod "client-envvars-a670b2c3-2be4-400c-9cb8-434b8701a450" satisfied condition "Succeeded or Failed"
Nov 25 07:00:58.060: INFO: Trying to get logs from node k8sconformance-m02 pod client-envvars-a670b2c3-2be4-400c-9cb8-434b8701a450 container env3cont: <nil>
STEP: delete the pod
Nov 25 07:00:58.074: INFO: Waiting for pod client-envvars-a670b2c3-2be4-400c-9cb8-434b8701a450 to disappear
Nov 25 07:00:58.075: INFO: Pod client-envvars-a670b2c3-2be4-400c-9cb8-434b8701a450 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:00:58.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-739" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":303,"completed":54,"skipped":851,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:00:58.081: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Nov 25 07:00:58.100: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-894937859 proxy --unix-socket=/tmp/kubectl-proxy-unix640313070/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:00:58.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3918" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":303,"completed":55,"skipped":855,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:00:58.160: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:00:58.418: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:01:01.460: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:01:01.464: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7045-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:01:02.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8058" for this suite.
STEP: Destroying namespace "webhook-8058-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":303,"completed":56,"skipped":893,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:01:02.584: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 25 07:01:02.604: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 25 07:01:02.610: INFO: Waiting for terminating namespaces to be deleted...
Nov 25 07:01:02.612: INFO: 
Logging pods the apiserver thinks is on node k8sconformance before test
Nov 25 07:01:02.615: INFO: coredns-f9fd979d6-z4vhz from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:02.615: INFO: 	Container coredns ready: true, restart count 0
Nov 25 07:01:02.615: INFO: etcd-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:02.615: INFO: 	Container etcd ready: true, restart count 0
Nov 25 07:01:02.615: INFO: kindnet-n566x from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:02.615: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 25 07:01:02.615: INFO: kube-apiserver-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:02.615: INFO: 	Container kube-apiserver ready: true, restart count 0
Nov 25 07:01:02.615: INFO: kube-controller-manager-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:02.615: INFO: 	Container kube-controller-manager ready: true, restart count 0
Nov 25 07:01:02.615: INFO: kube-proxy-58hhs from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:02.615: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 25 07:01:02.615: INFO: kube-scheduler-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:02.615: INFO: 	Container kube-scheduler ready: true, restart count 0
Nov 25 07:01:02.615: INFO: storage-provisioner from kube-system started at 2020-11-25 06:44:00 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:02.615: INFO: 	Container storage-provisioner ready: true, restart count 0
Nov 25 07:01:02.615: INFO: sonobuoy from sonobuoy started at 2020-11-25 06:44:55 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:02.615: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 25 07:01:02.615: INFO: sonobuoy-e2e-job-7010f57f836a484c from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:01:02.615: INFO: 	Container e2e ready: true, restart count 0
Nov 25 07:01:02.615: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 25 07:01:02.615: INFO: sonobuoy-systemd-logs-daemon-set-25ff39ce45304256-mnllj from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:01:02.615: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 25 07:01:02.615: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 25 07:01:02.615: INFO: 
Logging pods the apiserver thinks is on node k8sconformance-m02 before test
Nov 25 07:01:02.619: INFO: kindnet-6lgph from kube-system started at 2020-11-25 06:44:50 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:02.619: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 25 07:01:02.619: INFO: kube-proxy-8kw9q from kube-system started at 2020-11-25 06:44:50 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:02.619: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 25 07:01:02.619: INFO: server-envvars-55ca26b7-2134-4a0e-abe4-9ca426b52edc from pods-739 started at 2020-11-25 07:00:54 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:02.619: INFO: 	Container srv ready: true, restart count 0
Nov 25 07:01:02.619: INFO: sonobuoy-systemd-logs-daemon-set-25ff39ce45304256-nnr45 from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:01:02.619: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 25 07:01:02.619: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 25 07:01:02.619: INFO: sample-webhook-deployment-cbccbf6bb-wpr7v from webhook-8058 started at 2020-11-25 07:00:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:02.619: INFO: 	Container sample-webhook ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-0c14e625-8e75-41b4-a7f2-5b8f13f51e72 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-0c14e625-8e75-41b4-a7f2-5b8f13f51e72 off the node k8sconformance-m02
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0c14e625-8e75-41b4-a7f2-5b8f13f51e72
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:01:10.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4393" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.112 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":303,"completed":57,"skipped":904,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:01:10.696: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:01:10.730: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Nov 25 07:01:12.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-5839 create -f -'
Nov 25 07:01:13.781: INFO: stderr: ""
Nov 25 07:01:13.782: INFO: stdout: "e2e-test-crd-publish-openapi-3736-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 25 07:01:13.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-5839 delete e2e-test-crd-publish-openapi-3736-crds test-foo'
Nov 25 07:01:13.856: INFO: stderr: ""
Nov 25 07:01:13.856: INFO: stdout: "e2e-test-crd-publish-openapi-3736-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov 25 07:01:13.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-5839 apply -f -'
Nov 25 07:01:14.145: INFO: stderr: ""
Nov 25 07:01:14.145: INFO: stdout: "e2e-test-crd-publish-openapi-3736-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 25 07:01:14.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-5839 delete e2e-test-crd-publish-openapi-3736-crds test-foo'
Nov 25 07:01:14.220: INFO: stderr: ""
Nov 25 07:01:14.220: INFO: stdout: "e2e-test-crd-publish-openapi-3736-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Nov 25 07:01:14.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-5839 create -f -'
Nov 25 07:01:14.389: INFO: rc: 1
Nov 25 07:01:14.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-5839 apply -f -'
Nov 25 07:01:14.556: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Nov 25 07:01:14.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-5839 create -f -'
Nov 25 07:01:14.728: INFO: rc: 1
Nov 25 07:01:14.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-5839 apply -f -'
Nov 25 07:01:14.897: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Nov 25 07:01:14.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 explain e2e-test-crd-publish-openapi-3736-crds'
Nov 25 07:01:15.071: INFO: stderr: ""
Nov 25 07:01:15.071: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3736-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Nov 25 07:01:15.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 explain e2e-test-crd-publish-openapi-3736-crds.metadata'
Nov 25 07:01:15.242: INFO: stderr: ""
Nov 25 07:01:15.242: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3736-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov 25 07:01:15.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 explain e2e-test-crd-publish-openapi-3736-crds.spec'
Nov 25 07:01:15.411: INFO: stderr: ""
Nov 25 07:01:15.411: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3736-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov 25 07:01:15.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 explain e2e-test-crd-publish-openapi-3736-crds.spec.bars'
Nov 25 07:01:15.588: INFO: stderr: ""
Nov 25 07:01:15.588: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3736-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Nov 25 07:01:15.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 explain e2e-test-crd-publish-openapi-3736-crds.spec.bars2'
Nov 25 07:01:15.811: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:01:19.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5839" for this suite.

• [SLOW TEST:8.527 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":303,"completed":58,"skipped":947,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:01:19.223: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:01:19.553: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 25 07:01:21.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884479, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884479, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884479, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884479, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:01:24.615: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:01:24.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3" for this suite.
STEP: Destroying namespace "webhook-3-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.500 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":303,"completed":59,"skipped":954,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:01:24.724: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:01:25.441: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 25 07:01:27.447: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884485, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884485, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884485, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884485, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:01:30.482: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Nov 25 07:01:30.495: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:01:30.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3846" for this suite.
STEP: Destroying namespace "webhook-3846-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.833 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":303,"completed":60,"skipped":967,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:01:30.558: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Nov 25 07:01:30.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 create -f - --namespace=kubectl-5570'
Nov 25 07:01:30.987: INFO: stderr: ""
Nov 25 07:01:30.987: INFO: stdout: "pod/pause created\n"
Nov 25 07:01:30.987: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 25 07:01:30.987: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5570" to be "running and ready"
Nov 25 07:01:31.018: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 30.826248ms
Nov 25 07:01:33.021: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.033882068s
Nov 25 07:01:33.021: INFO: Pod "pause" satisfied condition "running and ready"
Nov 25 07:01:33.021: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Nov 25 07:01:33.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 label pods pause testing-label=testing-label-value --namespace=kubectl-5570'
Nov 25 07:01:33.096: INFO: stderr: ""
Nov 25 07:01:33.096: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov 25 07:01:33.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pod pause -L testing-label --namespace=kubectl-5570'
Nov 25 07:01:33.163: INFO: stderr: ""
Nov 25 07:01:33.164: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov 25 07:01:33.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 label pods pause testing-label- --namespace=kubectl-5570'
Nov 25 07:01:33.234: INFO: stderr: ""
Nov 25 07:01:33.234: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov 25 07:01:33.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pod pause -L testing-label --namespace=kubectl-5570'
Nov 25 07:01:33.307: INFO: stderr: ""
Nov 25 07:01:33.307: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Nov 25 07:01:33.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 delete --grace-period=0 --force -f - --namespace=kubectl-5570'
Nov 25 07:01:33.379: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 25 07:01:33.379: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 25 07:01:33.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get rc,svc -l name=pause --no-headers --namespace=kubectl-5570'
Nov 25 07:01:33.479: INFO: stderr: "No resources found in kubectl-5570 namespace.\n"
Nov 25 07:01:33.479: INFO: stdout: ""
Nov 25 07:01:33.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods -l name=pause --namespace=kubectl-5570 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 25 07:01:33.546: INFO: stderr: ""
Nov 25 07:01:33.546: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:01:33.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5570" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":303,"completed":61,"skipped":974,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:01:33.552: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 07:01:33.619: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3f8bb4e-bb14-4577-962f-3e66e2dbc616" in namespace "projected-1009" to be "Succeeded or Failed"
Nov 25 07:01:33.621: INFO: Pod "downwardapi-volume-c3f8bb4e-bb14-4577-962f-3e66e2dbc616": Phase="Pending", Reason="", readiness=false. Elapsed: 1.656523ms
Nov 25 07:01:35.623: INFO: Pod "downwardapi-volume-c3f8bb4e-bb14-4577-962f-3e66e2dbc616": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003401216s
Nov 25 07:01:37.626: INFO: Pod "downwardapi-volume-c3f8bb4e-bb14-4577-962f-3e66e2dbc616": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006280712s
STEP: Saw pod success
Nov 25 07:01:37.626: INFO: Pod "downwardapi-volume-c3f8bb4e-bb14-4577-962f-3e66e2dbc616" satisfied condition "Succeeded or Failed"
Nov 25 07:01:37.627: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-c3f8bb4e-bb14-4577-962f-3e66e2dbc616 container client-container: <nil>
STEP: delete the pod
Nov 25 07:01:37.693: INFO: Waiting for pod downwardapi-volume-c3f8bb4e-bb14-4577-962f-3e66e2dbc616 to disappear
Nov 25 07:01:37.695: INFO: Pod downwardapi-volume-c3f8bb4e-bb14-4577-962f-3e66e2dbc616 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:01:37.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1009" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":62,"skipped":991,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:01:37.700: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:01:37.749: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"77e1dece-d819-4167-9334-65f4e8f01f49", Controller:(*bool)(0xc004ad83ba), BlockOwnerDeletion:(*bool)(0xc004ad83bb)}}
Nov 25 07:01:37.753: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9f358a55-d3d2-49d0-96ed-0488e8a77cd1", Controller:(*bool)(0xc004ad8686), BlockOwnerDeletion:(*bool)(0xc004ad8687)}}
Nov 25 07:01:37.827: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4b846f5c-8b7b-44c7-9024-9de550711a10", Controller:(*bool)(0xc004a5dc86), BlockOwnerDeletion:(*bool)(0xc004a5dc87)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:01:42.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7300" for this suite.

• [SLOW TEST:5.138 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":303,"completed":63,"skipped":1006,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:01:42.838: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 25 07:01:42.859: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 25 07:01:42.863: INFO: Waiting for terminating namespaces to be deleted...
Nov 25 07:01:42.865: INFO: 
Logging pods the apiserver thinks is on node k8sconformance before test
Nov 25 07:01:42.868: INFO: coredns-f9fd979d6-z4vhz from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:42.868: INFO: 	Container coredns ready: true, restart count 0
Nov 25 07:01:42.868: INFO: etcd-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:42.868: INFO: 	Container etcd ready: true, restart count 0
Nov 25 07:01:42.868: INFO: kindnet-n566x from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:42.868: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 25 07:01:42.868: INFO: kube-apiserver-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:42.868: INFO: 	Container kube-apiserver ready: true, restart count 0
Nov 25 07:01:42.868: INFO: kube-controller-manager-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:42.868: INFO: 	Container kube-controller-manager ready: true, restart count 0
Nov 25 07:01:42.868: INFO: kube-proxy-58hhs from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:42.868: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 25 07:01:42.868: INFO: kube-scheduler-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:42.868: INFO: 	Container kube-scheduler ready: true, restart count 0
Nov 25 07:01:42.868: INFO: storage-provisioner from kube-system started at 2020-11-25 06:44:00 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:42.868: INFO: 	Container storage-provisioner ready: true, restart count 0
Nov 25 07:01:42.868: INFO: sonobuoy from sonobuoy started at 2020-11-25 06:44:55 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:42.868: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 25 07:01:42.868: INFO: sonobuoy-e2e-job-7010f57f836a484c from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:01:42.868: INFO: 	Container e2e ready: true, restart count 0
Nov 25 07:01:42.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 25 07:01:42.868: INFO: sonobuoy-systemd-logs-daemon-set-25ff39ce45304256-mnllj from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:01:42.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 25 07:01:42.868: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 25 07:01:42.868: INFO: 
Logging pods the apiserver thinks is on node k8sconformance-m02 before test
Nov 25 07:01:42.871: INFO: kindnet-6lgph from kube-system started at 2020-11-25 06:44:50 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:42.871: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 25 07:01:42.871: INFO: kube-proxy-8kw9q from kube-system started at 2020-11-25 06:44:50 +0000 UTC (1 container statuses recorded)
Nov 25 07:01:42.871: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 25 07:01:42.871: INFO: sonobuoy-systemd-logs-daemon-set-25ff39ce45304256-nnr45 from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:01:42.871: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 25 07:01:42.871: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.164aae27c23224ad], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:01:43.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2705" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":303,"completed":64,"skipped":1011,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:01:43.901: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2837 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2837;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2837 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2837;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2837.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2837.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2837.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2837.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2837.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2837.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2837.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2837.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2837.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2837.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2837.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 9.14.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.14.9_udp@PTR;check="$$(dig +tcp +noall +answer +search 9.14.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.14.9_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2837 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2837;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2837 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2837;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2837.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2837.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2837.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2837.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2837.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2837.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2837.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2837.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2837.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2837.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2837.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2837.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 9.14.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.14.9_udp@PTR;check="$$(dig +tcp +noall +answer +search 9.14.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.14.9_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 25 07:01:47.957: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:47.959: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:47.961: INFO: Unable to read wheezy_udp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:47.963: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:47.965: INFO: Unable to read wheezy_udp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:47.970: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:47.972: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:47.974: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:47.991: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:48.009: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:48.012: INFO: Unable to read jessie_udp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:48.014: INFO: Unable to read jessie_tcp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:48.016: INFO: Unable to read jessie_udp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:48.018: INFO: Unable to read jessie_tcp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:48.020: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:48.022: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:48.035: INFO: Lookups using dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2837 wheezy_tcp@dns-test-service.dns-2837 wheezy_udp@dns-test-service.dns-2837.svc wheezy_tcp@dns-test-service.dns-2837.svc wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2837 jessie_tcp@dns-test-service.dns-2837 jessie_udp@dns-test-service.dns-2837.svc jessie_tcp@dns-test-service.dns-2837.svc jessie_udp@_http._tcp.dns-test-service.dns-2837.svc jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc]

Nov 25 07:01:53.038: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.041: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.044: INFO: Unable to read wheezy_udp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.046: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.049: INFO: Unable to read wheezy_udp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.051: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.053: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.055: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.070: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.072: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.074: INFO: Unable to read jessie_udp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.076: INFO: Unable to read jessie_tcp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.078: INFO: Unable to read jessie_udp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.080: INFO: Unable to read jessie_tcp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.082: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.084: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:53.096: INFO: Lookups using dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2837 wheezy_tcp@dns-test-service.dns-2837 wheezy_udp@dns-test-service.dns-2837.svc wheezy_tcp@dns-test-service.dns-2837.svc wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2837 jessie_tcp@dns-test-service.dns-2837 jessie_udp@dns-test-service.dns-2837.svc jessie_tcp@dns-test-service.dns-2837.svc jessie_udp@_http._tcp.dns-test-service.dns-2837.svc jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc]

Nov 25 07:01:58.038: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.041: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.043: INFO: Unable to read wheezy_udp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.045: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.047: INFO: Unable to read wheezy_udp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.049: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.051: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.053: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.067: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.069: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.071: INFO: Unable to read jessie_udp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.073: INFO: Unable to read jessie_tcp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.074: INFO: Unable to read jessie_udp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.076: INFO: Unable to read jessie_tcp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.078: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.080: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:01:58.092: INFO: Lookups using dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2837 wheezy_tcp@dns-test-service.dns-2837 wheezy_udp@dns-test-service.dns-2837.svc wheezy_tcp@dns-test-service.dns-2837.svc wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2837 jessie_tcp@dns-test-service.dns-2837 jessie_udp@dns-test-service.dns-2837.svc jessie_tcp@dns-test-service.dns-2837.svc jessie_udp@_http._tcp.dns-test-service.dns-2837.svc jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc]

Nov 25 07:02:03.039: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.041: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.043: INFO: Unable to read wheezy_udp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.046: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.048: INFO: Unable to read wheezy_udp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.051: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.053: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.055: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.070: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.072: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.074: INFO: Unable to read jessie_udp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.076: INFO: Unable to read jessie_tcp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.079: INFO: Unable to read jessie_udp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.081: INFO: Unable to read jessie_tcp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.083: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.085: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:03.096: INFO: Lookups using dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2837 wheezy_tcp@dns-test-service.dns-2837 wheezy_udp@dns-test-service.dns-2837.svc wheezy_tcp@dns-test-service.dns-2837.svc wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2837 jessie_tcp@dns-test-service.dns-2837 jessie_udp@dns-test-service.dns-2837.svc jessie_tcp@dns-test-service.dns-2837.svc jessie_udp@_http._tcp.dns-test-service.dns-2837.svc jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc]

Nov 25 07:02:08.038: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.041: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.043: INFO: Unable to read wheezy_udp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.045: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.047: INFO: Unable to read wheezy_udp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.049: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.051: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.053: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.066: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.068: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.070: INFO: Unable to read jessie_udp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.072: INFO: Unable to read jessie_tcp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.074: INFO: Unable to read jessie_udp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.076: INFO: Unable to read jessie_tcp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.078: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.080: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:08.091: INFO: Lookups using dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2837 wheezy_tcp@dns-test-service.dns-2837 wheezy_udp@dns-test-service.dns-2837.svc wheezy_tcp@dns-test-service.dns-2837.svc wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2837 jessie_tcp@dns-test-service.dns-2837 jessie_udp@dns-test-service.dns-2837.svc jessie_tcp@dns-test-service.dns-2837.svc jessie_udp@_http._tcp.dns-test-service.dns-2837.svc jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc]

Nov 25 07:02:13.038: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.041: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.043: INFO: Unable to read wheezy_udp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.045: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.047: INFO: Unable to read wheezy_udp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.049: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.051: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.053: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.070: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.072: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.074: INFO: Unable to read jessie_udp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.076: INFO: Unable to read jessie_tcp@dns-test-service.dns-2837 from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.078: INFO: Unable to read jessie_udp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.080: INFO: Unable to read jessie_tcp@dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.082: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.084: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc from pod dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529: the server could not find the requested resource (get pods dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529)
Nov 25 07:02:13.095: INFO: Lookups using dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2837 wheezy_tcp@dns-test-service.dns-2837 wheezy_udp@dns-test-service.dns-2837.svc wheezy_tcp@dns-test-service.dns-2837.svc wheezy_udp@_http._tcp.dns-test-service.dns-2837.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2837.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2837 jessie_tcp@dns-test-service.dns-2837 jessie_udp@dns-test-service.dns-2837.svc jessie_tcp@dns-test-service.dns-2837.svc jessie_udp@_http._tcp.dns-test-service.dns-2837.svc jessie_tcp@_http._tcp.dns-test-service.dns-2837.svc]

Nov 25 07:02:18.093: INFO: DNS probes using dns-2837/dns-test-9f743ea4-caa2-4591-b861-d9b0f039f529 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:02:18.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2837" for this suite.

• [SLOW TEST:34.310 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":303,"completed":65,"skipped":1021,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:02:18.212: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-8d247511-0b90-4946-8ded-4ac5ea149592 in namespace container-probe-3655
Nov 25 07:02:20.307: INFO: Started pod busybox-8d247511-0b90-4946-8ded-4ac5ea149592 in namespace container-probe-3655
STEP: checking the pod's current state and verifying that restartCount is present
Nov 25 07:02:20.309: INFO: Initial restart count of pod busybox-8d247511-0b90-4946-8ded-4ac5ea149592 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:06:20.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3655" for this suite.

• [SLOW TEST:242.489 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":303,"completed":66,"skipped":1036,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:06:20.701: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Nov 25 07:06:20.723: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:06:30.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2911" for this suite.

• [SLOW TEST:9.961 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":303,"completed":67,"skipped":1046,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:06:30.663: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 25 07:06:30.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-5409'
Nov 25 07:06:30.765: INFO: stderr: ""
Nov 25 07:06:30.765: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Nov 25 07:06:35.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pod e2e-test-httpd-pod --namespace=kubectl-5409 -o json'
Nov 25 07:06:35.897: INFO: stderr: ""
Nov 25 07:06:35.897: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-11-25T07:06:30Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-25T07:06:30Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.244.1.71\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-25T07:06:32Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5409\",\n        \"resourceVersion\": \"5095\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5409/pods/e2e-test-httpd-pod\",\n        \"uid\": \"e0df4e88-9b79-46ab-b91c-1efcdb7daa5b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-pvnxf\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8sconformance-m02\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-pvnxf\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-pvnxf\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-25T07:06:30Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-25T07:06:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-25T07:06:32Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-25T07:06:30Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://62ccfd6b81591e36bb86282685e2a79d0f3c6a6abc4d245850e6e1f26a6a8f66\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-11-25T07:06:31Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.49.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.71\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.71\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-11-25T07:06:30Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov 25 07:06:35.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 replace -f - --namespace=kubectl-5409'
Nov 25 07:06:36.221: INFO: stderr: ""
Nov 25 07:06:36.221: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Nov 25 07:06:36.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 delete pods e2e-test-httpd-pod --namespace=kubectl-5409'
Nov 25 07:06:39.091: INFO: stderr: ""
Nov 25 07:06:39.091: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:06:39.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5409" for this suite.

• [SLOW TEST:8.434 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":303,"completed":68,"skipped":1051,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:06:39.097: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:06:46.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9651" for this suite.

• [SLOW TEST:7.050 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":303,"completed":69,"skipped":1061,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:06:46.148: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Nov 25 07:06:46.171: INFO: Major version: 1
STEP: Confirm minor version
Nov 25 07:06:46.171: INFO: cleanMinorVersion: 19
Nov 25 07:06:46.171: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:06:46.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-9829" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":303,"completed":70,"skipped":1067,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:06:46.176: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 25 07:06:46.200: INFO: Waiting up to 5m0s for pod "downward-api-2ad617a5-ffe2-4669-b1fa-ee37385f5f28" in namespace "downward-api-8882" to be "Succeeded or Failed"
Nov 25 07:06:46.205: INFO: Pod "downward-api-2ad617a5-ffe2-4669-b1fa-ee37385f5f28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.40654ms
Nov 25 07:06:48.207: INFO: Pod "downward-api-2ad617a5-ffe2-4669-b1fa-ee37385f5f28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007049561s
STEP: Saw pod success
Nov 25 07:06:48.207: INFO: Pod "downward-api-2ad617a5-ffe2-4669-b1fa-ee37385f5f28" satisfied condition "Succeeded or Failed"
Nov 25 07:06:48.209: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-2ad617a5-ffe2-4669-b1fa-ee37385f5f28 container dapi-container: <nil>
STEP: delete the pod
Nov 25 07:06:48.227: INFO: Waiting for pod downward-api-2ad617a5-ffe2-4669-b1fa-ee37385f5f28 to disappear
Nov 25 07:06:48.229: INFO: Pod downward-api-2ad617a5-ffe2-4669-b1fa-ee37385f5f28 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:06:48.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8882" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":303,"completed":71,"skipped":1068,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:06:48.234: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6513
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6513
I1125 07:06:48.324966      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6513, replica count: 2
I1125 07:06:51.375412      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 25 07:06:51.375: INFO: Creating new exec pod
Nov 25 07:06:54.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6513 execpodg4lml -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 25 07:06:54.573: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 25 07:06:54.573: INFO: stdout: ""
Nov 25 07:06:54.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6513 execpodg4lml -- /bin/sh -x -c nc -zv -t -w 2 10.102.161.234 80'
Nov 25 07:06:54.722: INFO: stderr: "+ nc -zv -t -w 2 10.102.161.234 80\nConnection to 10.102.161.234 80 port [tcp/http] succeeded!\n"
Nov 25 07:06:54.722: INFO: stdout: ""
Nov 25 07:06:54.722: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:06:54.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6513" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.532 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":303,"completed":72,"skipped":1077,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:06:54.766: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Nov 25 07:06:58.836: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4380 PodName:pod-sharedvolume-2c91fcf5-30c0-41cd-bcbd-5021add2a74d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:06:58.836: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:06:58.910: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:06:58.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4380" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":303,"completed":73,"skipped":1081,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:06:58.917: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5021
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5021
STEP: creating replication controller externalsvc in namespace services-5021
I1125 07:06:59.043428      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-5021, replica count: 2
I1125 07:07:02.109083      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Nov 25 07:07:02.126: INFO: Creating new exec pod
Nov 25 07:07:04.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-5021 execpod75bps -- /bin/sh -x -c nslookup nodeport-service.services-5021.svc.cluster.local'
Nov 25 07:07:04.430: INFO: stderr: "+ nslookup nodeport-service.services-5021.svc.cluster.local\n"
Nov 25 07:07:04.430: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-5021.svc.cluster.local\tcanonical name = externalsvc.services-5021.svc.cluster.local.\nName:\texternalsvc.services-5021.svc.cluster.local\nAddress: 10.98.23.220\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5021, will wait for the garbage collector to delete the pods
Nov 25 07:07:04.488: INFO: Deleting ReplicationController externalsvc took: 4.534929ms
Nov 25 07:07:04.588: INFO: Terminating ReplicationController externalsvc pods took: 100.157394ms
Nov 25 07:07:18.600: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:07:18.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5021" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:19.698 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":303,"completed":74,"skipped":1103,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:07:18.616: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:07:19.026: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 25 07:07:21.033: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884839, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884839, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884839, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884839, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:07:24.046: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:07:34.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8643" for this suite.
STEP: Destroying namespace "webhook-8643-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.593 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":303,"completed":75,"skipped":1118,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:07:34.209: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 25 07:07:34.272: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 25 07:07:34.278: INFO: Waiting for terminating namespaces to be deleted...
Nov 25 07:07:34.280: INFO: 
Logging pods the apiserver thinks is on node k8sconformance before test
Nov 25 07:07:34.285: INFO: coredns-f9fd979d6-z4vhz from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:07:34.285: INFO: 	Container coredns ready: true, restart count 0
Nov 25 07:07:34.285: INFO: etcd-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:07:34.285: INFO: 	Container etcd ready: true, restart count 0
Nov 25 07:07:34.285: INFO: kindnet-n566x from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:07:34.285: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 25 07:07:34.285: INFO: kube-apiserver-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:07:34.285: INFO: 	Container kube-apiserver ready: true, restart count 0
Nov 25 07:07:34.285: INFO: kube-controller-manager-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:07:34.285: INFO: 	Container kube-controller-manager ready: true, restart count 0
Nov 25 07:07:34.285: INFO: kube-proxy-58hhs from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:07:34.285: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 25 07:07:34.285: INFO: kube-scheduler-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:07:34.285: INFO: 	Container kube-scheduler ready: true, restart count 0
Nov 25 07:07:34.285: INFO: storage-provisioner from kube-system started at 2020-11-25 06:44:00 +0000 UTC (1 container statuses recorded)
Nov 25 07:07:34.285: INFO: 	Container storage-provisioner ready: true, restart count 0
Nov 25 07:07:34.285: INFO: sonobuoy from sonobuoy started at 2020-11-25 06:44:55 +0000 UTC (1 container statuses recorded)
Nov 25 07:07:34.285: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 25 07:07:34.285: INFO: sonobuoy-e2e-job-7010f57f836a484c from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:07:34.285: INFO: 	Container e2e ready: true, restart count 0
Nov 25 07:07:34.285: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 25 07:07:34.285: INFO: sonobuoy-systemd-logs-daemon-set-25ff39ce45304256-mnllj from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:07:34.285: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 25 07:07:34.285: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 25 07:07:34.285: INFO: 
Logging pods the apiserver thinks is on node k8sconformance-m02 before test
Nov 25 07:07:34.288: INFO: kindnet-6lgph from kube-system started at 2020-11-25 06:44:50 +0000 UTC (1 container statuses recorded)
Nov 25 07:07:34.288: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 25 07:07:34.288: INFO: kube-proxy-8kw9q from kube-system started at 2020-11-25 06:44:50 +0000 UTC (1 container statuses recorded)
Nov 25 07:07:34.288: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 25 07:07:34.288: INFO: sonobuoy-systemd-logs-daemon-set-25ff39ce45304256-nnr45 from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:07:34.288: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 25 07:07:34.288: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-02761468-60a1-44f8-bfa5-cecf39888a37 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-02761468-60a1-44f8-bfa5-cecf39888a37 off the node k8sconformance-m02
STEP: verifying the node doesn't have the label kubernetes.io/e2e-02761468-60a1-44f8-bfa5-cecf39888a37
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:07:38.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6072" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":303,"completed":76,"skipped":1118,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:07:38.453: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 07:07:38.484: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6be7b37e-03a8-4d6d-b2f3-fdbcc7d9b30f" in namespace "projected-4408" to be "Succeeded or Failed"
Nov 25 07:07:38.485: INFO: Pod "downwardapi-volume-6be7b37e-03a8-4d6d-b2f3-fdbcc7d9b30f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.522181ms
Nov 25 07:07:40.488: INFO: Pod "downwardapi-volume-6be7b37e-03a8-4d6d-b2f3-fdbcc7d9b30f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004519565s
STEP: Saw pod success
Nov 25 07:07:40.488: INFO: Pod "downwardapi-volume-6be7b37e-03a8-4d6d-b2f3-fdbcc7d9b30f" satisfied condition "Succeeded or Failed"
Nov 25 07:07:40.490: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-6be7b37e-03a8-4d6d-b2f3-fdbcc7d9b30f container client-container: <nil>
STEP: delete the pod
Nov 25 07:07:40.507: INFO: Waiting for pod downwardapi-volume-6be7b37e-03a8-4d6d-b2f3-fdbcc7d9b30f to disappear
Nov 25 07:07:40.509: INFO: Pod downwardapi-volume-6be7b37e-03a8-4d6d-b2f3-fdbcc7d9b30f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:07:40.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4408" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":303,"completed":77,"skipped":1123,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:07:40.514: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-74faddf5-d2bd-4bc7-817f-378ff7493153
STEP: Creating a pod to test consume configMaps
Nov 25 07:07:40.566: INFO: Waiting up to 5m0s for pod "pod-configmaps-a2989ba2-578c-486e-a8ad-bf2057162db1" in namespace "configmap-7439" to be "Succeeded or Failed"
Nov 25 07:07:40.568: INFO: Pod "pod-configmaps-a2989ba2-578c-486e-a8ad-bf2057162db1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.153258ms
Nov 25 07:07:42.571: INFO: Pod "pod-configmaps-a2989ba2-578c-486e-a8ad-bf2057162db1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004853838s
STEP: Saw pod success
Nov 25 07:07:42.571: INFO: Pod "pod-configmaps-a2989ba2-578c-486e-a8ad-bf2057162db1" satisfied condition "Succeeded or Failed"
Nov 25 07:07:42.573: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-a2989ba2-578c-486e-a8ad-bf2057162db1 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 25 07:07:42.588: INFO: Waiting for pod pod-configmaps-a2989ba2-578c-486e-a8ad-bf2057162db1 to disappear
Nov 25 07:07:42.590: INFO: Pod pod-configmaps-a2989ba2-578c-486e-a8ad-bf2057162db1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:07:42.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7439" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":78,"skipped":1126,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:07:42.595: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:07:44.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9451" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":79,"skipped":1129,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:07:44.642: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:07:44.667: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 25 07:07:47.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-9055 create -f -'
Nov 25 07:07:48.406: INFO: stderr: ""
Nov 25 07:07:48.407: INFO: stdout: "e2e-test-crd-publish-openapi-2207-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 25 07:07:48.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-9055 delete e2e-test-crd-publish-openapi-2207-crds test-cr'
Nov 25 07:07:48.479: INFO: stderr: ""
Nov 25 07:07:48.479: INFO: stdout: "e2e-test-crd-publish-openapi-2207-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov 25 07:07:48.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-9055 apply -f -'
Nov 25 07:07:48.770: INFO: stderr: ""
Nov 25 07:07:48.770: INFO: stdout: "e2e-test-crd-publish-openapi-2207-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 25 07:07:48.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-9055 delete e2e-test-crd-publish-openapi-2207-crds test-cr'
Nov 25 07:07:48.841: INFO: stderr: ""
Nov 25 07:07:48.841: INFO: stdout: "e2e-test-crd-publish-openapi-2207-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 25 07:07:48.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 explain e2e-test-crd-publish-openapi-2207-crds'
Nov 25 07:07:49.047: INFO: stderr: ""
Nov 25 07:07:49.047: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2207-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:07:52.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9055" for this suite.

• [SLOW TEST:7.723 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":303,"completed":80,"skipped":1139,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:07:52.365: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-1358
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 25 07:07:52.387: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 25 07:07:52.403: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 25 07:07:54.406: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:07:56.406: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:07:58.406: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:08:00.406: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:08:02.406: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:08:04.406: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:08:06.406: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:08:08.406: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 25 07:08:08.410: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 25 07:08:10.422: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.86:8080/dial?request=hostname&protocol=udp&host=10.244.0.16&port=8081&tries=1'] Namespace:pod-network-test-1358 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:08:10.422: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:08:10.528: INFO: Waiting for responses: map[]
Nov 25 07:08:10.530: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.86:8080/dial?request=hostname&protocol=udp&host=10.244.1.85&port=8081&tries=1'] Namespace:pod-network-test-1358 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:08:10.530: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:08:10.599: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:08:10.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1358" for this suite.

• [SLOW TEST:18.241 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":303,"completed":81,"skipped":1145,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:08:10.606: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 25 07:08:10.626: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:08:15.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7875" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":303,"completed":82,"skipped":1150,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:08:15.293: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov 25 07:08:15.318: INFO: Waiting up to 5m0s for pod "pod-556d48c0-28d3-44b8-a51c-e3c2f10eda76" in namespace "emptydir-1573" to be "Succeeded or Failed"
Nov 25 07:08:15.319: INFO: Pod "pod-556d48c0-28d3-44b8-a51c-e3c2f10eda76": Phase="Pending", Reason="", readiness=false. Elapsed: 1.535878ms
Nov 25 07:08:17.323: INFO: Pod "pod-556d48c0-28d3-44b8-a51c-e3c2f10eda76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004730022s
STEP: Saw pod success
Nov 25 07:08:17.323: INFO: Pod "pod-556d48c0-28d3-44b8-a51c-e3c2f10eda76" satisfied condition "Succeeded or Failed"
Nov 25 07:08:17.340: INFO: Trying to get logs from node k8sconformance-m02 pod pod-556d48c0-28d3-44b8-a51c-e3c2f10eda76 container test-container: <nil>
STEP: delete the pod
Nov 25 07:08:17.356: INFO: Waiting for pod pod-556d48c0-28d3-44b8-a51c-e3c2f10eda76 to disappear
Nov 25 07:08:17.357: INFO: Pod pod-556d48c0-28d3-44b8-a51c-e3c2f10eda76 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:08:17.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1573" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":83,"skipped":1176,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:08:17.362: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:08:21.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3692" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":303,"completed":84,"skipped":1178,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:08:21.403: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 25 07:08:21.442: INFO: Waiting up to 5m0s for pod "pod-5a522875-75eb-438e-be38-4627fe8176c5" in namespace "emptydir-3689" to be "Succeeded or Failed"
Nov 25 07:08:21.446: INFO: Pod "pod-5a522875-75eb-438e-be38-4627fe8176c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.431953ms
Nov 25 07:08:23.449: INFO: Pod "pod-5a522875-75eb-438e-be38-4627fe8176c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007453483s
STEP: Saw pod success
Nov 25 07:08:23.449: INFO: Pod "pod-5a522875-75eb-438e-be38-4627fe8176c5" satisfied condition "Succeeded or Failed"
Nov 25 07:08:23.451: INFO: Trying to get logs from node k8sconformance-m02 pod pod-5a522875-75eb-438e-be38-4627fe8176c5 container test-container: <nil>
STEP: delete the pod
Nov 25 07:08:23.467: INFO: Waiting for pod pod-5a522875-75eb-438e-be38-4627fe8176c5 to disappear
Nov 25 07:08:23.468: INFO: Pod pod-5a522875-75eb-438e-be38-4627fe8176c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:08:23.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3689" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":85,"skipped":1186,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:08:23.473: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 07:08:23.517: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f5a6d48-b8d6-4e93-a9c2-44eb8f93ec23" in namespace "downward-api-4227" to be "Succeeded or Failed"
Nov 25 07:08:23.520: INFO: Pod "downwardapi-volume-5f5a6d48-b8d6-4e93-a9c2-44eb8f93ec23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921437ms
Nov 25 07:08:25.523: INFO: Pod "downwardapi-volume-5f5a6d48-b8d6-4e93-a9c2-44eb8f93ec23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005069297s
STEP: Saw pod success
Nov 25 07:08:25.523: INFO: Pod "downwardapi-volume-5f5a6d48-b8d6-4e93-a9c2-44eb8f93ec23" satisfied condition "Succeeded or Failed"
Nov 25 07:08:25.524: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-5f5a6d48-b8d6-4e93-a9c2-44eb8f93ec23 container client-container: <nil>
STEP: delete the pod
Nov 25 07:08:25.537: INFO: Waiting for pod downwardapi-volume-5f5a6d48-b8d6-4e93-a9c2-44eb8f93ec23 to disappear
Nov 25 07:08:25.539: INFO: Pod downwardapi-volume-5f5a6d48-b8d6-4e93-a9c2-44eb8f93ec23 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:08:25.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4227" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":303,"completed":86,"skipped":1209,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:08:25.544: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-44cbabe5-edb5-49e5-9233-a02cc205748a
STEP: Creating a pod to test consume secrets
Nov 25 07:08:25.567: INFO: Waiting up to 5m0s for pod "pod-secrets-ea506651-0853-46e6-95c7-378d67c862c0" in namespace "secrets-3978" to be "Succeeded or Failed"
Nov 25 07:08:25.568: INFO: Pod "pod-secrets-ea506651-0853-46e6-95c7-378d67c862c0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.443628ms
Nov 25 07:08:27.570: INFO: Pod "pod-secrets-ea506651-0853-46e6-95c7-378d67c862c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003566896s
STEP: Saw pod success
Nov 25 07:08:27.570: INFO: Pod "pod-secrets-ea506651-0853-46e6-95c7-378d67c862c0" satisfied condition "Succeeded or Failed"
Nov 25 07:08:27.572: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-ea506651-0853-46e6-95c7-378d67c862c0 container secret-volume-test: <nil>
STEP: delete the pod
Nov 25 07:08:27.581: INFO: Waiting for pod pod-secrets-ea506651-0853-46e6-95c7-378d67c862c0 to disappear
Nov 25 07:08:27.582: INFO: Pod pod-secrets-ea506651-0853-46e6-95c7-378d67c862c0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:08:27.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3978" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":87,"skipped":1226,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:08:27.587: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2237
STEP: creating service affinity-nodeport in namespace services-2237
STEP: creating replication controller affinity-nodeport in namespace services-2237
I1125 07:08:27.626194      23 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-2237, replica count: 3
I1125 07:08:30.676595      23 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 25 07:08:30.682: INFO: Creating new exec pod
Nov 25 07:08:33.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2237 execpod-affinity2rjsr -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Nov 25 07:08:33.893: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Nov 25 07:08:33.893: INFO: stdout: ""
Nov 25 07:08:33.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2237 execpod-affinity2rjsr -- /bin/sh -x -c nc -zv -t -w 2 10.106.63.49 80'
Nov 25 07:08:34.042: INFO: stderr: "+ nc -zv -t -w 2 10.106.63.49 80\nConnection to 10.106.63.49 80 port [tcp/http] succeeded!\n"
Nov 25 07:08:34.042: INFO: stdout: ""
Nov 25 07:08:34.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2237 execpod-affinity2rjsr -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.2 32276'
Nov 25 07:08:34.200: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.2 32276\nConnection to 192.168.49.2 32276 port [tcp/32276] succeeded!\n"
Nov 25 07:08:34.200: INFO: stdout: ""
Nov 25 07:08:34.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2237 execpod-affinity2rjsr -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.3 32276'
Nov 25 07:08:34.341: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.3 32276\nConnection to 192.168.49.3 32276 port [tcp/32276] succeeded!\n"
Nov 25 07:08:34.341: INFO: stdout: ""
Nov 25 07:08:34.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2237 execpod-affinity2rjsr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:32276/ ; done'
Nov 25 07:08:34.562: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32276/\n"
Nov 25 07:08:34.562: INFO: stdout: "\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx\naffinity-nodeport-nfjcx"
Nov 25 07:08:34.562: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.562: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.562: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.562: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.562: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.562: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.563: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.563: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.563: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.563: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.563: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.563: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.563: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.563: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.563: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.563: INFO: Received response from host: affinity-nodeport-nfjcx
Nov 25 07:08:34.563: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2237, will wait for the garbage collector to delete the pods
Nov 25 07:08:34.630: INFO: Deleting ReplicationController affinity-nodeport took: 4.631343ms
Nov 25 07:08:35.030: INFO: Terminating ReplicationController affinity-nodeport pods took: 400.328654ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:08:50.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2237" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:23.170 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":88,"skipped":1240,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:08:50.757: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-155/secret-test-f95ed3d0-5b64-4afb-b2ab-fda4e115a4c9
STEP: Creating a pod to test consume secrets
Nov 25 07:08:50.810: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ac00a83-7fd2-41f0-ab9f-2784ccb89eec" in namespace "secrets-155" to be "Succeeded or Failed"
Nov 25 07:08:50.812: INFO: Pod "pod-configmaps-6ac00a83-7fd2-41f0-ab9f-2784ccb89eec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.238845ms
Nov 25 07:08:52.816: INFO: Pod "pod-configmaps-6ac00a83-7fd2-41f0-ab9f-2784ccb89eec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005557341s
STEP: Saw pod success
Nov 25 07:08:52.816: INFO: Pod "pod-configmaps-6ac00a83-7fd2-41f0-ab9f-2784ccb89eec" satisfied condition "Succeeded or Failed"
Nov 25 07:08:52.818: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-6ac00a83-7fd2-41f0-ab9f-2784ccb89eec container env-test: <nil>
STEP: delete the pod
Nov 25 07:08:52.834: INFO: Waiting for pod pod-configmaps-6ac00a83-7fd2-41f0-ab9f-2784ccb89eec to disappear
Nov 25 07:08:52.835: INFO: Pod pod-configmaps-6ac00a83-7fd2-41f0-ab9f-2784ccb89eec no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:08:52.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-155" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":303,"completed":89,"skipped":1267,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:08:52.840: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-3c56039a-6fdd-4d5e-b948-54f936741ee3
STEP: Creating a pod to test consume configMaps
Nov 25 07:08:52.892: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-901ac0f5-3e29-4e76-85e0-ef786860ed12" in namespace "projected-849" to be "Succeeded or Failed"
Nov 25 07:08:52.897: INFO: Pod "pod-projected-configmaps-901ac0f5-3e29-4e76-85e0-ef786860ed12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.929996ms
Nov 25 07:08:54.900: INFO: Pod "pod-projected-configmaps-901ac0f5-3e29-4e76-85e0-ef786860ed12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008026447s
STEP: Saw pod success
Nov 25 07:08:54.900: INFO: Pod "pod-projected-configmaps-901ac0f5-3e29-4e76-85e0-ef786860ed12" satisfied condition "Succeeded or Failed"
Nov 25 07:08:54.902: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-901ac0f5-3e29-4e76-85e0-ef786860ed12 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 25 07:08:54.926: INFO: Waiting for pod pod-projected-configmaps-901ac0f5-3e29-4e76-85e0-ef786860ed12 to disappear
Nov 25 07:08:54.928: INFO: Pod pod-projected-configmaps-901ac0f5-3e29-4e76-85e0-ef786860ed12 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:08:54.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-849" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":90,"skipped":1289,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:08:54.933: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-0f890417-19bf-44cc-b54b-b71b7dd49982
STEP: Creating a pod to test consume configMaps
Nov 25 07:08:54.987: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f68bf099-168e-45e5-b7f7-47c421561379" in namespace "projected-205" to be "Succeeded or Failed"
Nov 25 07:08:54.991: INFO: Pod "pod-projected-configmaps-f68bf099-168e-45e5-b7f7-47c421561379": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057659ms
Nov 25 07:08:56.994: INFO: Pod "pod-projected-configmaps-f68bf099-168e-45e5-b7f7-47c421561379": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006893358s
STEP: Saw pod success
Nov 25 07:08:56.994: INFO: Pod "pod-projected-configmaps-f68bf099-168e-45e5-b7f7-47c421561379" satisfied condition "Succeeded or Failed"
Nov 25 07:08:56.996: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-f68bf099-168e-45e5-b7f7-47c421561379 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 25 07:08:57.006: INFO: Waiting for pod pod-projected-configmaps-f68bf099-168e-45e5-b7f7-47c421561379 to disappear
Nov 25 07:08:57.011: INFO: Pod pod-projected-configmaps-f68bf099-168e-45e5-b7f7-47c421561379 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:08:57.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-205" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":303,"completed":91,"skipped":1297,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:08:57.016: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:08:57.776: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 25 07:08:59.783: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884937, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884937, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884937, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884937, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:09:02.824: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:09:02.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1111" for this suite.
STEP: Destroying namespace "webhook-1111-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.869 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":303,"completed":92,"skipped":1309,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:09:02.886: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:09:03.323: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:09:06.411: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:09:07.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3930" for this suite.
STEP: Destroying namespace "webhook-3930-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":303,"completed":93,"skipped":1311,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:09:07.546: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:09:07.735: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 25 07:09:12.737: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 25 07:09:12.738: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 25 07:09:14.741: INFO: Creating deployment "test-rollover-deployment"
Nov 25 07:09:14.946: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 25 07:09:16.951: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 25 07:09:16.955: INFO: Ensure that both replica sets have 1 created replica
Nov 25 07:09:16.958: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 25 07:09:16.963: INFO: Updating deployment test-rollover-deployment
Nov 25 07:09:16.963: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 25 07:09:18.968: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 25 07:09:18.972: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 25 07:09:18.976: INFO: all replica sets need to contain the pod-template-hash label
Nov 25 07:09:18.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884958, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 25 07:09:20.982: INFO: all replica sets need to contain the pod-template-hash label
Nov 25 07:09:20.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884958, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 25 07:09:22.981: INFO: all replica sets need to contain the pod-template-hash label
Nov 25 07:09:22.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884958, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 25 07:09:24.982: INFO: all replica sets need to contain the pod-template-hash label
Nov 25 07:09:24.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884958, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 25 07:09:26.982: INFO: all replica sets need to contain the pod-template-hash label
Nov 25 07:09:26.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884958, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884955, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 25 07:09:28.981: INFO: 
Nov 25 07:09:28.981: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 25 07:09:28.987: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4054 /apis/apps/v1/namespaces/deployment-4054/deployments/test-rollover-deployment 0d439792-ffba-465c-9ff5-263ff32e11c9 6535 2 2020-11-25 07:09:14 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-25 07:09:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-25 07:09:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a6dc08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-25 07:09:15 +0000 UTC,LastTransitionTime:2020-11-25 07:09:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2020-11-25 07:09:28 +0000 UTC,LastTransitionTime:2020-11-25 07:09:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 25 07:09:28.990: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-4054 /apis/apps/v1/namespaces/deployment-4054/replicasets/test-rollover-deployment-5797c7764 73ee6278-0e65-4a89-a418-fc2f55880772 6524 2 2020-11-25 07:09:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 0d439792-ffba-465c-9ff5-263ff32e11c9 0xc002c0c7d0 0xc002c0c7d1}] []  [{kube-controller-manager Update apps/v1 2020-11-25 07:09:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0d439792-ffba-465c-9ff5-263ff32e11c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c0c9b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 25 07:09:28.990: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 25 07:09:28.990: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4054 /apis/apps/v1/namespaces/deployment-4054/replicasets/test-rollover-controller 6df83f5f-0862-4d46-933f-6101fe8a3d1a 6534 2 2020-11-25 07:09:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 0d439792-ffba-465c-9ff5-263ff32e11c9 0xc002c0c18f 0xc002c0c210}] []  [{e2e.test Update apps/v1 2020-11-25 07:09:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-25 07:09:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0d439792-ffba-465c-9ff5-263ff32e11c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002c0c5c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 25 07:09:28.990: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-4054 /apis/apps/v1/namespaces/deployment-4054/replicasets/test-rollover-deployment-78bc8b888c 43361929-9404-4d3c-a0e8-77304d543606 6494 2 2020-11-25 07:09:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 0d439792-ffba-465c-9ff5-263ff32e11c9 0xc002c0ca17 0xc002c0ca18}] []  [{kube-controller-manager Update apps/v1 2020-11-25 07:09:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0d439792-ffba-465c-9ff5-263ff32e11c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c0caa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 25 07:09:28.992: INFO: Pod "test-rollover-deployment-5797c7764-xlz55" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-xlz55 test-rollover-deployment-5797c7764- deployment-4054 /api/v1/namespaces/deployment-4054/pods/test-rollover-deployment-5797c7764-xlz55 0b1da3a2-84ec-4f19-9341-420f6784ab7a 6512 0 2020-11-25 07:09:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 73ee6278-0e65-4a89-a418-fc2f55880772 0xc002c0dac0 0xc002c0dac1}] []  [{kube-controller-manager Update v1 2020-11-25 07:09:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"73ee6278-0e65-4a89-a418-fc2f55880772\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:09:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-x7m7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-x7m7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-x7m7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:09:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.104,StartTime:2020-11-25 07:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-25 07:09:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://04dfda633fef05fadc25436214f774b77dfd927e6ca4bf2f010c012e34e7686a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:09:28.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4054" for this suite.

• [SLOW TEST:21.451 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":303,"completed":94,"skipped":1353,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:09:28.998: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:09:29.816: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 25 07:09:31.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884969, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884969, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884969, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741884969, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:09:34.910: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:09:47.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-293" for this suite.
STEP: Destroying namespace "webhook-293-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.092 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":303,"completed":95,"skipped":1363,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:09:47.090: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:09:58.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4959" for this suite.

• [SLOW TEST:11.086 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":303,"completed":96,"skipped":1371,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:09:58.176: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 25 07:09:58.217: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:10:01.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1596" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":303,"completed":97,"skipped":1380,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:10:01.549: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-2652
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 25 07:10:01.593: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 25 07:10:01.610: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 25 07:10:03.614: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:10:05.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:10:07.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:10:09.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:10:11.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:10:13.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:10:15.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:10:17.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:10:19.613: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 25 07:10:19.616: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 25 07:10:21.646: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.18 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2652 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:10:21.646: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:10:22.722: INFO: Found all expected endpoints: [netserver-0]
Nov 25 07:10:22.725: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.107 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2652 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:10:22.725: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:10:23.802: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:10:23.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2652" for this suite.

• [SLOW TEST:22.260 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":98,"skipped":1418,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:10:23.810: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-663
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-663
STEP: Creating statefulset with conflicting port in namespace statefulset-663
STEP: Waiting until pod test-pod will start running in namespace statefulset-663
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-663
Nov 25 07:10:27.856: INFO: Observed stateful pod in namespace: statefulset-663, name: ss-0, uid: e8dcd203-0c25-4081-aca8-f5e5eb060563, status phase: Failed. Waiting for statefulset controller to delete.
Nov 25 07:10:27.858: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-663
STEP: Removing pod with conflicting port in namespace statefulset-663
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-663 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 25 07:10:31.879: INFO: Deleting all statefulset in ns statefulset-663
Nov 25 07:10:31.881: INFO: Scaling statefulset ss to 0
Nov 25 07:10:41.896: INFO: Waiting for statefulset status.replicas updated to 0
Nov 25 07:10:41.898: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:10:41.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-663" for this suite.

• [SLOW TEST:18.103 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":303,"completed":99,"skipped":1424,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:10:41.913: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 25 07:10:41.955: INFO: Number of nodes with available pods: 0
Nov 25 07:10:41.955: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:10:42.960: INFO: Number of nodes with available pods: 0
Nov 25 07:10:42.960: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:10:43.960: INFO: Number of nodes with available pods: 1
Nov 25 07:10:43.960: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:10:44.960: INFO: Number of nodes with available pods: 2
Nov 25 07:10:44.960: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov 25 07:10:44.975: INFO: Number of nodes with available pods: 1
Nov 25 07:10:44.975: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:10:45.981: INFO: Number of nodes with available pods: 1
Nov 25 07:10:45.981: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:10:46.980: INFO: Number of nodes with available pods: 2
Nov 25 07:10:46.980: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5809, will wait for the garbage collector to delete the pods
Nov 25 07:10:47.071: INFO: Deleting DaemonSet.extensions daemon-set took: 4.491633ms
Nov 25 07:10:47.171: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.39449ms
Nov 25 07:11:00.674: INFO: Number of nodes with available pods: 0
Nov 25 07:11:00.674: INFO: Number of running nodes: 0, number of available pods: 0
Nov 25 07:11:00.676: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5809/daemonsets","resourceVersion":"7102"},"items":null}

Nov 25 07:11:00.677: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5809/pods","resourceVersion":"7102"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:11:00.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5809" for this suite.

• [SLOW TEST:18.774 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":303,"completed":100,"skipped":1430,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:11:00.688: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Nov 25 07:11:00.710: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:11:04.046: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:11:15.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7384" for this suite.

• [SLOW TEST:14.645 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":303,"completed":101,"skipped":1432,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:11:15.332: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-1f5c5f7d-fb34-4735-8998-7d89b7ac6401
STEP: Creating a pod to test consume secrets
Nov 25 07:11:15.412: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2d133658-34b4-496c-826b-eb03fa4c272f" in namespace "projected-4483" to be "Succeeded or Failed"
Nov 25 07:11:15.413: INFO: Pod "pod-projected-secrets-2d133658-34b4-496c-826b-eb03fa4c272f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.426682ms
Nov 25 07:11:17.416: INFO: Pod "pod-projected-secrets-2d133658-34b4-496c-826b-eb03fa4c272f": Phase="Running", Reason="", readiness=true. Elapsed: 2.004371227s
Nov 25 07:11:19.419: INFO: Pod "pod-projected-secrets-2d133658-34b4-496c-826b-eb03fa4c272f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007264958s
STEP: Saw pod success
Nov 25 07:11:19.419: INFO: Pod "pod-projected-secrets-2d133658-34b4-496c-826b-eb03fa4c272f" satisfied condition "Succeeded or Failed"
Nov 25 07:11:19.421: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-2d133658-34b4-496c-826b-eb03fa4c272f container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 25 07:11:19.439: INFO: Waiting for pod pod-projected-secrets-2d133658-34b4-496c-826b-eb03fa4c272f to disappear
Nov 25 07:11:19.442: INFO: Pod pod-projected-secrets-2d133658-34b4-496c-826b-eb03fa4c272f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:11:19.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4483" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":102,"skipped":1444,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:11:19.448: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:11:19.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2937" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":303,"completed":103,"skipped":1445,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:11:19.483: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 25 07:11:22.067: INFO: Successfully updated pod "pod-update-1fa4de4b-ae3b-4ef9-bd2a-a2279f967fde"
STEP: verifying the updated pod is in kubernetes
Nov 25 07:11:22.073: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:11:22.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-136" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":303,"completed":104,"skipped":1467,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:11:22.079: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 07:11:22.102: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5ff6030-4cec-4a2f-9dce-e516c6493269" in namespace "projected-5863" to be "Succeeded or Failed"
Nov 25 07:11:22.104: INFO: Pod "downwardapi-volume-c5ff6030-4cec-4a2f-9dce-e516c6493269": Phase="Pending", Reason="", readiness=false. Elapsed: 1.432793ms
Nov 25 07:11:24.107: INFO: Pod "downwardapi-volume-c5ff6030-4cec-4a2f-9dce-e516c6493269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004234121s
STEP: Saw pod success
Nov 25 07:11:24.107: INFO: Pod "downwardapi-volume-c5ff6030-4cec-4a2f-9dce-e516c6493269" satisfied condition "Succeeded or Failed"
Nov 25 07:11:24.109: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-c5ff6030-4cec-4a2f-9dce-e516c6493269 container client-container: <nil>
STEP: delete the pod
Nov 25 07:11:24.172: INFO: Waiting for pod downwardapi-volume-c5ff6030-4cec-4a2f-9dce-e516c6493269 to disappear
Nov 25 07:11:24.174: INFO: Pod downwardapi-volume-c5ff6030-4cec-4a2f-9dce-e516c6493269 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:11:24.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5863" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":303,"completed":105,"skipped":1543,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:11:24.182: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:11:24.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3263" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":303,"completed":106,"skipped":1562,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:11:24.219: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:11:24.785: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 25 07:11:26.791: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741885084, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741885084, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741885084, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741885084, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:11:29.802: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:11:29.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8973" for this suite.
STEP: Destroying namespace "webhook-8973-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.792 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":303,"completed":107,"skipped":1584,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:11:30.011: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 25 07:11:32.062: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:11:32.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1591" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":108,"skipped":1617,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:11:32.109: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 25 07:11:34.650: INFO: Successfully updated pod "labelsupdatebb11b610-61a2-409e-a65f-f1e6307abae2"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:11:36.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4769" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":303,"completed":109,"skipped":1619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:11:36.665: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 25 07:11:36.688: INFO: Waiting up to 5m0s for pod "pod-1750714a-9ef5-445b-a92c-2d88871758d4" in namespace "emptydir-9347" to be "Succeeded or Failed"
Nov 25 07:11:36.690: INFO: Pod "pod-1750714a-9ef5-445b-a92c-2d88871758d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.3959ms
Nov 25 07:11:38.693: INFO: Pod "pod-1750714a-9ef5-445b-a92c-2d88871758d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004423866s
Nov 25 07:11:40.695: INFO: Pod "pod-1750714a-9ef5-445b-a92c-2d88871758d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006569643s
STEP: Saw pod success
Nov 25 07:11:40.695: INFO: Pod "pod-1750714a-9ef5-445b-a92c-2d88871758d4" satisfied condition "Succeeded or Failed"
Nov 25 07:11:40.697: INFO: Trying to get logs from node k8sconformance-m02 pod pod-1750714a-9ef5-445b-a92c-2d88871758d4 container test-container: <nil>
STEP: delete the pod
Nov 25 07:11:40.706: INFO: Waiting for pod pod-1750714a-9ef5-445b-a92c-2d88871758d4 to disappear
Nov 25 07:11:40.711: INFO: Pod pod-1750714a-9ef5-445b-a92c-2d88871758d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:11:40.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9347" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":110,"skipped":1658,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:11:40.716: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-f48cccdc-38af-4d7a-bf7d-58a660b5bda0
STEP: Creating a pod to test consume secrets
Nov 25 07:11:40.741: INFO: Waiting up to 5m0s for pod "pod-secrets-c7181893-4d7e-488c-97b0-6cf3d2efbed1" in namespace "secrets-1789" to be "Succeeded or Failed"
Nov 25 07:11:40.743: INFO: Pod "pod-secrets-c7181893-4d7e-488c-97b0-6cf3d2efbed1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.87311ms
Nov 25 07:11:42.745: INFO: Pod "pod-secrets-c7181893-4d7e-488c-97b0-6cf3d2efbed1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004149626s
Nov 25 07:11:44.748: INFO: Pod "pod-secrets-c7181893-4d7e-488c-97b0-6cf3d2efbed1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007102119s
STEP: Saw pod success
Nov 25 07:11:44.748: INFO: Pod "pod-secrets-c7181893-4d7e-488c-97b0-6cf3d2efbed1" satisfied condition "Succeeded or Failed"
Nov 25 07:11:44.750: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-c7181893-4d7e-488c-97b0-6cf3d2efbed1 container secret-volume-test: <nil>
STEP: delete the pod
Nov 25 07:11:44.761: INFO: Waiting for pod pod-secrets-c7181893-4d7e-488c-97b0-6cf3d2efbed1 to disappear
Nov 25 07:11:44.804: INFO: Pod pod-secrets-c7181893-4d7e-488c-97b0-6cf3d2efbed1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:11:44.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1789" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":111,"skipped":1722,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:11:44.809: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Nov 25 07:11:44.834: INFO: Waiting up to 5m0s for pod "client-containers-c1a4f3c4-9e98-4299-a9ff-8a4d636edd85" in namespace "containers-1147" to be "Succeeded or Failed"
Nov 25 07:11:44.836: INFO: Pod "client-containers-c1a4f3c4-9e98-4299-a9ff-8a4d636edd85": Phase="Pending", Reason="", readiness=false. Elapsed: 1.521311ms
Nov 25 07:11:46.909: INFO: Pod "client-containers-c1a4f3c4-9e98-4299-a9ff-8a4d636edd85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075007637s
Nov 25 07:11:48.912: INFO: Pod "client-containers-c1a4f3c4-9e98-4299-a9ff-8a4d636edd85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077742538s
STEP: Saw pod success
Nov 25 07:11:48.912: INFO: Pod "client-containers-c1a4f3c4-9e98-4299-a9ff-8a4d636edd85" satisfied condition "Succeeded or Failed"
Nov 25 07:11:48.914: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-c1a4f3c4-9e98-4299-a9ff-8a4d636edd85 container test-container: <nil>
STEP: delete the pod
Nov 25 07:11:48.928: INFO: Waiting for pod client-containers-c1a4f3c4-9e98-4299-a9ff-8a4d636edd85 to disappear
Nov 25 07:11:48.930: INFO: Pod client-containers-c1a4f3c4-9e98-4299-a9ff-8a4d636edd85 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:11:48.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1147" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":303,"completed":112,"skipped":1723,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:11:48.935: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:11:48.954: INFO: Creating deployment "test-recreate-deployment"
Nov 25 07:11:48.957: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 25 07:11:48.993: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov 25 07:11:50.998: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 25 07:11:51.000: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 25 07:11:51.004: INFO: Updating deployment test-recreate-deployment
Nov 25 07:11:51.004: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 25 07:11:51.179: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9581 /apis/apps/v1/namespaces/deployment-9581/deployments/test-recreate-deployment afee13aa-efc5-4dc7-af58-fd052b970582 7597 2 2020-11-25 07:11:48 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-25 07:11:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-25 07:11:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b60b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-11-25 07:11:51 +0000 UTC,LastTransitionTime:2020-11-25 07:11:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2020-11-25 07:11:51 +0000 UTC,LastTransitionTime:2020-11-25 07:11:48 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov 25 07:11:51.181: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-9581 /apis/apps/v1/namespaces/deployment-9581/replicasets/test-recreate-deployment-f79dd4667 777a9192-2d65-4043-8074-1db5c1fc697b 7596 1 2020-11-25 07:11:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment afee13aa-efc5-4dc7-af58-fd052b970582 0xc003b614b0 0xc003b614b1}] []  [{kube-controller-manager Update apps/v1 2020-11-25 07:11:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afee13aa-efc5-4dc7-af58-fd052b970582\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b615b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 25 07:11:51.181: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 25 07:11:51.181: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-9581 /apis/apps/v1/namespaces/deployment-9581/replicasets/test-recreate-deployment-c96cf48f d7b68c0e-038d-4e5c-9da1-78c12cf0e222 7586 2 2020-11-25 07:11:48 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment afee13aa-efc5-4dc7-af58-fd052b970582 0xc003b6127f 0xc003b612b0}] []  [{kube-controller-manager Update apps/v1 2020-11-25 07:11:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afee13aa-efc5-4dc7-af58-fd052b970582\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b61438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 25 07:11:51.183: INFO: Pod "test-recreate-deployment-f79dd4667-sdw8m" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-sdw8m test-recreate-deployment-f79dd4667- deployment-9581 /api/v1/namespaces/deployment-9581/pods/test-recreate-deployment-f79dd4667-sdw8m e14f046d-2c0d-4d2b-b93f-49177364c56a 7598 0 2020-11-25 07:11:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 777a9192-2d65-4043-8074-1db5c1fc697b 0xc003b61f90 0xc003b61f91}] []  [{kube-controller-manager Update v1 2020-11-25 07:11:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"777a9192-2d65-4043-8074-1db5c1fc697b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:11:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6gvlk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6gvlk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6gvlk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:11:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:11:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:11:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:11:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2020-11-25 07:11:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:11:51.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9581" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":303,"completed":113,"skipped":1729,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:11:51.188: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:11:51.219: INFO: Create a RollingUpdate DaemonSet
Nov 25 07:11:51.221: INFO: Check that daemon pods launch on every node of the cluster
Nov 25 07:11:51.227: INFO: Number of nodes with available pods: 0
Nov 25 07:11:51.227: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:11:52.232: INFO: Number of nodes with available pods: 0
Nov 25 07:11:52.232: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:11:53.232: INFO: Number of nodes with available pods: 1
Nov 25 07:11:53.232: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:11:54.231: INFO: Number of nodes with available pods: 2
Nov 25 07:11:54.231: INFO: Number of running nodes: 2, number of available pods: 2
Nov 25 07:11:54.231: INFO: Update the DaemonSet to trigger a rollout
Nov 25 07:11:54.236: INFO: Updating DaemonSet daemon-set
Nov 25 07:11:58.246: INFO: Roll back the DaemonSet before rollout is complete
Nov 25 07:11:58.251: INFO: Updating DaemonSet daemon-set
Nov 25 07:11:58.251: INFO: Make sure DaemonSet rollback is complete
Nov 25 07:11:58.253: INFO: Wrong image for pod: daemon-set-4npkr. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 25 07:11:58.253: INFO: Pod daemon-set-4npkr is not available
Nov 25 07:11:59.260: INFO: Pod daemon-set-nvxpv is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4767, will wait for the garbage collector to delete the pods
Nov 25 07:11:59.328: INFO: Deleting DaemonSet.extensions daemon-set took: 6.632877ms
Nov 25 07:11:59.428: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.220511ms
Nov 25 07:12:08.530: INFO: Number of nodes with available pods: 0
Nov 25 07:12:08.530: INFO: Number of running nodes: 0, number of available pods: 0
Nov 25 07:12:08.532: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4767/daemonsets","resourceVersion":"7744"},"items":null}

Nov 25 07:12:08.533: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4767/pods","resourceVersion":"7744"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:12:08.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4767" for this suite.

• [SLOW TEST:17.355 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":303,"completed":114,"skipped":1730,"failed":0}
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:12:08.543: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:12:08.563: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:12:10.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4523" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":303,"completed":115,"skipped":1730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:12:10.587: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:12:10.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4338" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":303,"completed":116,"skipped":1760,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:12:10.636: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:12:10.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2373" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":303,"completed":117,"skipped":1778,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:12:10.678: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:12:10.740: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Creating first CR 
Nov 25 07:12:11.329: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-25T07:12:11Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-25T07:12:11Z]] name:name1 resourceVersion:7800 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d46a162f-6481-42de-ba91-e1c4e2b33038] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Nov 25 07:12:21.333: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-25T07:12:21Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-25T07:12:21Z]] name:name2 resourceVersion:7859 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:25732aa2-eadc-4963-934f-edfe170292a6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Nov 25 07:12:31.338: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-25T07:12:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-25T07:12:31Z]] name:name1 resourceVersion:7868 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d46a162f-6481-42de-ba91-e1c4e2b33038] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Nov 25 07:12:41.342: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-25T07:12:21Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-25T07:12:41Z]] name:name2 resourceVersion:7877 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:25732aa2-eadc-4963-934f-edfe170292a6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Nov 25 07:12:51.348: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-25T07:12:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-25T07:12:31Z]] name:name1 resourceVersion:7889 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d46a162f-6481-42de-ba91-e1c4e2b33038] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Nov 25 07:13:01.353: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-25T07:12:21Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-25T07:12:41Z]] name:name2 resourceVersion:7900 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:25732aa2-eadc-4963-934f-edfe170292a6] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:13:11.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5153" for this suite.

• [SLOW TEST:61.190 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":303,"completed":118,"skipped":1802,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:13:11.868: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-rmlg
STEP: Creating a pod to test atomic-volume-subpath
Nov 25 07:13:11.898: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rmlg" in namespace "subpath-6289" to be "Succeeded or Failed"
Nov 25 07:13:11.909: INFO: Pod "pod-subpath-test-projected-rmlg": Phase="Pending", Reason="", readiness=false. Elapsed: 10.810481ms
Nov 25 07:13:13.912: INFO: Pod "pod-subpath-test-projected-rmlg": Phase="Running", Reason="", readiness=true. Elapsed: 2.013515409s
Nov 25 07:13:15.918: INFO: Pod "pod-subpath-test-projected-rmlg": Phase="Running", Reason="", readiness=true. Elapsed: 4.019791879s
Nov 25 07:13:17.921: INFO: Pod "pod-subpath-test-projected-rmlg": Phase="Running", Reason="", readiness=true. Elapsed: 6.023143975s
Nov 25 07:13:19.924: INFO: Pod "pod-subpath-test-projected-rmlg": Phase="Running", Reason="", readiness=true. Elapsed: 8.026109903s
Nov 25 07:13:21.927: INFO: Pod "pod-subpath-test-projected-rmlg": Phase="Running", Reason="", readiness=true. Elapsed: 10.028888627s
Nov 25 07:13:23.930: INFO: Pod "pod-subpath-test-projected-rmlg": Phase="Running", Reason="", readiness=true. Elapsed: 12.032031275s
Nov 25 07:13:25.933: INFO: Pod "pod-subpath-test-projected-rmlg": Phase="Running", Reason="", readiness=true. Elapsed: 14.035189434s
Nov 25 07:13:27.937: INFO: Pod "pod-subpath-test-projected-rmlg": Phase="Running", Reason="", readiness=true. Elapsed: 16.038549955s
Nov 25 07:13:29.940: INFO: Pod "pod-subpath-test-projected-rmlg": Phase="Running", Reason="", readiness=true. Elapsed: 18.041703484s
Nov 25 07:13:31.943: INFO: Pod "pod-subpath-test-projected-rmlg": Phase="Running", Reason="", readiness=true. Elapsed: 20.044391884s
Nov 25 07:13:33.945: INFO: Pod "pod-subpath-test-projected-rmlg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.047106665s
STEP: Saw pod success
Nov 25 07:13:33.945: INFO: Pod "pod-subpath-test-projected-rmlg" satisfied condition "Succeeded or Failed"
Nov 25 07:13:33.947: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-projected-rmlg container test-container-subpath-projected-rmlg: <nil>
STEP: delete the pod
Nov 25 07:13:33.960: INFO: Waiting for pod pod-subpath-test-projected-rmlg to disappear
Nov 25 07:13:33.961: INFO: Pod pod-subpath-test-projected-rmlg no longer exists
STEP: Deleting pod pod-subpath-test-projected-rmlg
Nov 25 07:13:33.961: INFO: Deleting pod "pod-subpath-test-projected-rmlg" in namespace "subpath-6289"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:13:33.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6289" for this suite.

• [SLOW TEST:22.101 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":303,"completed":119,"skipped":1829,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:13:33.969: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Nov 25 07:13:34.009: INFO: Waiting up to 5m0s for pod "var-expansion-b5749e33-13dd-41dc-b10a-a643db135d3d" in namespace "var-expansion-2013" to be "Succeeded or Failed"
Nov 25 07:13:34.011: INFO: Pod "var-expansion-b5749e33-13dd-41dc-b10a-a643db135d3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.197613ms
Nov 25 07:13:36.014: INFO: Pod "var-expansion-b5749e33-13dd-41dc-b10a-a643db135d3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004735257s
STEP: Saw pod success
Nov 25 07:13:36.014: INFO: Pod "var-expansion-b5749e33-13dd-41dc-b10a-a643db135d3d" satisfied condition "Succeeded or Failed"
Nov 25 07:13:36.016: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-b5749e33-13dd-41dc-b10a-a643db135d3d container dapi-container: <nil>
STEP: delete the pod
Nov 25 07:13:36.031: INFO: Waiting for pod var-expansion-b5749e33-13dd-41dc-b10a-a643db135d3d to disappear
Nov 25 07:13:36.033: INFO: Pod var-expansion-b5749e33-13dd-41dc-b10a-a643db135d3d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:13:36.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2013" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":303,"completed":120,"skipped":1838,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:13:36.037: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4327
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4327
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4327
Nov 25 07:13:36.069: INFO: Found 0 stateful pods, waiting for 1
Nov 25 07:13:46.072: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov 25 07:13:46.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-4327 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 25 07:13:46.281: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 25 07:13:46.281: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 25 07:13:46.281: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 25 07:13:46.283: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 25 07:13:56.286: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 25 07:13:56.286: INFO: Waiting for statefulset status.replicas updated to 0
Nov 25 07:13:56.295: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999757s
Nov 25 07:13:57.298: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997881287s
Nov 25 07:13:58.302: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99455933s
Nov 25 07:13:59.304: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991401804s
Nov 25 07:14:00.306: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989158672s
Nov 25 07:14:01.309: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.986443121s
Nov 25 07:14:02.312: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.984064207s
Nov 25 07:14:03.315: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.981037937s
Nov 25 07:14:04.318: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.977899083s
Nov 25 07:14:05.322: INFO: Verifying statefulset ss doesn't scale past 1 for another 974.953038ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4327
Nov 25 07:14:06.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-4327 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:14:06.470: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 25 07:14:06.470: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 25 07:14:06.470: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 25 07:14:06.472: INFO: Found 1 stateful pods, waiting for 3
Nov 25 07:14:16.476: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 25 07:14:16.476: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 25 07:14:16.476: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov 25 07:14:16.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-4327 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 25 07:14:16.644: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 25 07:14:16.644: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 25 07:14:16.644: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 25 07:14:16.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-4327 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 25 07:14:16.833: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 25 07:14:16.833: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 25 07:14:16.833: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 25 07:14:16.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-4327 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 25 07:14:16.996: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 25 07:14:16.997: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 25 07:14:16.997: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 25 07:14:16.997: INFO: Waiting for statefulset status.replicas updated to 0
Nov 25 07:14:16.999: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Nov 25 07:14:27.004: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 25 07:14:27.004: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 25 07:14:27.004: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 25 07:14:27.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999736s
Nov 25 07:14:28.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.925609777s
Nov 25 07:14:29.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.922181621s
Nov 25 07:14:30.093: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.918244543s
Nov 25 07:14:31.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.915383094s
Nov 25 07:14:32.300: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.711805392s
Nov 25 07:14:33.303: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.708472672s
Nov 25 07:14:34.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.705270444s
Nov 25 07:14:35.309: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.702052836s
Nov 25 07:14:36.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 699.023677ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4327
Nov 25 07:14:37.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-4327 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:14:37.453: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 25 07:14:37.453: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 25 07:14:37.453: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 25 07:14:37.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-4327 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:14:37.665: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 25 07:14:37.665: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 25 07:14:37.665: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 25 07:14:37.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-4327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:14:37.821: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 25 07:14:37.821: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 25 07:14:37.821: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 25 07:14:37.821: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 25 07:14:57.832: INFO: Deleting all statefulset in ns statefulset-4327
Nov 25 07:14:57.834: INFO: Scaling statefulset ss to 0
Nov 25 07:14:57.840: INFO: Waiting for statefulset status.replicas updated to 0
Nov 25 07:14:57.842: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:14:57.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4327" for this suite.

• [SLOW TEST:81.817 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":303,"completed":121,"skipped":1851,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:14:57.854: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:14:58.335: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 25 07:15:00.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741885298, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741885298, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741885298, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741885298, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:15:03.383: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:15:03.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2780" for this suite.
STEP: Destroying namespace "webhook-2780-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.592 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":303,"completed":122,"skipped":1866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:15:03.447: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:15:04.130: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 25 07:15:06.137: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741885304, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741885304, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741885304, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741885304, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:15:09.151: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:15:09.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5391" for this suite.
STEP: Destroying namespace "webhook-5391-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.939 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":303,"completed":123,"skipped":1909,"failed":0}
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:15:09.387: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Nov 25 07:15:12.021: INFO: Successfully updated pod "adopt-release-9kqqt"
STEP: Checking that the Job readopts the Pod
Nov 25 07:15:12.021: INFO: Waiting up to 15m0s for pod "adopt-release-9kqqt" in namespace "job-9995" to be "adopted"
Nov 25 07:15:12.027: INFO: Pod "adopt-release-9kqqt": Phase="Running", Reason="", readiness=true. Elapsed: 5.353494ms
Nov 25 07:15:14.030: INFO: Pod "adopt-release-9kqqt": Phase="Running", Reason="", readiness=true. Elapsed: 2.008239782s
Nov 25 07:15:14.030: INFO: Pod "adopt-release-9kqqt" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Nov 25 07:15:14.536: INFO: Successfully updated pod "adopt-release-9kqqt"
STEP: Checking that the Job releases the Pod
Nov 25 07:15:14.536: INFO: Waiting up to 15m0s for pod "adopt-release-9kqqt" in namespace "job-9995" to be "released"
Nov 25 07:15:14.540: INFO: Pod "adopt-release-9kqqt": Phase="Running", Reason="", readiness=true. Elapsed: 4.353979ms
Nov 25 07:15:16.547: INFO: Pod "adopt-release-9kqqt": Phase="Running", Reason="", readiness=true. Elapsed: 2.011731538s
Nov 25 07:15:16.547: INFO: Pod "adopt-release-9kqqt" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:15:16.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9995" for this suite.

• [SLOW TEST:7.166 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":303,"completed":124,"skipped":1909,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:15:16.553: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Nov 25 07:15:16.573: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:15:31.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3682" for this suite.

• [SLOW TEST:15.332 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":303,"completed":125,"skipped":1909,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:15:31.886: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Nov 25 07:15:33.923: INFO: Pod pod-hostip-08472515-c43f-4806-b464-9fe9517e81c0 has hostIP: 192.168.49.3
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:15:33.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9913" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":303,"completed":126,"skipped":1997,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:15:33.928: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Nov 25 07:15:33.946: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Nov 25 07:15:45.520: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:15:47.351: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:15:57.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6293" for this suite.

• [SLOW TEST:23.586 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":303,"completed":127,"skipped":2011,"failed":0}
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:15:57.514: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-26f23300-e92e-4a64-8833-9ddfbbb309cf in namespace container-probe-7624
Nov 25 07:15:59.540: INFO: Started pod liveness-26f23300-e92e-4a64-8833-9ddfbbb309cf in namespace container-probe-7624
STEP: checking the pod's current state and verifying that restartCount is present
Nov 25 07:15:59.541: INFO: Initial restart count of pod liveness-26f23300-e92e-4a64-8833-9ddfbbb309cf is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:19:59.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7624" for this suite.

• [SLOW TEST:242.429 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":303,"completed":128,"skipped":2011,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:19:59.943: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-6332
STEP: creating replication controller nodeport-test in namespace services-6332
I1125 07:20:00.576273      23 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-6332, replica count: 2
I1125 07:20:03.626895      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 25 07:20:03.627: INFO: Creating new exec pod
Nov 25 07:20:06.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6332 execpodknmvb -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Nov 25 07:20:07.440: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 25 07:20:07.440: INFO: stdout: ""
Nov 25 07:20:07.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6332 execpodknmvb -- /bin/sh -x -c nc -zv -t -w 2 10.105.54.111 80'
Nov 25 07:20:07.574: INFO: stderr: "+ nc -zv -t -w 2 10.105.54.111 80\nConnection to 10.105.54.111 80 port [tcp/http] succeeded!\n"
Nov 25 07:20:07.574: INFO: stdout: ""
Nov 25 07:20:07.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6332 execpodknmvb -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.2 31811'
Nov 25 07:20:07.717: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.2 31811\nConnection to 192.168.49.2 31811 port [tcp/31811] succeeded!\n"
Nov 25 07:20:07.717: INFO: stdout: ""
Nov 25 07:20:07.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6332 execpodknmvb -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.3 31811'
Nov 25 07:20:07.861: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.3 31811\nConnection to 192.168.49.3 31811 port [tcp/31811] succeeded!\n"
Nov 25 07:20:07.861: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:20:07.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6332" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.925 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":303,"completed":129,"skipped":2012,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:20:07.868: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-b871caed-1055-41fd-9e9d-02806e409042 in namespace container-probe-8426
Nov 25 07:20:09.993: INFO: Started pod test-webserver-b871caed-1055-41fd-9e9d-02806e409042 in namespace container-probe-8426
STEP: checking the pod's current state and verifying that restartCount is present
Nov 25 07:20:09.994: INFO: Initial restart count of pod test-webserver-b871caed-1055-41fd-9e9d-02806e409042 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:24:10.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8426" for this suite.

• [SLOW TEST:242.619 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":303,"completed":130,"skipped":2048,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:24:10.487: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-3715
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3715 to expose endpoints map[]
Nov 25 07:24:10.627: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Nov 25 07:24:11.634: INFO: successfully validated that service endpoint-test2 in namespace services-3715 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3715
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3715 to expose endpoints map[pod1:[80]]
Nov 25 07:24:14.686: INFO: successfully validated that service endpoint-test2 in namespace services-3715 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-3715
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3715 to expose endpoints map[pod1:[80] pod2:[80]]
Nov 25 07:24:16.744: INFO: successfully validated that service endpoint-test2 in namespace services-3715 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-3715
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3715 to expose endpoints map[pod2:[80]]
Nov 25 07:24:16.759: INFO: successfully validated that service endpoint-test2 in namespace services-3715 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-3715
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3715 to expose endpoints map[]
Nov 25 07:24:17.788: INFO: successfully validated that service endpoint-test2 in namespace services-3715 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:24:17.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3715" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.380 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":303,"completed":131,"skipped":2052,"failed":0}
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:24:17.867: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:24:17.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6558" for this suite.
STEP: Destroying namespace "nspatchtest-a1e11ba6-476c-4209-86a7-fa80f3afbe36-5246" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":303,"completed":132,"skipped":2052,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:24:17.928: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 25 07:24:17.955: INFO: Waiting up to 5m0s for pod "pod-0b7c4dce-384a-4dd7-a3ad-a69ef33567c5" in namespace "emptydir-5298" to be "Succeeded or Failed"
Nov 25 07:24:17.960: INFO: Pod "pod-0b7c4dce-384a-4dd7-a3ad-a69ef33567c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.671354ms
Nov 25 07:24:19.964: INFO: Pod "pod-0b7c4dce-384a-4dd7-a3ad-a69ef33567c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008205381s
STEP: Saw pod success
Nov 25 07:24:19.964: INFO: Pod "pod-0b7c4dce-384a-4dd7-a3ad-a69ef33567c5" satisfied condition "Succeeded or Failed"
Nov 25 07:24:19.966: INFO: Trying to get logs from node k8sconformance-m02 pod pod-0b7c4dce-384a-4dd7-a3ad-a69ef33567c5 container test-container: <nil>
STEP: delete the pod
Nov 25 07:24:19.988: INFO: Waiting for pod pod-0b7c4dce-384a-4dd7-a3ad-a69ef33567c5 to disappear
Nov 25 07:24:19.993: INFO: Pod pod-0b7c4dce-384a-4dd7-a3ad-a69ef33567c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:24:19.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5298" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":133,"skipped":2080,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:24:20.000: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-2a22d397-234e-4cef-b6e2-2bef708fd24b
STEP: Creating a pod to test consume configMaps
Nov 25 07:24:20.048: INFO: Waiting up to 5m0s for pod "pod-configmaps-7440f551-51a3-4d49-bfcf-8d447ff1b6f2" in namespace "configmap-3967" to be "Succeeded or Failed"
Nov 25 07:24:20.050: INFO: Pod "pod-configmaps-7440f551-51a3-4d49-bfcf-8d447ff1b6f2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.686448ms
Nov 25 07:24:22.053: INFO: Pod "pod-configmaps-7440f551-51a3-4d49-bfcf-8d447ff1b6f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00485332s
STEP: Saw pod success
Nov 25 07:24:22.053: INFO: Pod "pod-configmaps-7440f551-51a3-4d49-bfcf-8d447ff1b6f2" satisfied condition "Succeeded or Failed"
Nov 25 07:24:22.055: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-7440f551-51a3-4d49-bfcf-8d447ff1b6f2 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 25 07:24:22.066: INFO: Waiting for pod pod-configmaps-7440f551-51a3-4d49-bfcf-8d447ff1b6f2 to disappear
Nov 25 07:24:22.067: INFO: Pod pod-configmaps-7440f551-51a3-4d49-bfcf-8d447ff1b6f2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:24:22.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3967" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":134,"skipped":2121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:24:22.073: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov 25 07:24:22.103: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 25 07:25:22.116: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Nov 25 07:25:22.146: INFO: Created pod: pod0-sched-preemption-low-priority
Nov 25 07:25:22.155: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:25:40.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1320" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:78.152 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":303,"completed":135,"skipped":2143,"failed":0}
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:25:40.226: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Nov 25 07:25:40.314: INFO: Created pod &Pod{ObjectMeta:{dns-4420  dns-4420 /api/v1/namespaces/dns-4420/pods/dns-4420 a3d203a3-4601-4ae9-98df-158a32b40890 9544 0 2020-11-25 07:25:40 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2020-11-25 07:25:40 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9krhp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9krhp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9krhp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:25:40.316: INFO: The status of Pod dns-4420 is Pending, waiting for it to be Running (with Ready = true)
Nov 25 07:25:42.318: INFO: The status of Pod dns-4420 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Nov 25 07:25:42.318: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4420 PodName:dns-4420 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:25:42.318: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Verifying customized DNS server is configured on pod...
Nov 25 07:25:42.405: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4420 PodName:dns-4420 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:25:42.405: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:25:42.484: INFO: Deleting pod dns-4420...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:25:42.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4420" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":303,"completed":136,"skipped":2143,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:25:42.501: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-2194cf41-46d9-4381-a4b2-ab156f3eb2b0
STEP: Creating a pod to test consume configMaps
Nov 25 07:25:42.533: INFO: Waiting up to 5m0s for pod "pod-configmaps-0213b416-e98e-4c3e-8994-2103deb5416b" in namespace "configmap-502" to be "Succeeded or Failed"
Nov 25 07:25:42.633: INFO: Pod "pod-configmaps-0213b416-e98e-4c3e-8994-2103deb5416b": Phase="Pending", Reason="", readiness=false. Elapsed: 99.77311ms
Nov 25 07:25:44.640: INFO: Pod "pod-configmaps-0213b416-e98e-4c3e-8994-2103deb5416b": Phase="Running", Reason="", readiness=true. Elapsed: 2.106392539s
Nov 25 07:25:46.642: INFO: Pod "pod-configmaps-0213b416-e98e-4c3e-8994-2103deb5416b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.108963697s
STEP: Saw pod success
Nov 25 07:25:46.643: INFO: Pod "pod-configmaps-0213b416-e98e-4c3e-8994-2103deb5416b" satisfied condition "Succeeded or Failed"
Nov 25 07:25:46.645: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-0213b416-e98e-4c3e-8994-2103deb5416b container configmap-volume-test: <nil>
STEP: delete the pod
Nov 25 07:25:46.661: INFO: Waiting for pod pod-configmaps-0213b416-e98e-4c3e-8994-2103deb5416b to disappear
Nov 25 07:25:46.663: INFO: Pod pod-configmaps-0213b416-e98e-4c3e-8994-2103deb5416b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:25:46.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-502" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":303,"completed":137,"skipped":2161,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:25:46.668: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 25 07:25:46.722: INFO: Waiting up to 5m0s for pod "downward-api-0ec38f8c-1b15-4cc6-9b78-27c5edf486f6" in namespace "downward-api-7561" to be "Succeeded or Failed"
Nov 25 07:25:46.724: INFO: Pod "downward-api-0ec38f8c-1b15-4cc6-9b78-27c5edf486f6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.672197ms
Nov 25 07:25:48.727: INFO: Pod "downward-api-0ec38f8c-1b15-4cc6-9b78-27c5edf486f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004908476s
STEP: Saw pod success
Nov 25 07:25:48.727: INFO: Pod "downward-api-0ec38f8c-1b15-4cc6-9b78-27c5edf486f6" satisfied condition "Succeeded or Failed"
Nov 25 07:25:48.729: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-0ec38f8c-1b15-4cc6-9b78-27c5edf486f6 container dapi-container: <nil>
STEP: delete the pod
Nov 25 07:25:48.744: INFO: Waiting for pod downward-api-0ec38f8c-1b15-4cc6-9b78-27c5edf486f6 to disappear
Nov 25 07:25:48.746: INFO: Pod downward-api-0ec38f8c-1b15-4cc6-9b78-27c5edf486f6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:25:48.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7561" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":303,"completed":138,"skipped":2185,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:25:48.751: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 25 07:25:50.782: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:25:50.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3709" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":139,"skipped":2223,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:25:50.800: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-81bb35f7-9e59-429d-9966-551def4a0f9d
STEP: Creating secret with name s-test-opt-upd-e927940d-3dad-45ac-80fa-e7665e7fbf8c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-81bb35f7-9e59-429d-9966-551def4a0f9d
STEP: Updating secret s-test-opt-upd-e927940d-3dad-45ac-80fa-e7665e7fbf8c
STEP: Creating secret with name s-test-opt-create-027849db-f957-4f2d-8406-79e3fdc7a22c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:27:25.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2948" for this suite.

• [SLOW TEST:94.328 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":140,"skipped":2224,"failed":0}
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:27:25.128: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 25 07:27:27.679: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a8b17eef-fcf5-4847-9aa1-9757523889d3"
Nov 25 07:27:27.679: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a8b17eef-fcf5-4847-9aa1-9757523889d3" in namespace "pods-6414" to be "terminated due to deadline exceeded"
Nov 25 07:27:27.684: INFO: Pod "pod-update-activedeadlineseconds-a8b17eef-fcf5-4847-9aa1-9757523889d3": Phase="Running", Reason="", readiness=true. Elapsed: 4.123872ms
Nov 25 07:27:29.686: INFO: Pod "pod-update-activedeadlineseconds-a8b17eef-fcf5-4847-9aa1-9757523889d3": Phase="Running", Reason="", readiness=true. Elapsed: 2.00674983s
Nov 25 07:27:31.689: INFO: Pod "pod-update-activedeadlineseconds-a8b17eef-fcf5-4847-9aa1-9757523889d3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.009836268s
Nov 25 07:27:31.689: INFO: Pod "pod-update-activedeadlineseconds-a8b17eef-fcf5-4847-9aa1-9757523889d3" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:27:31.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6414" for this suite.

• [SLOW TEST:6.567 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":303,"completed":141,"skipped":2224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:27:31.696: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-b2cd76ae-6e9e-4679-97af-8db17dc4a714 in namespace container-probe-4039
Nov 25 07:27:33.743: INFO: Started pod liveness-b2cd76ae-6e9e-4679-97af-8db17dc4a714 in namespace container-probe-4039
STEP: checking the pod's current state and verifying that restartCount is present
Nov 25 07:27:33.745: INFO: Initial restart count of pod liveness-b2cd76ae-6e9e-4679-97af-8db17dc4a714 is 0
Nov 25 07:27:57.780: INFO: Restart count of pod container-probe-4039/liveness-b2cd76ae-6e9e-4679-97af-8db17dc4a714 is now 1 (24.035358648s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:27:57.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4039" for this suite.

• [SLOW TEST:26.100 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":303,"completed":142,"skipped":2272,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:27:57.796: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 07:27:57.890: INFO: Waiting up to 5m0s for pod "downwardapi-volume-759d3733-a5d8-4ba6-b140-3050a3ed97f3" in namespace "downward-api-8430" to be "Succeeded or Failed"
Nov 25 07:27:57.892: INFO: Pod "downwardapi-volume-759d3733-a5d8-4ba6-b140-3050a3ed97f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.217284ms
Nov 25 07:27:59.894: INFO: Pod "downwardapi-volume-759d3733-a5d8-4ba6-b140-3050a3ed97f3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004632346s
Nov 25 07:28:01.897: INFO: Pod "downwardapi-volume-759d3733-a5d8-4ba6-b140-3050a3ed97f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007630376s
STEP: Saw pod success
Nov 25 07:28:01.897: INFO: Pod "downwardapi-volume-759d3733-a5d8-4ba6-b140-3050a3ed97f3" satisfied condition "Succeeded or Failed"
Nov 25 07:28:01.899: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-759d3733-a5d8-4ba6-b140-3050a3ed97f3 container client-container: <nil>
STEP: delete the pod
Nov 25 07:28:01.914: INFO: Waiting for pod downwardapi-volume-759d3733-a5d8-4ba6-b140-3050a3ed97f3 to disappear
Nov 25 07:28:01.916: INFO: Pod downwardapi-volume-759d3733-a5d8-4ba6-b140-3050a3ed97f3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:28:01.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8430" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":303,"completed":143,"skipped":2285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:28:01.921: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-79ee3a91-6ed8-459e-8e0f-7ed6114c32db
STEP: Creating a pod to test consume configMaps
Nov 25 07:28:01.949: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-22663fbd-0d13-4eb0-95f1-6b198dcdc57b" in namespace "projected-2767" to be "Succeeded or Failed"
Nov 25 07:28:01.956: INFO: Pod "pod-projected-configmaps-22663fbd-0d13-4eb0-95f1-6b198dcdc57b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.676339ms
Nov 25 07:28:03.958: INFO: Pod "pod-projected-configmaps-22663fbd-0d13-4eb0-95f1-6b198dcdc57b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009188306s
Nov 25 07:28:05.962: INFO: Pod "pod-projected-configmaps-22663fbd-0d13-4eb0-95f1-6b198dcdc57b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012671828s
STEP: Saw pod success
Nov 25 07:28:05.962: INFO: Pod "pod-projected-configmaps-22663fbd-0d13-4eb0-95f1-6b198dcdc57b" satisfied condition "Succeeded or Failed"
Nov 25 07:28:05.964: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-22663fbd-0d13-4eb0-95f1-6b198dcdc57b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 25 07:28:05.978: INFO: Waiting for pod pod-projected-configmaps-22663fbd-0d13-4eb0-95f1-6b198dcdc57b to disappear
Nov 25 07:28:05.980: INFO: Pod pod-projected-configmaps-22663fbd-0d13-4eb0-95f1-6b198dcdc57b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:28:05.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2767" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":303,"completed":144,"skipped":2341,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:28:05.985: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:28:06.005: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:28:07.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3311" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":303,"completed":145,"skipped":2346,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:28:07.136: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6230
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-6230
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6230
Nov 25 07:28:07.169: INFO: Found 0 stateful pods, waiting for 1
Nov 25 07:28:17.172: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov 25 07:28:17.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 25 07:28:17.423: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 25 07:28:17.423: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 25 07:28:17.423: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 25 07:28:17.426: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 25 07:28:27.430: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 25 07:28:27.430: INFO: Waiting for statefulset status.replicas updated to 0
Nov 25 07:28:27.444: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 25 07:28:27.444: INFO: ss-0  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:07 +0000 UTC  }]
Nov 25 07:28:27.444: INFO: 
Nov 25 07:28:27.444: INFO: StatefulSet ss has not reached scale 3, at 1
Nov 25 07:28:28.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993995069s
Nov 25 07:28:29.451: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990549128s
Nov 25 07:28:30.455: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98676347s
Nov 25 07:28:31.459: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982815924s
Nov 25 07:28:32.463: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97920454s
Nov 25 07:28:33.466: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975390681s
Nov 25 07:28:34.470: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971850134s
Nov 25 07:28:35.473: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96829166s
Nov 25 07:28:36.477: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.548121ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6230
Nov 25 07:28:37.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:28:37.622: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 25 07:28:37.622: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 25 07:28:37.622: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 25 07:28:37.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:28:37.799: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 25 07:28:37.799: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 25 07:28:37.799: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 25 07:28:37.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:28:37.985: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 25 07:28:37.985: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 25 07:28:37.985: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 25 07:28:37.988: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 25 07:28:37.988: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 25 07:28:37.988: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov 25 07:28:37.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 25 07:28:38.134: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 25 07:28:38.134: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 25 07:28:38.134: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 25 07:28:38.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 25 07:28:38.320: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 25 07:28:38.320: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 25 07:28:38.320: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 25 07:28:38.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 25 07:28:38.475: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 25 07:28:38.475: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 25 07:28:38.475: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 25 07:28:38.475: INFO: Waiting for statefulset status.replicas updated to 0
Nov 25 07:28:38.477: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Nov 25 07:28:48.483: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 25 07:28:48.483: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 25 07:28:48.483: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 25 07:28:48.490: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 25 07:28:48.490: INFO: ss-0  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:07 +0000 UTC  }]
Nov 25 07:28:48.490: INFO: ss-1  k8sconformance      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  }]
Nov 25 07:28:48.490: INFO: ss-2  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  }]
Nov 25 07:28:48.490: INFO: 
Nov 25 07:28:48.490: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 25 07:28:49.493: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 25 07:28:49.493: INFO: ss-0  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:07 +0000 UTC  }]
Nov 25 07:28:49.493: INFO: ss-1  k8sconformance      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  }]
Nov 25 07:28:49.493: INFO: ss-2  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  }]
Nov 25 07:28:49.493: INFO: 
Nov 25 07:28:49.493: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 25 07:28:50.497: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 25 07:28:50.497: INFO: ss-0  k8sconformance-m02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:07 +0000 UTC  }]
Nov 25 07:28:50.497: INFO: ss-1  k8sconformance      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  }]
Nov 25 07:28:50.497: INFO: ss-2  k8sconformance-m02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  }]
Nov 25 07:28:50.497: INFO: 
Nov 25 07:28:50.497: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 25 07:28:51.500: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 25 07:28:51.500: INFO: ss-2  k8sconformance-m02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  }]
Nov 25 07:28:51.500: INFO: 
Nov 25 07:28:51.500: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 25 07:28:52.503: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 25 07:28:52.503: INFO: ss-2  k8sconformance-m02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  }]
Nov 25 07:28:52.503: INFO: 
Nov 25 07:28:52.503: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 25 07:28:53.506: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 25 07:28:53.506: INFO: ss-2  k8sconformance-m02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  }]
Nov 25 07:28:53.506: INFO: 
Nov 25 07:28:53.506: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 25 07:28:54.509: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 25 07:28:54.509: INFO: ss-2  k8sconformance-m02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  }]
Nov 25 07:28:54.509: INFO: 
Nov 25 07:28:54.509: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 25 07:28:55.512: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 25 07:28:55.512: INFO: ss-2  k8sconformance-m02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  }]
Nov 25 07:28:55.512: INFO: 
Nov 25 07:28:55.512: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 25 07:28:56.516: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 25 07:28:56.516: INFO: ss-2  k8sconformance-m02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  }]
Nov 25 07:28:56.516: INFO: 
Nov 25 07:28:56.516: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 25 07:28:57.519: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 25 07:28:57.519: INFO: ss-2  k8sconformance-m02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-25 07:28:27 +0000 UTC  }]
Nov 25 07:28:57.519: INFO: 
Nov 25 07:28:57.519: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6230
Nov 25 07:28:58.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:28:58.612: INFO: rc: 1
Nov 25 07:28:58.612: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 25 07:29:08.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:29:08.684: INFO: rc: 1
Nov 25 07:29:08.684: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:29:18.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:29:18.755: INFO: rc: 1
Nov 25 07:29:18.755: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:29:28.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:29:28.824: INFO: rc: 1
Nov 25 07:29:28.824: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:29:38.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:29:38.892: INFO: rc: 1
Nov 25 07:29:38.892: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:29:48.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:29:48.962: INFO: rc: 1
Nov 25 07:29:48.962: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:29:58.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:29:59.034: INFO: rc: 1
Nov 25 07:29:59.034: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:30:09.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:30:09.690: INFO: rc: 1
Nov 25 07:30:09.690: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:30:19.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:30:19.759: INFO: rc: 1
Nov 25 07:30:19.759: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:30:29.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:30:29.832: INFO: rc: 1
Nov 25 07:30:29.832: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:30:39.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:30:39.901: INFO: rc: 1
Nov 25 07:30:39.902: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:30:49.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:30:49.971: INFO: rc: 1
Nov 25 07:30:49.971: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:30:59.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:31:00.041: INFO: rc: 1
Nov 25 07:31:00.041: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:31:10.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:31:10.114: INFO: rc: 1
Nov 25 07:31:10.114: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:31:20.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:31:20.186: INFO: rc: 1
Nov 25 07:31:20.186: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:31:30.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:31:30.255: INFO: rc: 1
Nov 25 07:31:30.255: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:31:40.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:31:40.322: INFO: rc: 1
Nov 25 07:31:40.323: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:31:50.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:31:50.391: INFO: rc: 1
Nov 25 07:31:50.391: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:32:00.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:32:00.461: INFO: rc: 1
Nov 25 07:32:00.461: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:32:10.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:32:10.530: INFO: rc: 1
Nov 25 07:32:10.530: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:32:20.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:32:20.599: INFO: rc: 1
Nov 25 07:32:20.599: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:32:30.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:32:30.669: INFO: rc: 1
Nov 25 07:32:30.669: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:32:40.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:32:40.767: INFO: rc: 1
Nov 25 07:32:40.767: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:32:50.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:32:50.835: INFO: rc: 1
Nov 25 07:32:50.835: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:33:00.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:33:00.904: INFO: rc: 1
Nov 25 07:33:00.904: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:33:10.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:33:10.971: INFO: rc: 1
Nov 25 07:33:10.971: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:33:20.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:33:21.042: INFO: rc: 1
Nov 25 07:33:21.042: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:33:31.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:33:31.111: INFO: rc: 1
Nov 25 07:33:31.111: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:33:41.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:33:41.181: INFO: rc: 1
Nov 25 07:33:41.181: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:33:51.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:33:51.251: INFO: rc: 1
Nov 25 07:33:51.251: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 25 07:34:01.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=statefulset-6230 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 25 07:34:01.319: INFO: rc: 1
Nov 25 07:34:01.319: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Nov 25 07:34:01.319: INFO: Scaling statefulset ss to 0
Nov 25 07:34:01.325: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 25 07:34:01.327: INFO: Deleting all statefulset in ns statefulset-6230
Nov 25 07:34:01.328: INFO: Scaling statefulset ss to 0
Nov 25 07:34:01.337: INFO: Waiting for statefulset status.replicas updated to 0
Nov 25 07:34:01.338: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:34:01.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6230" for this suite.

• [SLOW TEST:354.213 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":303,"completed":146,"skipped":2348,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:34:01.350: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:34:12.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3320" for this suite.

• [SLOW TEST:11.074 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":303,"completed":147,"skipped":2382,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:34:12.424: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-b12cf259-577c-42f5-9cb6-6d829f34a88d
STEP: Creating a pod to test consume secrets
Nov 25 07:34:12.509: INFO: Waiting up to 5m0s for pod "pod-secrets-a5a4e109-7044-4c93-ba69-66f4e1447e75" in namespace "secrets-473" to be "Succeeded or Failed"
Nov 25 07:34:12.513: INFO: Pod "pod-secrets-a5a4e109-7044-4c93-ba69-66f4e1447e75": Phase="Pending", Reason="", readiness=false. Elapsed: 3.901437ms
Nov 25 07:34:14.516: INFO: Pod "pod-secrets-a5a4e109-7044-4c93-ba69-66f4e1447e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00679595s
Nov 25 07:34:16.519: INFO: Pod "pod-secrets-a5a4e109-7044-4c93-ba69-66f4e1447e75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009704894s
STEP: Saw pod success
Nov 25 07:34:16.519: INFO: Pod "pod-secrets-a5a4e109-7044-4c93-ba69-66f4e1447e75" satisfied condition "Succeeded or Failed"
Nov 25 07:34:16.521: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-a5a4e109-7044-4c93-ba69-66f4e1447e75 container secret-volume-test: <nil>
STEP: delete the pod
Nov 25 07:34:16.543: INFO: Waiting for pod pod-secrets-a5a4e109-7044-4c93-ba69-66f4e1447e75 to disappear
Nov 25 07:34:16.545: INFO: Pod pod-secrets-a5a4e109-7044-4c93-ba69-66f4e1447e75 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:34:16.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-473" for this suite.
STEP: Destroying namespace "secret-namespace-4687" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":303,"completed":148,"skipped":2386,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:34:16.552: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:34:16.579: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov 25 07:34:16.588: INFO: Number of nodes with available pods: 0
Nov 25 07:34:16.588: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:34:17.676: INFO: Number of nodes with available pods: 0
Nov 25 07:34:17.676: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:34:18.594: INFO: Number of nodes with available pods: 1
Nov 25 07:34:18.594: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 25 07:34:19.594: INFO: Number of nodes with available pods: 2
Nov 25 07:34:19.594: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov 25 07:34:19.614: INFO: Wrong image for pod: daemon-set-cqhjq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:19.615: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:20.624: INFO: Wrong image for pod: daemon-set-cqhjq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:20.624: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:21.623: INFO: Wrong image for pod: daemon-set-cqhjq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:21.623: INFO: Pod daemon-set-cqhjq is not available
Nov 25 07:34:21.623: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:22.624: INFO: Wrong image for pod: daemon-set-cqhjq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:22.624: INFO: Pod daemon-set-cqhjq is not available
Nov 25 07:34:22.624: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:23.624: INFO: Wrong image for pod: daemon-set-cqhjq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:23.624: INFO: Pod daemon-set-cqhjq is not available
Nov 25 07:34:23.624: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:24.624: INFO: Wrong image for pod: daemon-set-cqhjq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:24.624: INFO: Pod daemon-set-cqhjq is not available
Nov 25 07:34:24.624: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:25.624: INFO: Wrong image for pod: daemon-set-cqhjq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:25.624: INFO: Pod daemon-set-cqhjq is not available
Nov 25 07:34:25.624: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:26.624: INFO: Wrong image for pod: daemon-set-cqhjq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:26.624: INFO: Pod daemon-set-cqhjq is not available
Nov 25 07:34:26.624: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:27.624: INFO: Wrong image for pod: daemon-set-cqhjq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:27.624: INFO: Pod daemon-set-cqhjq is not available
Nov 25 07:34:27.624: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:28.624: INFO: Wrong image for pod: daemon-set-cqhjq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:28.624: INFO: Pod daemon-set-cqhjq is not available
Nov 25 07:34:28.624: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:29.627: INFO: Wrong image for pod: daemon-set-cqhjq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:29.627: INFO: Pod daemon-set-cqhjq is not available
Nov 25 07:34:29.627: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:30.624: INFO: Wrong image for pod: daemon-set-cqhjq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:30.624: INFO: Pod daemon-set-cqhjq is not available
Nov 25 07:34:30.624: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:31.627: INFO: Pod daemon-set-frh5m is not available
Nov 25 07:34:31.627: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:32.624: INFO: Pod daemon-set-frh5m is not available
Nov 25 07:34:32.624: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:33.624: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:33.624: INFO: Pod daemon-set-srxbm is not available
Nov 25 07:34:34.623: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:34.623: INFO: Pod daemon-set-srxbm is not available
Nov 25 07:34:35.624: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:35.624: INFO: Pod daemon-set-srxbm is not available
Nov 25 07:34:36.624: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:36.624: INFO: Pod daemon-set-srxbm is not available
Nov 25 07:34:37.624: INFO: Wrong image for pod: daemon-set-srxbm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 25 07:34:37.624: INFO: Pod daemon-set-srxbm is not available
Nov 25 07:34:38.623: INFO: Pod daemon-set-2hd7z is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Nov 25 07:34:38.630: INFO: Number of nodes with available pods: 1
Nov 25 07:34:38.630: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:34:39.635: INFO: Number of nodes with available pods: 1
Nov 25 07:34:39.635: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:34:40.635: INFO: Number of nodes with available pods: 2
Nov 25 07:34:40.635: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2191, will wait for the garbage collector to delete the pods
Nov 25 07:34:40.701: INFO: Deleting DaemonSet.extensions daemon-set took: 4.361559ms
Nov 25 07:34:41.101: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.265982ms
Nov 25 07:34:48.604: INFO: Number of nodes with available pods: 0
Nov 25 07:34:48.604: INFO: Number of running nodes: 0, number of available pods: 0
Nov 25 07:34:48.605: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2191/daemonsets","resourceVersion":"10714"},"items":null}

Nov 25 07:34:48.607: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2191/pods","resourceVersion":"10714"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:34:48.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2191" for this suite.

• [SLOW TEST:32.066 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":303,"completed":149,"skipped":2435,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:34:48.618: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-42b42595-e126-4f8e-96c6-78d1561ff5e1
STEP: Creating a pod to test consume secrets
Nov 25 07:34:48.659: INFO: Waiting up to 5m0s for pod "pod-secrets-7e304f13-442a-4027-8ffa-2f9d0cf23aa4" in namespace "secrets-5086" to be "Succeeded or Failed"
Nov 25 07:34:48.661: INFO: Pod "pod-secrets-7e304f13-442a-4027-8ffa-2f9d0cf23aa4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.590852ms
Nov 25 07:34:50.664: INFO: Pod "pod-secrets-7e304f13-442a-4027-8ffa-2f9d0cf23aa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004651465s
Nov 25 07:34:52.667: INFO: Pod "pod-secrets-7e304f13-442a-4027-8ffa-2f9d0cf23aa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007192717s
STEP: Saw pod success
Nov 25 07:34:52.667: INFO: Pod "pod-secrets-7e304f13-442a-4027-8ffa-2f9d0cf23aa4" satisfied condition "Succeeded or Failed"
Nov 25 07:34:52.669: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-7e304f13-442a-4027-8ffa-2f9d0cf23aa4 container secret-volume-test: <nil>
STEP: delete the pod
Nov 25 07:34:52.681: INFO: Waiting for pod pod-secrets-7e304f13-442a-4027-8ffa-2f9d0cf23aa4 to disappear
Nov 25 07:34:52.682: INFO: Pod pod-secrets-7e304f13-442a-4027-8ffa-2f9d0cf23aa4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:34:52.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5086" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":150,"skipped":2439,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:34:52.687: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-d7359482-9026-4d3d-86d4-2a6c33b07d2e
STEP: Creating a pod to test consume secrets
Nov 25 07:34:52.711: INFO: Waiting up to 5m0s for pod "pod-secrets-32dc008e-61a2-40b9-9e21-49fe46eb4adc" in namespace "secrets-7883" to be "Succeeded or Failed"
Nov 25 07:34:52.712: INFO: Pod "pod-secrets-32dc008e-61a2-40b9-9e21-49fe46eb4adc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.390276ms
Nov 25 07:34:54.727: INFO: Pod "pod-secrets-32dc008e-61a2-40b9-9e21-49fe46eb4adc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015555178s
STEP: Saw pod success
Nov 25 07:34:54.727: INFO: Pod "pod-secrets-32dc008e-61a2-40b9-9e21-49fe46eb4adc" satisfied condition "Succeeded or Failed"
Nov 25 07:34:54.729: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-32dc008e-61a2-40b9-9e21-49fe46eb4adc container secret-volume-test: <nil>
STEP: delete the pod
Nov 25 07:34:54.739: INFO: Waiting for pod pod-secrets-32dc008e-61a2-40b9-9e21-49fe46eb4adc to disappear
Nov 25 07:34:54.741: INFO: Pod pod-secrets-32dc008e-61a2-40b9-9e21-49fe46eb4adc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:34:54.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7883" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":151,"skipped":2461,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:34:54.746: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1125 07:35:04.814852      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 25 07:36:06.827: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Nov 25 07:36:06.827: INFO: Deleting pod "simpletest-rc-to-be-deleted-2m6fv" in namespace "gc-7999"
Nov 25 07:36:06.838: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jqh4" in namespace "gc-7999"
Nov 25 07:36:06.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xj7s" in namespace "gc-7999"
Nov 25 07:36:06.874: INFO: Deleting pod "simpletest-rc-to-be-deleted-k8wnz" in namespace "gc-7999"
Nov 25 07:36:06.886: INFO: Deleting pod "simpletest-rc-to-be-deleted-kjms2" in namespace "gc-7999"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:36:06.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7999" for this suite.

• [SLOW TEST:72.169 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":303,"completed":152,"skipped":2464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:36:06.916: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1318
STEP: creating service affinity-clusterip-transition in namespace services-1318
STEP: creating replication controller affinity-clusterip-transition in namespace services-1318
I1125 07:36:07.028576      23 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-1318, replica count: 3
I1125 07:36:10.079002      23 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 25 07:36:10.083: INFO: Creating new exec pod
Nov 25 07:36:13.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-1318 execpod-affinitydtv2d -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Nov 25 07:36:13.276: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Nov 25 07:36:13.276: INFO: stdout: ""
Nov 25 07:36:13.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-1318 execpod-affinitydtv2d -- /bin/sh -x -c nc -zv -t -w 2 10.98.133.26 80'
Nov 25 07:36:13.426: INFO: stderr: "+ nc -zv -t -w 2 10.98.133.26 80\nConnection to 10.98.133.26 80 port [tcp/http] succeeded!\n"
Nov 25 07:36:13.426: INFO: stdout: ""
Nov 25 07:36:13.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-1318 execpod-affinitydtv2d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.133.26:80/ ; done'
Nov 25 07:36:13.673: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n"
Nov 25 07:36:13.673: INFO: stdout: "\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-gsjqq\naffinity-clusterip-transition-xlvhq\naffinity-clusterip-transition-xlvhq\naffinity-clusterip-transition-gsjqq\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-xlvhq\naffinity-clusterip-transition-xlvhq\naffinity-clusterip-transition-xlvhq\naffinity-clusterip-transition-xlvhq\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-xlvhq\naffinity-clusterip-transition-gsjqq\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-xlvhq\naffinity-clusterip-transition-h85rm"
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-gsjqq
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-xlvhq
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-xlvhq
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-gsjqq
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-xlvhq
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-xlvhq
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-xlvhq
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-xlvhq
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-xlvhq
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-gsjqq
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-xlvhq
Nov 25 07:36:13.673: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-1318 execpod-affinitydtv2d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.133.26:80/ ; done'
Nov 25 07:36:13.888: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.133.26:80/\n"
Nov 25 07:36:13.888: INFO: stdout: "\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm\naffinity-clusterip-transition-h85rm"
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Received response from host: affinity-clusterip-transition-h85rm
Nov 25 07:36:13.888: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1318, will wait for the garbage collector to delete the pods
Nov 25 07:36:13.956: INFO: Deleting ReplicationController affinity-clusterip-transition took: 4.598412ms
Nov 25 07:36:14.356: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 400.323973ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:36:28.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1318" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:21.765 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":153,"skipped":2487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:36:28.680: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:36:36.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6955" for this suite.

• [SLOW TEST:8.051 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":303,"completed":154,"skipped":2509,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:36:36.731: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 07:36:36.765: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1618546d-188c-4a1e-a2ad-bab6a41d85a6" in namespace "projected-7339" to be "Succeeded or Failed"
Nov 25 07:36:36.770: INFO: Pod "downwardapi-volume-1618546d-188c-4a1e-a2ad-bab6a41d85a6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.108169ms
Nov 25 07:36:38.773: INFO: Pod "downwardapi-volume-1618546d-188c-4a1e-a2ad-bab6a41d85a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007995059s
STEP: Saw pod success
Nov 25 07:36:38.773: INFO: Pod "downwardapi-volume-1618546d-188c-4a1e-a2ad-bab6a41d85a6" satisfied condition "Succeeded or Failed"
Nov 25 07:36:38.775: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-1618546d-188c-4a1e-a2ad-bab6a41d85a6 container client-container: <nil>
STEP: delete the pod
Nov 25 07:36:38.794: INFO: Waiting for pod downwardapi-volume-1618546d-188c-4a1e-a2ad-bab6a41d85a6 to disappear
Nov 25 07:36:38.796: INFO: Pod downwardapi-volume-1618546d-188c-4a1e-a2ad-bab6a41d85a6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:36:38.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7339" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":303,"completed":155,"skipped":2512,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:36:38.801: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:36:38.825: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-c036d43d-3e80-4e1b-bb5c-338231c80cfb" in namespace "security-context-test-3860" to be "Succeeded or Failed"
Nov 25 07:36:38.826: INFO: Pod "busybox-privileged-false-c036d43d-3e80-4e1b-bb5c-338231c80cfb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.595868ms
Nov 25 07:36:40.828: INFO: Pod "busybox-privileged-false-c036d43d-3e80-4e1b-bb5c-338231c80cfb": Phase="Running", Reason="", readiness=true. Elapsed: 2.003671822s
Nov 25 07:36:42.831: INFO: Pod "busybox-privileged-false-c036d43d-3e80-4e1b-bb5c-338231c80cfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006291766s
Nov 25 07:36:42.831: INFO: Pod "busybox-privileged-false-c036d43d-3e80-4e1b-bb5c-338231c80cfb" satisfied condition "Succeeded or Failed"
Nov 25 07:36:42.836: INFO: Got logs for pod "busybox-privileged-false-c036d43d-3e80-4e1b-bb5c-338231c80cfb": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:36:42.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3860" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":156,"skipped":2528,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:36:42.841: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 25 07:36:42.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-2295'
Nov 25 07:36:42.937: INFO: stderr: ""
Nov 25 07:36:42.937: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Nov 25 07:36:42.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pod e2e-test-httpd-pod -o json --namespace=kubectl-2295'
Nov 25 07:36:43.023: INFO: stderr: ""
Nov 25 07:36:43.023: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-11-25T07:36:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-25T07:36:42Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-25T07:36:42Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2295\",\n        \"resourceVersion\": \"11495\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2295/pods/e2e-test-httpd-pod\",\n        \"uid\": \"6e36e221-7f81-4c84-8cca-494a13ecaffd\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-zgtzd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8sconformance-m02\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-zgtzd\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-zgtzd\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-25T07:36:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-25T07:36:42Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-25T07:36:42Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-25T07:36:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.49.3\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-11-25T07:36:42Z\"\n    }\n}\n"
Nov 25 07:36:43.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 replace -f - --dry-run server --namespace=kubectl-2295'
Nov 25 07:36:43.316: INFO: stderr: "W1125 07:36:43.076812    1458 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Nov 25 07:36:43.316: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Nov 25 07:36:43.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 delete pods e2e-test-httpd-pod --namespace=kubectl-2295'
Nov 25 07:36:44.947: INFO: stderr: ""
Nov 25 07:36:44.947: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:36:44.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2295" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":303,"completed":157,"skipped":2530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:36:44.953: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-38a128ba-e98a-4055-932a-4c143eaee383
STEP: Creating a pod to test consume configMaps
Nov 25 07:36:44.984: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-477bd8f3-ede5-46ac-ab4d-8c6ab4db3960" in namespace "projected-6142" to be "Succeeded or Failed"
Nov 25 07:36:44.990: INFO: Pod "pod-projected-configmaps-477bd8f3-ede5-46ac-ab4d-8c6ab4db3960": Phase="Pending", Reason="", readiness=false. Elapsed: 5.521272ms
Nov 25 07:36:46.993: INFO: Pod "pod-projected-configmaps-477bd8f3-ede5-46ac-ab4d-8c6ab4db3960": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008530675s
STEP: Saw pod success
Nov 25 07:36:46.993: INFO: Pod "pod-projected-configmaps-477bd8f3-ede5-46ac-ab4d-8c6ab4db3960" satisfied condition "Succeeded or Failed"
Nov 25 07:36:46.995: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-477bd8f3-ede5-46ac-ab4d-8c6ab4db3960 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 25 07:36:47.010: INFO: Waiting for pod pod-projected-configmaps-477bd8f3-ede5-46ac-ab4d-8c6ab4db3960 to disappear
Nov 25 07:36:47.012: INFO: Pod pod-projected-configmaps-477bd8f3-ede5-46ac-ab4d-8c6ab4db3960 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:36:47.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6142" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":158,"skipped":2582,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:36:47.018: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 07:36:47.056: INFO: Waiting up to 5m0s for pod "downwardapi-volume-945dac29-c600-4237-8158-84d617abb6eb" in namespace "downward-api-9263" to be "Succeeded or Failed"
Nov 25 07:36:47.058: INFO: Pod "downwardapi-volume-945dac29-c600-4237-8158-84d617abb6eb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.827796ms
Nov 25 07:36:49.061: INFO: Pod "downwardapi-volume-945dac29-c600-4237-8158-84d617abb6eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004800269s
STEP: Saw pod success
Nov 25 07:36:49.061: INFO: Pod "downwardapi-volume-945dac29-c600-4237-8158-84d617abb6eb" satisfied condition "Succeeded or Failed"
Nov 25 07:36:49.063: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-945dac29-c600-4237-8158-84d617abb6eb container client-container: <nil>
STEP: delete the pod
Nov 25 07:36:49.077: INFO: Waiting for pod downwardapi-volume-945dac29-c600-4237-8158-84d617abb6eb to disappear
Nov 25 07:36:49.080: INFO: Pod downwardapi-volume-945dac29-c600-4237-8158-84d617abb6eb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:36:49.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9263" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":159,"skipped":2585,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:36:49.086: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 25 07:36:49.113: INFO: Waiting up to 5m0s for pod "downward-api-3b94a533-81a1-4987-8417-2bf3e36d2a6b" in namespace "downward-api-9904" to be "Succeeded or Failed"
Nov 25 07:36:49.115: INFO: Pod "downward-api-3b94a533-81a1-4987-8417-2bf3e36d2a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.978923ms
Nov 25 07:36:51.118: INFO: Pod "downward-api-3b94a533-81a1-4987-8417-2bf3e36d2a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004518305s
STEP: Saw pod success
Nov 25 07:36:51.118: INFO: Pod "downward-api-3b94a533-81a1-4987-8417-2bf3e36d2a6b" satisfied condition "Succeeded or Failed"
Nov 25 07:36:51.120: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-3b94a533-81a1-4987-8417-2bf3e36d2a6b container dapi-container: <nil>
STEP: delete the pod
Nov 25 07:36:51.134: INFO: Waiting for pod downward-api-3b94a533-81a1-4987-8417-2bf3e36d2a6b to disappear
Nov 25 07:36:51.155: INFO: Pod downward-api-3b94a533-81a1-4987-8417-2bf3e36d2a6b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:36:51.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9904" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":303,"completed":160,"skipped":2600,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:36:51.160: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:36:54.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4422" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":303,"completed":161,"skipped":2627,"failed":0}
S
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:36:54.214: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:36:54.772: INFO: Checking APIGroup: apiregistration.k8s.io
Nov 25 07:36:54.773: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Nov 25 07:36:54.773: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Nov 25 07:36:54.773: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Nov 25 07:36:54.773: INFO: Checking APIGroup: extensions
Nov 25 07:36:54.774: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Nov 25 07:36:54.774: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Nov 25 07:36:54.774: INFO: extensions/v1beta1 matches extensions/v1beta1
Nov 25 07:36:54.774: INFO: Checking APIGroup: apps
Nov 25 07:36:54.774: INFO: PreferredVersion.GroupVersion: apps/v1
Nov 25 07:36:54.774: INFO: Versions found [{apps/v1 v1}]
Nov 25 07:36:54.774: INFO: apps/v1 matches apps/v1
Nov 25 07:36:54.774: INFO: Checking APIGroup: events.k8s.io
Nov 25 07:36:54.775: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Nov 25 07:36:54.775: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Nov 25 07:36:54.775: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Nov 25 07:36:54.775: INFO: Checking APIGroup: authentication.k8s.io
Nov 25 07:36:54.776: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Nov 25 07:36:54.776: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Nov 25 07:36:54.776: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Nov 25 07:36:54.776: INFO: Checking APIGroup: authorization.k8s.io
Nov 25 07:36:54.776: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Nov 25 07:36:54.776: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Nov 25 07:36:54.776: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Nov 25 07:36:54.776: INFO: Checking APIGroup: autoscaling
Nov 25 07:36:54.777: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Nov 25 07:36:54.777: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Nov 25 07:36:54.777: INFO: autoscaling/v1 matches autoscaling/v1
Nov 25 07:36:54.777: INFO: Checking APIGroup: batch
Nov 25 07:36:54.778: INFO: PreferredVersion.GroupVersion: batch/v1
Nov 25 07:36:54.778: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Nov 25 07:36:54.778: INFO: batch/v1 matches batch/v1
Nov 25 07:36:54.778: INFO: Checking APIGroup: certificates.k8s.io
Nov 25 07:36:54.778: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Nov 25 07:36:54.778: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Nov 25 07:36:54.778: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Nov 25 07:36:54.778: INFO: Checking APIGroup: networking.k8s.io
Nov 25 07:36:54.779: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Nov 25 07:36:54.779: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Nov 25 07:36:54.779: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Nov 25 07:36:54.779: INFO: Checking APIGroup: policy
Nov 25 07:36:54.779: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Nov 25 07:36:54.779: INFO: Versions found [{policy/v1beta1 v1beta1}]
Nov 25 07:36:54.779: INFO: policy/v1beta1 matches policy/v1beta1
Nov 25 07:36:54.779: INFO: Checking APIGroup: rbac.authorization.k8s.io
Nov 25 07:36:54.780: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Nov 25 07:36:54.780: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Nov 25 07:36:54.780: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Nov 25 07:36:54.780: INFO: Checking APIGroup: storage.k8s.io
Nov 25 07:36:54.781: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Nov 25 07:36:54.781: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Nov 25 07:36:54.781: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Nov 25 07:36:54.781: INFO: Checking APIGroup: admissionregistration.k8s.io
Nov 25 07:36:54.781: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Nov 25 07:36:54.781: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Nov 25 07:36:54.781: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Nov 25 07:36:54.781: INFO: Checking APIGroup: apiextensions.k8s.io
Nov 25 07:36:54.782: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Nov 25 07:36:54.782: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Nov 25 07:36:54.782: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Nov 25 07:36:54.782: INFO: Checking APIGroup: scheduling.k8s.io
Nov 25 07:36:54.782: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Nov 25 07:36:54.782: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Nov 25 07:36:54.782: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Nov 25 07:36:54.782: INFO: Checking APIGroup: coordination.k8s.io
Nov 25 07:36:54.783: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Nov 25 07:36:54.783: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Nov 25 07:36:54.783: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Nov 25 07:36:54.783: INFO: Checking APIGroup: node.k8s.io
Nov 25 07:36:54.784: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Nov 25 07:36:54.784: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Nov 25 07:36:54.784: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Nov 25 07:36:54.784: INFO: Checking APIGroup: discovery.k8s.io
Nov 25 07:36:54.784: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Nov 25 07:36:54.784: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Nov 25 07:36:54.784: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:36:54.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3225" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":303,"completed":162,"skipped":2628,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:36:54.789: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Nov 25 07:36:57.360: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-26 pod-service-account-06863136-873f-4bab-92c4-9c4c218e2099 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Nov 25 07:36:57.511: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-26 pod-service-account-06863136-873f-4bab-92c4-9c4c218e2099 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Nov 25 07:36:57.654: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-26 pod-service-account-06863136-873f-4bab-92c4-9c4c218e2099 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:36:57.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-26" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":303,"completed":163,"skipped":2677,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:36:57.816: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-2994
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 25 07:36:57.837: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 25 07:36:57.853: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 25 07:36:59.855: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:37:01.856: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:37:03.869: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:37:05.856: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:37:07.856: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:37:09.856: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:37:11.856: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:37:13.856: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 25 07:37:13.859: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 25 07:37:15.862: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 25 07:37:17.862: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 25 07:37:19.863: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 25 07:37:23.876: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.180:8080/dial?request=hostname&protocol=http&host=10.244.0.41&port=8080&tries=1'] Namespace:pod-network-test-2994 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:37:23.876: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:37:23.973: INFO: Waiting for responses: map[]
Nov 25 07:37:23.975: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.180:8080/dial?request=hostname&protocol=http&host=10.244.1.179&port=8080&tries=1'] Namespace:pod-network-test-2994 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:37:23.975: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:37:24.048: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:37:24.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2994" for this suite.

• [SLOW TEST:26.240 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":303,"completed":164,"skipped":2703,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:37:24.056: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:37:24.390: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 25 07:37:26.398: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741886644, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741886644, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741886644, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741886644, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:37:29.444: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:37:29.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9157" for this suite.
STEP: Destroying namespace "webhook-9157-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.509 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":303,"completed":165,"skipped":2727,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:37:29.564: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:37:29.987: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:37:33.002: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:37:34.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-527" for this suite.
STEP: Destroying namespace "webhook-527-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":303,"completed":166,"skipped":2731,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:37:34.109: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-z7g9
STEP: Creating a pod to test atomic-volume-subpath
Nov 25 07:37:34.146: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-z7g9" in namespace "subpath-655" to be "Succeeded or Failed"
Nov 25 07:37:34.152: INFO: Pod "pod-subpath-test-downwardapi-z7g9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.156046ms
Nov 25 07:37:36.155: INFO: Pod "pod-subpath-test-downwardapi-z7g9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00877065s
Nov 25 07:37:38.158: INFO: Pod "pod-subpath-test-downwardapi-z7g9": Phase="Running", Reason="", readiness=true. Elapsed: 4.011459388s
Nov 25 07:37:40.160: INFO: Pod "pod-subpath-test-downwardapi-z7g9": Phase="Running", Reason="", readiness=true. Elapsed: 6.014131292s
Nov 25 07:37:42.163: INFO: Pod "pod-subpath-test-downwardapi-z7g9": Phase="Running", Reason="", readiness=true. Elapsed: 8.017140721s
Nov 25 07:37:44.166: INFO: Pod "pod-subpath-test-downwardapi-z7g9": Phase="Running", Reason="", readiness=true. Elapsed: 10.020177739s
Nov 25 07:37:46.169: INFO: Pod "pod-subpath-test-downwardapi-z7g9": Phase="Running", Reason="", readiness=true. Elapsed: 12.022967279s
Nov 25 07:37:48.172: INFO: Pod "pod-subpath-test-downwardapi-z7g9": Phase="Running", Reason="", readiness=true. Elapsed: 14.025614973s
Nov 25 07:37:50.175: INFO: Pod "pod-subpath-test-downwardapi-z7g9": Phase="Running", Reason="", readiness=true. Elapsed: 16.028559967s
Nov 25 07:37:52.179: INFO: Pod "pod-subpath-test-downwardapi-z7g9": Phase="Running", Reason="", readiness=true. Elapsed: 18.032872473s
Nov 25 07:37:54.182: INFO: Pod "pod-subpath-test-downwardapi-z7g9": Phase="Running", Reason="", readiness=true. Elapsed: 20.035618032s
Nov 25 07:37:56.185: INFO: Pod "pod-subpath-test-downwardapi-z7g9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.03831199s
STEP: Saw pod success
Nov 25 07:37:56.185: INFO: Pod "pod-subpath-test-downwardapi-z7g9" satisfied condition "Succeeded or Failed"
Nov 25 07:37:56.186: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-downwardapi-z7g9 container test-container-subpath-downwardapi-z7g9: <nil>
STEP: delete the pod
Nov 25 07:37:56.200: INFO: Waiting for pod pod-subpath-test-downwardapi-z7g9 to disappear
Nov 25 07:37:56.202: INFO: Pod pod-subpath-test-downwardapi-z7g9 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-z7g9
Nov 25 07:37:56.202: INFO: Deleting pod "pod-subpath-test-downwardapi-z7g9" in namespace "subpath-655"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:37:56.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-655" for this suite.

• [SLOW TEST:22.100 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":303,"completed":167,"skipped":2741,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:37:56.210: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6505, will wait for the garbage collector to delete the pods
Nov 25 07:37:58.306: INFO: Deleting Job.batch foo took: 4.823668ms
Nov 25 07:37:58.706: INFO: Terminating Job.batch foo pods took: 400.289823ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:38:40.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6505" for this suite.

• [SLOW TEST:44.504 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":303,"completed":168,"skipped":2750,"failed":0}
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:38:40.714: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 25 07:38:41.437: INFO: starting watch
STEP: patching
STEP: updating
Nov 25 07:38:41.445: INFO: waiting for watch events with expected annotations
Nov 25 07:38:41.445: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:38:41.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-3732" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":303,"completed":169,"skipped":2751,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:38:41.506: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:38:41.784: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:38:44.804: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:38:44.807: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:38:46.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7578" for this suite.
STEP: Destroying namespace "webhook-7578-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":303,"completed":170,"skipped":2755,"failed":0}
SS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:38:46.128: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:38:46.154: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c6059a53-c119-4362-b958-205aefbfa97f" in namespace "security-context-test-7016" to be "Succeeded or Failed"
Nov 25 07:38:46.156: INFO: Pod "alpine-nnp-false-c6059a53-c119-4362-b958-205aefbfa97f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.81568ms
Nov 25 07:38:48.159: INFO: Pod "alpine-nnp-false-c6059a53-c119-4362-b958-205aefbfa97f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004716643s
Nov 25 07:38:50.162: INFO: Pod "alpine-nnp-false-c6059a53-c119-4362-b958-205aefbfa97f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007578283s
Nov 25 07:38:50.162: INFO: Pod "alpine-nnp-false-c6059a53-c119-4362-b958-205aefbfa97f" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:38:50.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7016" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":171,"skipped":2757,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:38:50.173: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:38:56.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5250" for this suite.
STEP: Destroying namespace "nsdeletetest-3804" for this suite.
Nov 25 07:38:56.275: INFO: Namespace nsdeletetest-3804 was already deleted
STEP: Destroying namespace "nsdeletetest-3428" for this suite.

• [SLOW TEST:6.104 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":303,"completed":172,"skipped":2766,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:38:56.278: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 25 07:38:56.323: INFO: Waiting up to 5m0s for pod "pod-8d2ee0b0-3361-4721-8959-38c86d8b3ed3" in namespace "emptydir-1408" to be "Succeeded or Failed"
Nov 25 07:38:56.326: INFO: Pod "pod-8d2ee0b0-3361-4721-8959-38c86d8b3ed3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.253479ms
Nov 25 07:38:58.329: INFO: Pod "pod-8d2ee0b0-3361-4721-8959-38c86d8b3ed3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005993528s
STEP: Saw pod success
Nov 25 07:38:58.329: INFO: Pod "pod-8d2ee0b0-3361-4721-8959-38c86d8b3ed3" satisfied condition "Succeeded or Failed"
Nov 25 07:38:58.331: INFO: Trying to get logs from node k8sconformance-m02 pod pod-8d2ee0b0-3361-4721-8959-38c86d8b3ed3 container test-container: <nil>
STEP: delete the pod
Nov 25 07:38:58.345: INFO: Waiting for pod pod-8d2ee0b0-3361-4721-8959-38c86d8b3ed3 to disappear
Nov 25 07:38:58.347: INFO: Pod pod-8d2ee0b0-3361-4721-8959-38c86d8b3ed3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:38:58.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1408" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":173,"skipped":2777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:38:58.357: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-107/configmap-test-2afec3e9-92b6-41aa-a974-da1a57e18cbd
STEP: Creating a pod to test consume configMaps
Nov 25 07:38:58.397: INFO: Waiting up to 5m0s for pod "pod-configmaps-420aff0b-b258-4372-be97-460d8d35844e" in namespace "configmap-107" to be "Succeeded or Failed"
Nov 25 07:38:58.399: INFO: Pod "pod-configmaps-420aff0b-b258-4372-be97-460d8d35844e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.705419ms
Nov 25 07:39:00.401: INFO: Pod "pod-configmaps-420aff0b-b258-4372-be97-460d8d35844e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004491302s
STEP: Saw pod success
Nov 25 07:39:00.401: INFO: Pod "pod-configmaps-420aff0b-b258-4372-be97-460d8d35844e" satisfied condition "Succeeded or Failed"
Nov 25 07:39:00.403: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-420aff0b-b258-4372-be97-460d8d35844e container env-test: <nil>
STEP: delete the pod
Nov 25 07:39:00.418: INFO: Waiting for pod pod-configmaps-420aff0b-b258-4372-be97-460d8d35844e to disappear
Nov 25 07:39:00.420: INFO: Pod pod-configmaps-420aff0b-b258-4372-be97-460d8d35844e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:39:00.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-107" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":303,"completed":174,"skipped":2810,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:39:00.425: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:39:01.025: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov 25 07:39:03.032: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741886741, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741886741, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741886741, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741886741, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:39:06.063: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:39:06.066: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:39:07.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-60" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.759 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":303,"completed":175,"skipped":2820,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:39:07.184: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:39:09.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4874" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":303,"completed":176,"skipped":2850,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:39:09.344: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-1526
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 25 07:39:09.376: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 25 07:39:09.429: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 25 07:39:11.432: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:39:13.431: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:39:15.431: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:39:17.432: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:39:19.431: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:39:21.432: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 25 07:39:23.432: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 25 07:39:23.436: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 25 07:39:25.438: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 25 07:39:27.439: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 25 07:39:29.439: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 25 07:39:31.462: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.42:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1526 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:39:31.462: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:39:31.569: INFO: Found all expected endpoints: [netserver-0]
Nov 25 07:39:31.571: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.192:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1526 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:39:31.571: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:39:31.660: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:39:31.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1526" for this suite.

• [SLOW TEST:22.323 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":177,"skipped":2851,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:39:31.667: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-5461f93d-889c-461c-959a-ec284a2e6127
STEP: Creating secret with name secret-projected-all-test-volume-1a0cbcbf-28a6-4399-803e-ca952e4e2b61
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov 25 07:39:31.734: INFO: Waiting up to 5m0s for pod "projected-volume-9cfc588e-ef3d-4f99-8702-65f32f25c161" in namespace "projected-3477" to be "Succeeded or Failed"
Nov 25 07:39:31.736: INFO: Pod "projected-volume-9cfc588e-ef3d-4f99-8702-65f32f25c161": Phase="Pending", Reason="", readiness=false. Elapsed: 1.750104ms
Nov 25 07:39:33.739: INFO: Pod "projected-volume-9cfc588e-ef3d-4f99-8702-65f32f25c161": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004702388s
STEP: Saw pod success
Nov 25 07:39:33.739: INFO: Pod "projected-volume-9cfc588e-ef3d-4f99-8702-65f32f25c161" satisfied condition "Succeeded or Failed"
Nov 25 07:39:33.741: INFO: Trying to get logs from node k8sconformance-m02 pod projected-volume-9cfc588e-ef3d-4f99-8702-65f32f25c161 container projected-all-volume-test: <nil>
STEP: delete the pod
Nov 25 07:39:33.757: INFO: Waiting for pod projected-volume-9cfc588e-ef3d-4f99-8702-65f32f25c161 to disappear
Nov 25 07:39:33.758: INFO: Pod projected-volume-9cfc588e-ef3d-4f99-8702-65f32f25c161 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:39:33.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3477" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":303,"completed":178,"skipped":2869,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:39:33.763: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 25 07:39:33.814: INFO: Waiting up to 5m0s for pod "downward-api-59df4b43-a1e3-4c9b-b8c5-de7a3215cf61" in namespace "downward-api-8744" to be "Succeeded or Failed"
Nov 25 07:39:33.819: INFO: Pod "downward-api-59df4b43-a1e3-4c9b-b8c5-de7a3215cf61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699502ms
Nov 25 07:39:35.822: INFO: Pod "downward-api-59df4b43-a1e3-4c9b-b8c5-de7a3215cf61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008077981s
STEP: Saw pod success
Nov 25 07:39:35.822: INFO: Pod "downward-api-59df4b43-a1e3-4c9b-b8c5-de7a3215cf61" satisfied condition "Succeeded or Failed"
Nov 25 07:39:35.824: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-59df4b43-a1e3-4c9b-b8c5-de7a3215cf61 container dapi-container: <nil>
STEP: delete the pod
Nov 25 07:39:35.839: INFO: Waiting for pod downward-api-59df4b43-a1e3-4c9b-b8c5-de7a3215cf61 to disappear
Nov 25 07:39:35.841: INFO: Pod downward-api-59df4b43-a1e3-4c9b-b8c5-de7a3215cf61 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:39:35.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8744" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":303,"completed":179,"skipped":2873,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:39:35.846: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1125 07:40:15.886743      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 25 07:41:17.898: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Nov 25 07:41:17.898: INFO: Deleting pod "simpletest.rc-64p49" in namespace "gc-7063"
Nov 25 07:41:17.939: INFO: Deleting pod "simpletest.rc-8z4p5" in namespace "gc-7063"
Nov 25 07:41:17.947: INFO: Deleting pod "simpletest.rc-9gfxw" in namespace "gc-7063"
Nov 25 07:41:17.965: INFO: Deleting pod "simpletest.rc-cdlm5" in namespace "gc-7063"
Nov 25 07:41:17.984: INFO: Deleting pod "simpletest.rc-fnh25" in namespace "gc-7063"
Nov 25 07:41:17.996: INFO: Deleting pod "simpletest.rc-g7p78" in namespace "gc-7063"
Nov 25 07:41:18.045: INFO: Deleting pod "simpletest.rc-hsmdd" in namespace "gc-7063"
Nov 25 07:41:18.060: INFO: Deleting pod "simpletest.rc-lwz5k" in namespace "gc-7063"
Nov 25 07:41:18.118: INFO: Deleting pod "simpletest.rc-t5w7h" in namespace "gc-7063"
Nov 25 07:41:18.132: INFO: Deleting pod "simpletest.rc-v9fp7" in namespace "gc-7063"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:41:18.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7063" for this suite.

• [SLOW TEST:102.307 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":303,"completed":180,"skipped":2904,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:41:18.153: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 07:41:18.319: INFO: Waiting up to 5m0s for pod "downwardapi-volume-389a1837-4656-426c-a697-8a2f855bd98b" in namespace "downward-api-6651" to be "Succeeded or Failed"
Nov 25 07:41:18.324: INFO: Pod "downwardapi-volume-389a1837-4656-426c-a697-8a2f855bd98b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.828157ms
Nov 25 07:41:20.327: INFO: Pod "downwardapi-volume-389a1837-4656-426c-a697-8a2f855bd98b": Phase="Running", Reason="", readiness=true. Elapsed: 2.007783624s
Nov 25 07:41:22.330: INFO: Pod "downwardapi-volume-389a1837-4656-426c-a697-8a2f855bd98b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01067111s
STEP: Saw pod success
Nov 25 07:41:22.330: INFO: Pod "downwardapi-volume-389a1837-4656-426c-a697-8a2f855bd98b" satisfied condition "Succeeded or Failed"
Nov 25 07:41:22.332: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-389a1837-4656-426c-a697-8a2f855bd98b container client-container: <nil>
STEP: delete the pod
Nov 25 07:41:22.353: INFO: Waiting for pod downwardapi-volume-389a1837-4656-426c-a697-8a2f855bd98b to disappear
Nov 25 07:41:22.355: INFO: Pod downwardapi-volume-389a1837-4656-426c-a697-8a2f855bd98b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:41:22.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6651" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":181,"skipped":2955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:41:22.360: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov 25 07:41:22.386: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3613 /api/v1/namespaces/watch-3613/configmaps/e2e-watch-test-configmap-a 51411086-a87e-4fb9-8f9d-9e57afbb35d7 12978 0 2020-11-25 07:41:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-25 07:41:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 25 07:41:22.386: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3613 /api/v1/namespaces/watch-3613/configmaps/e2e-watch-test-configmap-a 51411086-a87e-4fb9-8f9d-9e57afbb35d7 12978 0 2020-11-25 07:41:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-25 07:41:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov 25 07:41:32.392: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3613 /api/v1/namespaces/watch-3613/configmaps/e2e-watch-test-configmap-a 51411086-a87e-4fb9-8f9d-9e57afbb35d7 13072 0 2020-11-25 07:41:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-25 07:41:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 25 07:41:32.393: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3613 /api/v1/namespaces/watch-3613/configmaps/e2e-watch-test-configmap-a 51411086-a87e-4fb9-8f9d-9e57afbb35d7 13072 0 2020-11-25 07:41:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-25 07:41:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov 25 07:41:42.398: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3613 /api/v1/namespaces/watch-3613/configmaps/e2e-watch-test-configmap-a 51411086-a87e-4fb9-8f9d-9e57afbb35d7 13081 0 2020-11-25 07:41:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-25 07:41:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 25 07:41:42.398: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3613 /api/v1/namespaces/watch-3613/configmaps/e2e-watch-test-configmap-a 51411086-a87e-4fb9-8f9d-9e57afbb35d7 13081 0 2020-11-25 07:41:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-25 07:41:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov 25 07:41:52.403: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3613 /api/v1/namespaces/watch-3613/configmaps/e2e-watch-test-configmap-a 51411086-a87e-4fb9-8f9d-9e57afbb35d7 13090 0 2020-11-25 07:41:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-25 07:41:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 25 07:41:52.404: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3613 /api/v1/namespaces/watch-3613/configmaps/e2e-watch-test-configmap-a 51411086-a87e-4fb9-8f9d-9e57afbb35d7 13090 0 2020-11-25 07:41:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-25 07:41:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov 25 07:42:02.410: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3613 /api/v1/namespaces/watch-3613/configmaps/e2e-watch-test-configmap-b 4b7a7ffa-389a-445d-8ad8-11ea91ac28c4 13099 0 2020-11-25 07:42:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-25 07:42:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 25 07:42:02.410: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3613 /api/v1/namespaces/watch-3613/configmaps/e2e-watch-test-configmap-b 4b7a7ffa-389a-445d-8ad8-11ea91ac28c4 13099 0 2020-11-25 07:42:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-25 07:42:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov 25 07:42:12.415: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3613 /api/v1/namespaces/watch-3613/configmaps/e2e-watch-test-configmap-b 4b7a7ffa-389a-445d-8ad8-11ea91ac28c4 13108 0 2020-11-25 07:42:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-25 07:42:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 25 07:42:12.415: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3613 /api/v1/namespaces/watch-3613/configmaps/e2e-watch-test-configmap-b 4b7a7ffa-389a-445d-8ad8-11ea91ac28c4 13108 0 2020-11-25 07:42:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-25 07:42:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:42:22.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3613" for this suite.

• [SLOW TEST:60.062 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":303,"completed":182,"skipped":2999,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:42:22.422: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:42:22.445: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:42:23.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5199" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":303,"completed":183,"skipped":3008,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:42:23.463: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:42:27.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6832" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":303,"completed":184,"skipped":3009,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:42:27.510: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-16b823ca-72ed-45b0-a0ee-2bbf86197b4b
STEP: Creating a pod to test consume secrets
Nov 25 07:42:27.540: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-53fba5e0-caf6-420f-bca3-ad86501738fa" in namespace "projected-5679" to be "Succeeded or Failed"
Nov 25 07:42:27.549: INFO: Pod "pod-projected-secrets-53fba5e0-caf6-420f-bca3-ad86501738fa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.35486ms
Nov 25 07:42:29.552: INFO: Pod "pod-projected-secrets-53fba5e0-caf6-420f-bca3-ad86501738fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011334385s
Nov 25 07:42:31.555: INFO: Pod "pod-projected-secrets-53fba5e0-caf6-420f-bca3-ad86501738fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014448385s
STEP: Saw pod success
Nov 25 07:42:31.555: INFO: Pod "pod-projected-secrets-53fba5e0-caf6-420f-bca3-ad86501738fa" satisfied condition "Succeeded or Failed"
Nov 25 07:42:31.557: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-53fba5e0-caf6-420f-bca3-ad86501738fa container secret-volume-test: <nil>
STEP: delete the pod
Nov 25 07:42:31.572: INFO: Waiting for pod pod-projected-secrets-53fba5e0-caf6-420f-bca3-ad86501738fa to disappear
Nov 25 07:42:31.574: INFO: Pod pod-projected-secrets-53fba5e0-caf6-420f-bca3-ad86501738fa no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:42:31.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5679" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":303,"completed":185,"skipped":3035,"failed":0}

------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:42:31.580: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Nov 25 07:42:31.605: INFO: created test-event-1
Nov 25 07:42:31.607: INFO: created test-event-2
Nov 25 07:42:31.611: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Nov 25 07:42:31.613: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Nov 25 07:42:31.625: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:42:31.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2495" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":303,"completed":186,"skipped":3035,"failed":0}
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:42:31.632: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:42:31.662: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6301
I1125 07:42:31.672508      23 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6301, replica count: 1
I1125 07:42:32.722987      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 25 07:42:32.833: INFO: Created: latency-svc-4pf7g
Nov 25 07:42:32.836: INFO: Got endpoints: latency-svc-4pf7g [13.613864ms]
Nov 25 07:42:32.851: INFO: Created: latency-svc-6p4tp
Nov 25 07:42:32.854: INFO: Got endpoints: latency-svc-6p4tp [16.846997ms]
Nov 25 07:42:32.859: INFO: Created: latency-svc-pdh9z
Nov 25 07:42:32.865: INFO: Got endpoints: latency-svc-pdh9z [27.507464ms]
Nov 25 07:42:32.865: INFO: Created: latency-svc-c4lkg
Nov 25 07:42:32.912: INFO: Got endpoints: latency-svc-c4lkg [74.843415ms]
Nov 25 07:42:32.939: INFO: Created: latency-svc-rh6ps
Nov 25 07:42:32.942: INFO: Got endpoints: latency-svc-rh6ps [104.897905ms]
Nov 25 07:42:32.959: INFO: Created: latency-svc-9lnn8
Nov 25 07:42:32.967: INFO: Created: latency-svc-pbhjc
Nov 25 07:42:32.967: INFO: Got endpoints: latency-svc-9lnn8 [129.358226ms]
Nov 25 07:42:32.971: INFO: Got endpoints: latency-svc-pbhjc [133.266486ms]
Nov 25 07:42:32.998: INFO: Created: latency-svc-vsw4p
Nov 25 07:42:33.038: INFO: Got endpoints: latency-svc-vsw4p [200.149881ms]
Nov 25 07:42:33.039: INFO: Created: latency-svc-bzql8
Nov 25 07:42:33.043: INFO: Got endpoints: latency-svc-bzql8 [205.91992ms]
Nov 25 07:42:33.055: INFO: Created: latency-svc-mc94w
Nov 25 07:42:33.074: INFO: Got endpoints: latency-svc-mc94w [236.742253ms]
Nov 25 07:42:33.075: INFO: Created: latency-svc-2n29f
Nov 25 07:42:33.079: INFO: Got endpoints: latency-svc-2n29f [241.10077ms]
Nov 25 07:42:33.093: INFO: Created: latency-svc-8drxx
Nov 25 07:42:33.096: INFO: Got endpoints: latency-svc-8drxx [258.364201ms]
Nov 25 07:42:33.123: INFO: Created: latency-svc-tjxg9
Nov 25 07:42:33.203: INFO: Created: latency-svc-vxcx6
Nov 25 07:42:33.204: INFO: Got endpoints: latency-svc-tjxg9 [366.044208ms]
Nov 25 07:42:33.205: INFO: Got endpoints: latency-svc-vxcx6 [367.56685ms]
Nov 25 07:42:33.211: INFO: Created: latency-svc-d7zx9
Nov 25 07:42:33.214: INFO: Got endpoints: latency-svc-d7zx9 [376.208753ms]
Nov 25 07:42:33.220: INFO: Created: latency-svc-dphpq
Nov 25 07:42:33.223: INFO: Got endpoints: latency-svc-dphpq [17.475505ms]
Nov 25 07:42:33.230: INFO: Created: latency-svc-snm6p
Nov 25 07:42:33.236: INFO: Got endpoints: latency-svc-snm6p [398.479487ms]
Nov 25 07:42:33.251: INFO: Created: latency-svc-k7v4s
Nov 25 07:42:33.254: INFO: Got endpoints: latency-svc-k7v4s [399.917844ms]
Nov 25 07:42:33.264: INFO: Created: latency-svc-mnssb
Nov 25 07:42:33.268: INFO: Got endpoints: latency-svc-mnssb [403.1938ms]
Nov 25 07:42:33.273: INFO: Created: latency-svc-k7mt7
Nov 25 07:42:33.309: INFO: Got endpoints: latency-svc-k7mt7 [396.801038ms]
Nov 25 07:42:33.335: INFO: Created: latency-svc-97wcd
Nov 25 07:42:33.337: INFO: Got endpoints: latency-svc-97wcd [395.147312ms]
Nov 25 07:42:33.349: INFO: Created: latency-svc-rtvln
Nov 25 07:42:33.350: INFO: Got endpoints: latency-svc-rtvln [382.561309ms]
Nov 25 07:42:33.360: INFO: Created: latency-svc-lqpbs
Nov 25 07:42:33.362: INFO: Got endpoints: latency-svc-lqpbs [391.414505ms]
Nov 25 07:42:33.371: INFO: Created: latency-svc-fzx2h
Nov 25 07:42:33.376: INFO: Got endpoints: latency-svc-fzx2h [337.913917ms]
Nov 25 07:42:33.419: INFO: Created: latency-svc-7m64g
Nov 25 07:42:33.426: INFO: Got endpoints: latency-svc-7m64g [382.582949ms]
Nov 25 07:42:33.431: INFO: Created: latency-svc-mzbp5
Nov 25 07:42:33.465: INFO: Got endpoints: latency-svc-mzbp5 [390.182148ms]
Nov 25 07:42:33.468: INFO: Created: latency-svc-8j48n
Nov 25 07:42:33.485: INFO: Got endpoints: latency-svc-8j48n [406.052681ms]
Nov 25 07:42:33.489: INFO: Created: latency-svc-q2hzb
Nov 25 07:42:33.511: INFO: Got endpoints: latency-svc-q2hzb [414.972271ms]
Nov 25 07:42:33.518: INFO: Created: latency-svc-mqhkl
Nov 25 07:42:33.523: INFO: Got endpoints: latency-svc-mqhkl [319.670727ms]
Nov 25 07:42:33.555: INFO: Created: latency-svc-jv55r
Nov 25 07:42:33.564: INFO: Created: latency-svc-9w5jq
Nov 25 07:42:33.564: INFO: Got endpoints: latency-svc-jv55r [350.077088ms]
Nov 25 07:42:33.604: INFO: Got endpoints: latency-svc-9w5jq [381.166654ms]
Nov 25 07:42:33.611: INFO: Created: latency-svc-td627
Nov 25 07:42:33.627: INFO: Got endpoints: latency-svc-td627 [390.821349ms]
Nov 25 07:42:33.631: INFO: Created: latency-svc-wbq4r
Nov 25 07:42:33.635: INFO: Got endpoints: latency-svc-wbq4r [381.444852ms]
Nov 25 07:42:33.680: INFO: Created: latency-svc-r6m6v
Nov 25 07:42:33.691: INFO: Created: latency-svc-br4ql
Nov 25 07:42:33.691: INFO: Got endpoints: latency-svc-r6m6v [423.593888ms]
Nov 25 07:42:33.696: INFO: Got endpoints: latency-svc-br4ql [386.770902ms]
Nov 25 07:42:33.701: INFO: Created: latency-svc-7kxw8
Nov 25 07:42:33.742: INFO: Got endpoints: latency-svc-7kxw8 [405.073419ms]
Nov 25 07:42:33.744: INFO: Created: latency-svc-fptkt
Nov 25 07:42:33.747: INFO: Got endpoints: latency-svc-fptkt [397.011211ms]
Nov 25 07:42:33.775: INFO: Created: latency-svc-p486q
Nov 25 07:42:33.779: INFO: Got endpoints: latency-svc-p486q [416.532487ms]
Nov 25 07:42:33.787: INFO: Created: latency-svc-tg89m
Nov 25 07:42:33.789: INFO: Got endpoints: latency-svc-tg89m [413.608079ms]
Nov 25 07:42:33.805: INFO: Created: latency-svc-xlz5d
Nov 25 07:42:33.813: INFO: Got endpoints: latency-svc-xlz5d [387.177774ms]
Nov 25 07:42:33.815: INFO: Created: latency-svc-vshw5
Nov 25 07:42:33.817: INFO: Got endpoints: latency-svc-vshw5 [352.093626ms]
Nov 25 07:42:33.828: INFO: Created: latency-svc-8gmhc
Nov 25 07:42:33.830: INFO: Got endpoints: latency-svc-8gmhc [345.67837ms]
Nov 25 07:42:33.839: INFO: Created: latency-svc-vtjzc
Nov 25 07:42:33.864: INFO: Got endpoints: latency-svc-vtjzc [352.901951ms]
Nov 25 07:42:33.866: INFO: Created: latency-svc-865cz
Nov 25 07:42:33.867: INFO: Got endpoints: latency-svc-865cz [343.868799ms]
Nov 25 07:42:33.876: INFO: Created: latency-svc-f6vmm
Nov 25 07:42:33.877: INFO: Got endpoints: latency-svc-f6vmm [313.373474ms]
Nov 25 07:42:33.884: INFO: Created: latency-svc-976rh
Nov 25 07:42:33.915: INFO: Got endpoints: latency-svc-976rh [311.462191ms]
Nov 25 07:42:33.941: INFO: Created: latency-svc-vwxxn
Nov 25 07:42:33.960: INFO: Created: latency-svc-p6vkb
Nov 25 07:42:33.960: INFO: Got endpoints: latency-svc-vwxxn [333.417122ms]
Nov 25 07:42:33.963: INFO: Got endpoints: latency-svc-p6vkb [327.867282ms]
Nov 25 07:42:33.984: INFO: Created: latency-svc-75zmh
Nov 25 07:42:33.993: INFO: Created: latency-svc-r2k27
Nov 25 07:42:33.993: INFO: Got endpoints: latency-svc-75zmh [301.161548ms]
Nov 25 07:42:34.010: INFO: Got endpoints: latency-svc-r2k27 [313.930653ms]
Nov 25 07:42:34.012: INFO: Created: latency-svc-2gt74
Nov 25 07:42:34.016: INFO: Got endpoints: latency-svc-2gt74 [273.853741ms]
Nov 25 07:42:34.023: INFO: Created: latency-svc-jf2gc
Nov 25 07:42:34.030: INFO: Got endpoints: latency-svc-jf2gc [282.673404ms]
Nov 25 07:42:34.030: INFO: Created: latency-svc-2kdqh
Nov 25 07:42:34.034: INFO: Got endpoints: latency-svc-2kdqh [254.858174ms]
Nov 25 07:42:34.039: INFO: Created: latency-svc-m9xb6
Nov 25 07:42:34.047: INFO: Created: latency-svc-84ss7
Nov 25 07:42:34.047: INFO: Got endpoints: latency-svc-m9xb6 [257.809928ms]
Nov 25 07:42:34.071: INFO: Created: latency-svc-mb9f5
Nov 25 07:42:34.160: INFO: Got endpoints: latency-svc-mb9f5 [342.336146ms]
Nov 25 07:42:34.160: INFO: Got endpoints: latency-svc-84ss7 [346.130341ms]
Nov 25 07:42:34.161: INFO: Created: latency-svc-96zgq
Nov 25 07:42:34.213: INFO: Got endpoints: latency-svc-96zgq [382.443271ms]
Nov 25 07:42:34.214: INFO: Created: latency-svc-q4nqb
Nov 25 07:42:34.226: INFO: Created: latency-svc-mkmtv
Nov 25 07:42:34.235: INFO: Created: latency-svc-zpdjs
Nov 25 07:42:34.237: INFO: Got endpoints: latency-svc-q4nqb [373.347456ms]
Nov 25 07:42:34.246: INFO: Created: latency-svc-77jt7
Nov 25 07:42:34.254: INFO: Created: latency-svc-mqlzf
Nov 25 07:42:34.284: INFO: Created: latency-svc-gj948
Nov 25 07:42:34.286: INFO: Got endpoints: latency-svc-mkmtv [418.470295ms]
Nov 25 07:42:34.313: INFO: Created: latency-svc-d2hkb
Nov 25 07:42:34.321: INFO: Created: latency-svc-wmvn4
Nov 25 07:42:34.332: INFO: Created: latency-svc-x6kp5
Nov 25 07:42:34.341: INFO: Got endpoints: latency-svc-zpdjs [463.508624ms]
Nov 25 07:42:34.342: INFO: Created: latency-svc-8spls
Nov 25 07:42:34.350: INFO: Created: latency-svc-mhssf
Nov 25 07:42:34.359: INFO: Created: latency-svc-p8kts
Nov 25 07:42:34.369: INFO: Created: latency-svc-z7rc6
Nov 25 07:42:34.377: INFO: Created: latency-svc-wwksz
Nov 25 07:42:34.418: INFO: Got endpoints: latency-svc-77jt7 [502.793703ms]
Nov 25 07:42:34.419: INFO: Created: latency-svc-dnmwg
Nov 25 07:42:34.429: INFO: Created: latency-svc-v8m8d
Nov 25 07:42:34.439: INFO: Created: latency-svc-2bjsj
Nov 25 07:42:34.439: INFO: Got endpoints: latency-svc-mqlzf [477.403795ms]
Nov 25 07:42:34.465: INFO: Created: latency-svc-rnmvb
Nov 25 07:42:34.484: INFO: Created: latency-svc-6l6cp
Nov 25 07:42:34.488: INFO: Got endpoints: latency-svc-gj948 [524.533658ms]
Nov 25 07:42:34.511: INFO: Created: latency-svc-z9qk5
Nov 25 07:42:34.542: INFO: Created: latency-svc-7767g
Nov 25 07:42:34.542: INFO: Got endpoints: latency-svc-d2hkb [549.502424ms]
Nov 25 07:42:34.568: INFO: Created: latency-svc-2kllv
Nov 25 07:42:34.586: INFO: Got endpoints: latency-svc-wmvn4 [575.745902ms]
Nov 25 07:42:34.600: INFO: Created: latency-svc-2rn2m
Nov 25 07:42:34.636: INFO: Got endpoints: latency-svc-x6kp5 [619.508422ms]
Nov 25 07:42:34.686: INFO: Got endpoints: latency-svc-8spls [656.335277ms]
Nov 25 07:42:34.694: INFO: Created: latency-svc-vvhn9
Nov 25 07:42:34.702: INFO: Created: latency-svc-rz8v2
Nov 25 07:42:34.735: INFO: Got endpoints: latency-svc-mhssf [701.497587ms]
Nov 25 07:42:34.751: INFO: Created: latency-svc-vmrp9
Nov 25 07:42:34.785: INFO: Got endpoints: latency-svc-p8kts [738.091319ms]
Nov 25 07:42:34.829: INFO: Created: latency-svc-w254c
Nov 25 07:42:34.835: INFO: Got endpoints: latency-svc-z7rc6 [675.276126ms]
Nov 25 07:42:34.867: INFO: Created: latency-svc-t9v49
Nov 25 07:42:34.886: INFO: Got endpoints: latency-svc-wwksz [726.385141ms]
Nov 25 07:42:34.935: INFO: Created: latency-svc-b4ntt
Nov 25 07:42:34.938: INFO: Got endpoints: latency-svc-dnmwg [724.623135ms]
Nov 25 07:42:34.962: INFO: Created: latency-svc-4wgnz
Nov 25 07:42:35.011: INFO: Got endpoints: latency-svc-v8m8d [773.136474ms]
Nov 25 07:42:35.022: INFO: Created: latency-svc-fsp8h
Nov 25 07:42:35.042: INFO: Got endpoints: latency-svc-2bjsj [756.408277ms]
Nov 25 07:42:35.053: INFO: Created: latency-svc-hm4jj
Nov 25 07:42:35.085: INFO: Got endpoints: latency-svc-rnmvb [744.639299ms]
Nov 25 07:42:35.118: INFO: Created: latency-svc-ph784
Nov 25 07:42:35.135: INFO: Got endpoints: latency-svc-6l6cp [717.016694ms]
Nov 25 07:42:35.167: INFO: Created: latency-svc-kgpcz
Nov 25 07:42:35.186: INFO: Got endpoints: latency-svc-z9qk5 [746.658843ms]
Nov 25 07:42:35.218: INFO: Created: latency-svc-rxhvt
Nov 25 07:42:35.236: INFO: Got endpoints: latency-svc-7767g [748.124611ms]
Nov 25 07:42:35.274: INFO: Created: latency-svc-mq7vk
Nov 25 07:42:35.286: INFO: Got endpoints: latency-svc-2kllv [743.799626ms]
Nov 25 07:42:35.301: INFO: Created: latency-svc-97vpg
Nov 25 07:42:35.336: INFO: Got endpoints: latency-svc-2rn2m [750.29296ms]
Nov 25 07:42:35.352: INFO: Created: latency-svc-rtltj
Nov 25 07:42:35.388: INFO: Got endpoints: latency-svc-vvhn9 [752.114784ms]
Nov 25 07:42:35.400: INFO: Created: latency-svc-89bqp
Nov 25 07:42:35.435: INFO: Got endpoints: latency-svc-rz8v2 [749.049625ms]
Nov 25 07:42:35.463: INFO: Created: latency-svc-2wngl
Nov 25 07:42:35.485: INFO: Got endpoints: latency-svc-vmrp9 [749.749278ms]
Nov 25 07:42:35.520: INFO: Created: latency-svc-lsjld
Nov 25 07:42:35.536: INFO: Got endpoints: latency-svc-w254c [750.426664ms]
Nov 25 07:42:35.559: INFO: Created: latency-svc-7pgwv
Nov 25 07:42:35.586: INFO: Got endpoints: latency-svc-t9v49 [750.716233ms]
Nov 25 07:42:35.598: INFO: Created: latency-svc-mhxfc
Nov 25 07:42:35.636: INFO: Got endpoints: latency-svc-b4ntt [749.750721ms]
Nov 25 07:42:35.674: INFO: Created: latency-svc-rzgdc
Nov 25 07:42:35.690: INFO: Got endpoints: latency-svc-4wgnz [752.742685ms]
Nov 25 07:42:35.702: INFO: Created: latency-svc-8wfls
Nov 25 07:42:35.736: INFO: Got endpoints: latency-svc-fsp8h [725.505753ms]
Nov 25 07:42:35.759: INFO: Created: latency-svc-vm754
Nov 25 07:42:35.786: INFO: Got endpoints: latency-svc-hm4jj [743.76906ms]
Nov 25 07:42:35.797: INFO: Created: latency-svc-99c6g
Nov 25 07:42:35.847: INFO: Got endpoints: latency-svc-ph784 [761.595111ms]
Nov 25 07:42:35.861: INFO: Created: latency-svc-72mbn
Nov 25 07:42:35.886: INFO: Got endpoints: latency-svc-kgpcz [750.558181ms]
Nov 25 07:42:35.901: INFO: Created: latency-svc-gdrt5
Nov 25 07:42:35.940: INFO: Got endpoints: latency-svc-rxhvt [754.245852ms]
Nov 25 07:42:35.967: INFO: Created: latency-svc-zkd6v
Nov 25 07:42:36.011: INFO: Got endpoints: latency-svc-mq7vk [775.204964ms]
Nov 25 07:42:36.044: INFO: Got endpoints: latency-svc-97vpg [757.685732ms]
Nov 25 07:42:36.044: INFO: Created: latency-svc-glm8v
Nov 25 07:42:36.084: INFO: Created: latency-svc-gtkkz
Nov 25 07:42:36.111: INFO: Got endpoints: latency-svc-rtltj [774.772024ms]
Nov 25 07:42:36.123: INFO: Created: latency-svc-nspsx
Nov 25 07:42:36.135: INFO: Got endpoints: latency-svc-89bqp [746.981371ms]
Nov 25 07:42:36.147: INFO: Created: latency-svc-nmhpf
Nov 25 07:42:36.194: INFO: Got endpoints: latency-svc-2wngl [758.627313ms]
Nov 25 07:42:36.212: INFO: Created: latency-svc-7sr75
Nov 25 07:42:36.236: INFO: Got endpoints: latency-svc-lsjld [750.956235ms]
Nov 25 07:42:36.271: INFO: Created: latency-svc-q9dvk
Nov 25 07:42:36.287: INFO: Got endpoints: latency-svc-7pgwv [751.020554ms]
Nov 25 07:42:36.317: INFO: Created: latency-svc-5v7kb
Nov 25 07:42:36.338: INFO: Got endpoints: latency-svc-mhxfc [751.740969ms]
Nov 25 07:42:36.349: INFO: Created: latency-svc-2n5k2
Nov 25 07:42:36.386: INFO: Got endpoints: latency-svc-rzgdc [750.177972ms]
Nov 25 07:42:36.400: INFO: Created: latency-svc-wvdf8
Nov 25 07:42:36.436: INFO: Got endpoints: latency-svc-8wfls [745.826819ms]
Nov 25 07:42:36.467: INFO: Created: latency-svc-qk5bq
Nov 25 07:42:36.487: INFO: Got endpoints: latency-svc-vm754 [751.141496ms]
Nov 25 07:42:36.502: INFO: Created: latency-svc-dkjgg
Nov 25 07:42:36.546: INFO: Got endpoints: latency-svc-99c6g [760.115348ms]
Nov 25 07:42:36.568: INFO: Created: latency-svc-hbtbd
Nov 25 07:42:36.586: INFO: Got endpoints: latency-svc-72mbn [738.846291ms]
Nov 25 07:42:36.602: INFO: Created: latency-svc-bptwc
Nov 25 07:42:36.636: INFO: Got endpoints: latency-svc-gdrt5 [749.909777ms]
Nov 25 07:42:36.670: INFO: Created: latency-svc-p76qd
Nov 25 07:42:36.687: INFO: Got endpoints: latency-svc-zkd6v [746.791633ms]
Nov 25 07:42:36.722: INFO: Created: latency-svc-9tdnc
Nov 25 07:42:36.736: INFO: Got endpoints: latency-svc-glm8v [724.373133ms]
Nov 25 07:42:36.746: INFO: Created: latency-svc-rtvwx
Nov 25 07:42:36.813: INFO: Got endpoints: latency-svc-gtkkz [768.765973ms]
Nov 25 07:42:36.914: INFO: Created: latency-svc-7d68z
Nov 25 07:42:36.915: INFO: Got endpoints: latency-svc-nspsx [804.420328ms]
Nov 25 07:42:36.935: INFO: Got endpoints: latency-svc-nmhpf [799.490248ms]
Nov 25 07:42:36.941: INFO: Got endpoints: latency-svc-7sr75 [746.882657ms]
Nov 25 07:42:36.946: INFO: Created: latency-svc-b56kw
Nov 25 07:42:37.010: INFO: Created: latency-svc-p65cz
Nov 25 07:42:37.034: INFO: Created: latency-svc-zxt8c
Nov 25 07:42:37.034: INFO: Got endpoints: latency-svc-q9dvk [797.455873ms]
Nov 25 07:42:37.061: INFO: Got endpoints: latency-svc-5v7kb [774.514457ms]
Nov 25 07:42:37.083: INFO: Created: latency-svc-h9pdx
Nov 25 07:42:37.086: INFO: Got endpoints: latency-svc-2n5k2 [748.197607ms]
Nov 25 07:42:37.119: INFO: Created: latency-svc-jnljj
Nov 25 07:42:37.151: INFO: Created: latency-svc-zb2fj
Nov 25 07:42:37.151: INFO: Got endpoints: latency-svc-wvdf8 [764.61414ms]
Nov 25 07:42:37.170: INFO: Created: latency-svc-62wnt
Nov 25 07:42:37.212: INFO: Got endpoints: latency-svc-qk5bq [775.392174ms]
Nov 25 07:42:37.226: INFO: Created: latency-svc-5jhl5
Nov 25 07:42:37.235: INFO: Got endpoints: latency-svc-dkjgg [747.726191ms]
Nov 25 07:42:37.247: INFO: Created: latency-svc-9jsxt
Nov 25 07:42:37.309: INFO: Got endpoints: latency-svc-hbtbd [762.719866ms]
Nov 25 07:42:37.366: INFO: Got endpoints: latency-svc-bptwc [779.855301ms]
Nov 25 07:42:37.366: INFO: Created: latency-svc-kzjdx
Nov 25 07:42:37.382: INFO: Created: latency-svc-rhb2g
Nov 25 07:42:37.385: INFO: Got endpoints: latency-svc-p76qd [749.440995ms]
Nov 25 07:42:37.404: INFO: Created: latency-svc-94dwp
Nov 25 07:42:37.436: INFO: Got endpoints: latency-svc-9tdnc [748.650408ms]
Nov 25 07:42:37.447: INFO: Created: latency-svc-dsw5n
Nov 25 07:42:37.490: INFO: Got endpoints: latency-svc-rtvwx [754.322504ms]
Nov 25 07:42:37.501: INFO: Created: latency-svc-cs7qq
Nov 25 07:42:37.541: INFO: Got endpoints: latency-svc-7d68z [727.722024ms]
Nov 25 07:42:37.551: INFO: Created: latency-svc-pgw8t
Nov 25 07:42:37.586: INFO: Got endpoints: latency-svc-b56kw [670.857183ms]
Nov 25 07:42:37.599: INFO: Created: latency-svc-vnpjx
Nov 25 07:42:37.644: INFO: Got endpoints: latency-svc-p65cz [708.962689ms]
Nov 25 07:42:37.655: INFO: Created: latency-svc-p59k4
Nov 25 07:42:37.686: INFO: Got endpoints: latency-svc-zxt8c [744.743288ms]
Nov 25 07:42:37.697: INFO: Created: latency-svc-hsglw
Nov 25 07:42:37.741: INFO: Got endpoints: latency-svc-h9pdx [706.804457ms]
Nov 25 07:42:37.765: INFO: Created: latency-svc-48hh9
Nov 25 07:42:37.786: INFO: Got endpoints: latency-svc-jnljj [724.570943ms]
Nov 25 07:42:37.816: INFO: Created: latency-svc-8dksq
Nov 25 07:42:37.836: INFO: Got endpoints: latency-svc-zb2fj [750.022051ms]
Nov 25 07:42:37.847: INFO: Created: latency-svc-hgdr4
Nov 25 07:42:37.886: INFO: Got endpoints: latency-svc-62wnt [734.747579ms]
Nov 25 07:42:37.931: INFO: Created: latency-svc-htw6t
Nov 25 07:42:37.940: INFO: Got endpoints: latency-svc-5jhl5 [728.535101ms]
Nov 25 07:42:37.956: INFO: Created: latency-svc-p2l8d
Nov 25 07:42:38.020: INFO: Got endpoints: latency-svc-9jsxt [785.084343ms]
Nov 25 07:42:38.032: INFO: Created: latency-svc-8hh5k
Nov 25 07:42:38.036: INFO: Got endpoints: latency-svc-kzjdx [726.875126ms]
Nov 25 07:42:38.059: INFO: Created: latency-svc-4bdg7
Nov 25 07:42:38.086: INFO: Got endpoints: latency-svc-rhb2g [720.141916ms]
Nov 25 07:42:38.100: INFO: Created: latency-svc-g72qf
Nov 25 07:42:38.136: INFO: Got endpoints: latency-svc-94dwp [750.58925ms]
Nov 25 07:42:38.152: INFO: Created: latency-svc-97ds6
Nov 25 07:42:38.186: INFO: Got endpoints: latency-svc-dsw5n [750.42405ms]
Nov 25 07:42:38.212: INFO: Created: latency-svc-pvh98
Nov 25 07:42:38.247: INFO: Got endpoints: latency-svc-cs7qq [757.049774ms]
Nov 25 07:42:38.302: INFO: Got endpoints: latency-svc-pgw8t [761.678732ms]
Nov 25 07:42:38.303: INFO: Created: latency-svc-n6qfb
Nov 25 07:42:38.315: INFO: Created: latency-svc-mv495
Nov 25 07:42:38.336: INFO: Got endpoints: latency-svc-vnpjx [749.76239ms]
Nov 25 07:42:38.361: INFO: Created: latency-svc-g8mwb
Nov 25 07:42:38.386: INFO: Got endpoints: latency-svc-p59k4 [741.854682ms]
Nov 25 07:42:38.397: INFO: Created: latency-svc-69px8
Nov 25 07:42:38.436: INFO: Got endpoints: latency-svc-hsglw [749.758847ms]
Nov 25 07:42:38.449: INFO: Created: latency-svc-nkvn2
Nov 25 07:42:38.489: INFO: Got endpoints: latency-svc-48hh9 [748.70683ms]
Nov 25 07:42:38.501: INFO: Created: latency-svc-tk42v
Nov 25 07:42:38.538: INFO: Got endpoints: latency-svc-8dksq [751.799624ms]
Nov 25 07:42:38.569: INFO: Created: latency-svc-n578c
Nov 25 07:42:38.588: INFO: Got endpoints: latency-svc-hgdr4 [751.39754ms]
Nov 25 07:42:38.612: INFO: Created: latency-svc-bj5xv
Nov 25 07:42:38.638: INFO: Got endpoints: latency-svc-htw6t [751.9224ms]
Nov 25 07:42:38.650: INFO: Created: latency-svc-cjl9m
Nov 25 07:42:38.686: INFO: Got endpoints: latency-svc-p2l8d [745.47289ms]
Nov 25 07:42:38.700: INFO: Created: latency-svc-x7cxz
Nov 25 07:42:38.735: INFO: Got endpoints: latency-svc-8hh5k [715.142511ms]
Nov 25 07:42:38.750: INFO: Created: latency-svc-6229h
Nov 25 07:42:38.809: INFO: Got endpoints: latency-svc-4bdg7 [773.268902ms]
Nov 25 07:42:38.829: INFO: Created: latency-svc-2n4vh
Nov 25 07:42:38.837: INFO: Got endpoints: latency-svc-g72qf [750.498777ms]
Nov 25 07:42:38.847: INFO: Created: latency-svc-gdhhp
Nov 25 07:42:38.891: INFO: Got endpoints: latency-svc-97ds6 [754.466312ms]
Nov 25 07:42:38.909: INFO: Created: latency-svc-lh82b
Nov 25 07:42:38.945: INFO: Got endpoints: latency-svc-pvh98 [758.065554ms]
Nov 25 07:42:38.958: INFO: Created: latency-svc-hs44j
Nov 25 07:42:38.986: INFO: Got endpoints: latency-svc-n6qfb [738.710787ms]
Nov 25 07:42:39.038: INFO: Got endpoints: latency-svc-mv495 [735.179486ms]
Nov 25 07:42:39.043: INFO: Created: latency-svc-h94k5
Nov 25 07:42:39.068: INFO: Created: latency-svc-gg8fb
Nov 25 07:42:39.091: INFO: Got endpoints: latency-svc-g8mwb [754.427373ms]
Nov 25 07:42:39.132: INFO: Created: latency-svc-2l7hl
Nov 25 07:42:39.140: INFO: Got endpoints: latency-svc-69px8 [753.638023ms]
Nov 25 07:42:39.151: INFO: Created: latency-svc-xtz2s
Nov 25 07:42:39.186: INFO: Got endpoints: latency-svc-nkvn2 [749.895343ms]
Nov 25 07:42:39.235: INFO: Created: latency-svc-4nkf2
Nov 25 07:42:39.237: INFO: Got endpoints: latency-svc-tk42v [747.503148ms]
Nov 25 07:42:39.247: INFO: Created: latency-svc-cvn5g
Nov 25 07:42:39.291: INFO: Got endpoints: latency-svc-n578c [752.855327ms]
Nov 25 07:42:39.302: INFO: Created: latency-svc-5swgb
Nov 25 07:42:39.336: INFO: Got endpoints: latency-svc-bj5xv [748.498442ms]
Nov 25 07:42:39.349: INFO: Created: latency-svc-gvtx6
Nov 25 07:42:39.390: INFO: Got endpoints: latency-svc-cjl9m [752.288497ms]
Nov 25 07:42:39.406: INFO: Created: latency-svc-2m2sn
Nov 25 07:42:39.436: INFO: Got endpoints: latency-svc-x7cxz [750.165578ms]
Nov 25 07:42:39.447: INFO: Created: latency-svc-fz5bk
Nov 25 07:42:39.486: INFO: Got endpoints: latency-svc-6229h [750.516322ms]
Nov 25 07:42:39.499: INFO: Created: latency-svc-xwl9f
Nov 25 07:42:39.541: INFO: Got endpoints: latency-svc-2n4vh [731.59913ms]
Nov 25 07:42:39.570: INFO: Created: latency-svc-clq88
Nov 25 07:42:39.586: INFO: Got endpoints: latency-svc-gdhhp [749.293306ms]
Nov 25 07:42:39.641: INFO: Created: latency-svc-bzp66
Nov 25 07:42:39.642: INFO: Got endpoints: latency-svc-lh82b [750.984924ms]
Nov 25 07:42:39.656: INFO: Created: latency-svc-j8tmp
Nov 25 07:42:39.686: INFO: Got endpoints: latency-svc-hs44j [741.297112ms]
Nov 25 07:42:39.696: INFO: Created: latency-svc-dmfmx
Nov 25 07:42:39.736: INFO: Got endpoints: latency-svc-h94k5 [749.975318ms]
Nov 25 07:42:39.753: INFO: Created: latency-svc-kdtpt
Nov 25 07:42:39.791: INFO: Got endpoints: latency-svc-gg8fb [752.915271ms]
Nov 25 07:42:39.802: INFO: Created: latency-svc-v9t9w
Nov 25 07:42:39.836: INFO: Got endpoints: latency-svc-2l7hl [745.542376ms]
Nov 25 07:42:39.850: INFO: Created: latency-svc-msx2r
Nov 25 07:42:39.886: INFO: Got endpoints: latency-svc-xtz2s [746.354975ms]
Nov 25 07:42:39.896: INFO: Created: latency-svc-9sw76
Nov 25 07:42:39.938: INFO: Got endpoints: latency-svc-4nkf2 [752.36925ms]
Nov 25 07:42:39.949: INFO: Created: latency-svc-pxbfb
Nov 25 07:42:40.003: INFO: Got endpoints: latency-svc-cvn5g [766.218408ms]
Nov 25 07:42:40.035: INFO: Created: latency-svc-ts9fl
Nov 25 07:42:40.056: INFO: Got endpoints: latency-svc-5swgb [764.74181ms]
Nov 25 07:42:40.080: INFO: Created: latency-svc-tbtvr
Nov 25 07:42:40.086: INFO: Got endpoints: latency-svc-gvtx6 [749.400719ms]
Nov 25 07:42:40.109: INFO: Created: latency-svc-6xjw6
Nov 25 07:42:40.137: INFO: Got endpoints: latency-svc-2m2sn [747.057826ms]
Nov 25 07:42:40.150: INFO: Created: latency-svc-wdgts
Nov 25 07:42:40.186: INFO: Got endpoints: latency-svc-fz5bk [749.646498ms]
Nov 25 07:42:40.199: INFO: Created: latency-svc-2p8zr
Nov 25 07:42:40.273: INFO: Got endpoints: latency-svc-xwl9f [786.594546ms]
Nov 25 07:42:40.284: INFO: Created: latency-svc-4gpz7
Nov 25 07:42:40.285: INFO: Got endpoints: latency-svc-clq88 [744.615174ms]
Nov 25 07:42:40.298: INFO: Created: latency-svc-7gh5d
Nov 25 07:42:40.336: INFO: Got endpoints: latency-svc-bzp66 [749.88603ms]
Nov 25 07:42:40.350: INFO: Created: latency-svc-6p69c
Nov 25 07:42:40.396: INFO: Got endpoints: latency-svc-j8tmp [753.979323ms]
Nov 25 07:42:40.426: INFO: Created: latency-svc-qtfc2
Nov 25 07:42:40.436: INFO: Got endpoints: latency-svc-dmfmx [750.484918ms]
Nov 25 07:42:40.448: INFO: Created: latency-svc-dkw7q
Nov 25 07:42:40.486: INFO: Got endpoints: latency-svc-kdtpt [750.044873ms]
Nov 25 07:42:40.533: INFO: Created: latency-svc-vz9ln
Nov 25 07:42:40.536: INFO: Got endpoints: latency-svc-v9t9w [745.026633ms]
Nov 25 07:42:40.564: INFO: Created: latency-svc-mdjxm
Nov 25 07:42:40.586: INFO: Got endpoints: latency-svc-msx2r [750.00831ms]
Nov 25 07:42:40.600: INFO: Created: latency-svc-f9dv6
Nov 25 07:42:40.654: INFO: Got endpoints: latency-svc-9sw76 [767.362462ms]
Nov 25 07:42:40.681: INFO: Created: latency-svc-wsjcs
Nov 25 07:42:40.690: INFO: Got endpoints: latency-svc-pxbfb [751.830623ms]
Nov 25 07:42:40.737: INFO: Got endpoints: latency-svc-ts9fl [734.337558ms]
Nov 25 07:42:40.790: INFO: Got endpoints: latency-svc-tbtvr [734.169233ms]
Nov 25 07:42:40.836: INFO: Got endpoints: latency-svc-6xjw6 [750.420343ms]
Nov 25 07:42:40.891: INFO: Got endpoints: latency-svc-wdgts [753.298924ms]
Nov 25 07:42:40.936: INFO: Got endpoints: latency-svc-2p8zr [750.384672ms]
Nov 25 07:42:40.986: INFO: Got endpoints: latency-svc-4gpz7 [713.387486ms]
Nov 25 07:42:41.036: INFO: Got endpoints: latency-svc-7gh5d [750.76108ms]
Nov 25 07:42:41.087: INFO: Got endpoints: latency-svc-6p69c [751.388769ms]
Nov 25 07:42:41.136: INFO: Got endpoints: latency-svc-qtfc2 [740.475643ms]
Nov 25 07:42:41.186: INFO: Got endpoints: latency-svc-dkw7q [749.488038ms]
Nov 25 07:42:41.236: INFO: Got endpoints: latency-svc-vz9ln [749.786615ms]
Nov 25 07:42:41.286: INFO: Got endpoints: latency-svc-mdjxm [750.258601ms]
Nov 25 07:42:41.337: INFO: Got endpoints: latency-svc-f9dv6 [750.595327ms]
Nov 25 07:42:41.388: INFO: Got endpoints: latency-svc-wsjcs [733.932243ms]
Nov 25 07:42:41.388: INFO: Latencies: [16.846997ms 17.475505ms 27.507464ms 74.843415ms 104.897905ms 129.358226ms 133.266486ms 200.149881ms 205.91992ms 236.742253ms 241.10077ms 254.858174ms 257.809928ms 258.364201ms 273.853741ms 282.673404ms 301.161548ms 311.462191ms 313.373474ms 313.930653ms 319.670727ms 327.867282ms 333.417122ms 337.913917ms 342.336146ms 343.868799ms 345.67837ms 346.130341ms 350.077088ms 352.093626ms 352.901951ms 366.044208ms 367.56685ms 373.347456ms 376.208753ms 381.166654ms 381.444852ms 382.443271ms 382.561309ms 382.582949ms 386.770902ms 387.177774ms 390.182148ms 390.821349ms 391.414505ms 395.147312ms 396.801038ms 397.011211ms 398.479487ms 399.917844ms 403.1938ms 405.073419ms 406.052681ms 413.608079ms 414.972271ms 416.532487ms 418.470295ms 423.593888ms 463.508624ms 477.403795ms 502.793703ms 524.533658ms 549.502424ms 575.745902ms 619.508422ms 656.335277ms 670.857183ms 675.276126ms 701.497587ms 706.804457ms 708.962689ms 713.387486ms 715.142511ms 717.016694ms 720.141916ms 724.373133ms 724.570943ms 724.623135ms 725.505753ms 726.385141ms 726.875126ms 727.722024ms 728.535101ms 731.59913ms 733.932243ms 734.169233ms 734.337558ms 734.747579ms 735.179486ms 738.091319ms 738.710787ms 738.846291ms 740.475643ms 741.297112ms 741.854682ms 743.76906ms 743.799626ms 744.615174ms 744.639299ms 744.743288ms 745.026633ms 745.47289ms 745.542376ms 745.826819ms 746.354975ms 746.658843ms 746.791633ms 746.882657ms 746.981371ms 747.057826ms 747.503148ms 747.726191ms 748.124611ms 748.197607ms 748.498442ms 748.650408ms 748.70683ms 749.049625ms 749.293306ms 749.400719ms 749.440995ms 749.488038ms 749.646498ms 749.749278ms 749.750721ms 749.758847ms 749.76239ms 749.786615ms 749.88603ms 749.895343ms 749.909777ms 749.975318ms 750.00831ms 750.022051ms 750.044873ms 750.165578ms 750.177972ms 750.258601ms 750.29296ms 750.384672ms 750.420343ms 750.42405ms 750.426664ms 750.484918ms 750.498777ms 750.516322ms 750.558181ms 750.58925ms 750.595327ms 750.716233ms 750.76108ms 750.956235ms 750.984924ms 751.020554ms 751.141496ms 751.388769ms 751.39754ms 751.740969ms 751.799624ms 751.830623ms 751.9224ms 752.114784ms 752.288497ms 752.36925ms 752.742685ms 752.855327ms 752.915271ms 753.298924ms 753.638023ms 753.979323ms 754.245852ms 754.322504ms 754.427373ms 754.466312ms 756.408277ms 757.049774ms 757.685732ms 758.065554ms 758.627313ms 760.115348ms 761.595111ms 761.678732ms 762.719866ms 764.61414ms 764.74181ms 766.218408ms 767.362462ms 768.765973ms 773.136474ms 773.268902ms 774.514457ms 774.772024ms 775.204964ms 775.392174ms 779.855301ms 785.084343ms 786.594546ms 797.455873ms 799.490248ms 804.420328ms]
Nov 25 07:42:41.388: INFO: 50 %ile: 745.026633ms
Nov 25 07:42:41.388: INFO: 90 %ile: 761.595111ms
Nov 25 07:42:41.388: INFO: 99 %ile: 799.490248ms
Nov 25 07:42:41.388: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:42:41.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6301" for this suite.

• [SLOW TEST:9.766 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":303,"completed":187,"skipped":3041,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:42:41.398: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:42:41.420: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:42:47.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6982" for this suite.

• [SLOW TEST:6.546 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":303,"completed":188,"skipped":3043,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:42:47.944: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov 25 07:42:48.119: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 25 07:43:48.132: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:43:48.134: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Nov 25 07:43:50.297: INFO: found a healthy node: k8sconformance-m02
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:44:00.346: INFO: pods created so far: [1 1 1]
Nov 25 07:44:00.346: INFO: length of pods created so far: 3
Nov 25 07:44:08.353: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:44:15.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2186" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:44:15.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-799" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:87.464 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":303,"completed":189,"skipped":3055,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:44:15.408: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-949.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-949.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-949.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-949.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-949.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-949.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 25 07:44:19.474: INFO: DNS probes using dns-949/dns-test-9f3b817c-753a-4ae4-8a48-245b7105ff1e succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:44:19.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-949" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":303,"completed":190,"skipped":3070,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:44:19.570: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Nov 25 07:44:19.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 api-versions'
Nov 25 07:44:20.203: INFO: stderr: ""
Nov 25 07:44:20.203: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:44:20.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1963" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":303,"completed":191,"skipped":3105,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:44:20.209: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Nov 25 07:44:23.252: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:44:23.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3338" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":303,"completed":192,"skipped":3121,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:44:23.325: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 25 07:44:23.345: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 25 07:44:23.349: INFO: Waiting for terminating namespaces to be deleted...
Nov 25 07:44:23.351: INFO: 
Logging pods the apiserver thinks is on node k8sconformance before test
Nov 25 07:44:23.354: INFO: coredns-f9fd979d6-z4vhz from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:44:23.354: INFO: 	Container coredns ready: true, restart count 0
Nov 25 07:44:23.354: INFO: etcd-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:44:23.354: INFO: 	Container etcd ready: true, restart count 0
Nov 25 07:44:23.354: INFO: kindnet-n566x from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:44:23.354: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 25 07:44:23.354: INFO: kube-apiserver-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:44:23.354: INFO: 	Container kube-apiserver ready: true, restart count 0
Nov 25 07:44:23.354: INFO: kube-controller-manager-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:44:23.354: INFO: 	Container kube-controller-manager ready: true, restart count 0
Nov 25 07:44:23.354: INFO: kube-proxy-58hhs from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:44:23.354: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 25 07:44:23.354: INFO: kube-scheduler-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:44:23.354: INFO: 	Container kube-scheduler ready: true, restart count 0
Nov 25 07:44:23.354: INFO: storage-provisioner from kube-system started at 2020-11-25 06:44:00 +0000 UTC (1 container statuses recorded)
Nov 25 07:44:23.354: INFO: 	Container storage-provisioner ready: true, restart count 0
Nov 25 07:44:23.354: INFO: sonobuoy from sonobuoy started at 2020-11-25 06:44:55 +0000 UTC (1 container statuses recorded)
Nov 25 07:44:23.354: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 25 07:44:23.354: INFO: sonobuoy-e2e-job-7010f57f836a484c from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:44:23.354: INFO: 	Container e2e ready: true, restart count 0
Nov 25 07:44:23.354: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 25 07:44:23.354: INFO: sonobuoy-systemd-logs-daemon-set-25ff39ce45304256-mnllj from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:44:23.354: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 25 07:44:23.354: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 25 07:44:23.354: INFO: 
Logging pods the apiserver thinks is on node k8sconformance-m02 before test
Nov 25 07:44:23.357: INFO: kindnet-6lgph from kube-system started at 2020-11-25 06:44:50 +0000 UTC (1 container statuses recorded)
Nov 25 07:44:23.357: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 25 07:44:23.357: INFO: kube-proxy-8kw9q from kube-system started at 2020-11-25 06:44:50 +0000 UTC (1 container statuses recorded)
Nov 25 07:44:23.357: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 25 07:44:23.357: INFO: pod-adoption-release from replicaset-3338 started at 2020-11-25 07:44:20 +0000 UTC (1 container statuses recorded)
Nov 25 07:44:23.357: INFO: 	Container pod-adoption-release ready: true, restart count 0
Nov 25 07:44:23.357: INFO: pod-adoption-release-gvr8n from replicaset-3338 started at 2020-11-25 07:44:23 +0000 UTC (1 container statuses recorded)
Nov 25 07:44:23.357: INFO: 	Container pod-adoption-release ready: false, restart count 0
Nov 25 07:44:23.357: INFO: pod4 from sched-preemption-path-2186 started at 2020-11-25 07:44:06 +0000 UTC (1 container statuses recorded)
Nov 25 07:44:23.357: INFO: 	Container pod4 ready: false, restart count 0
Nov 25 07:44:23.357: INFO: sonobuoy-systemd-logs-daemon-set-25ff39ce45304256-nnr45 from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:44:23.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 25 07:44:23.357: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-0668f0f0-6bf0-46c4-833d-c8ae23fc04f3 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-0668f0f0-6bf0-46c4-833d-c8ae23fc04f3 off the node k8sconformance-m02
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0668f0f0-6bf0-46c4-833d-c8ae23fc04f3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:49:27.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-580" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.112 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":303,"completed":193,"skipped":3124,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:49:27.437: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Nov 25 07:49:27.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 create -f - --namespace=kubectl-1113'
Nov 25 07:49:28.264: INFO: stderr: ""
Nov 25 07:49:28.264: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 25 07:49:29.267: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 25 07:49:29.267: INFO: Found 0 / 1
Nov 25 07:49:30.267: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 25 07:49:30.267: INFO: Found 1 / 1
Nov 25 07:49:30.267: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov 25 07:49:30.287: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 25 07:49:30.287: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 25 07:49:30.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 patch pod agnhost-primary-x468p --namespace=kubectl-1113 -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 25 07:49:30.360: INFO: stderr: ""
Nov 25 07:49:30.360: INFO: stdout: "pod/agnhost-primary-x468p patched\n"
STEP: checking annotations
Nov 25 07:49:30.363: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 25 07:49:30.363: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:49:30.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1113" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":303,"completed":194,"skipped":3146,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:49:30.368: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 25 07:49:30.418: INFO: Waiting up to 5m0s for pod "pod-a89b1be8-8bc6-419c-8739-6fff28bf869d" in namespace "emptydir-1128" to be "Succeeded or Failed"
Nov 25 07:49:30.421: INFO: Pod "pod-a89b1be8-8bc6-419c-8739-6fff28bf869d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.519259ms
Nov 25 07:49:32.425: INFO: Pod "pod-a89b1be8-8bc6-419c-8739-6fff28bf869d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006818165s
STEP: Saw pod success
Nov 25 07:49:32.425: INFO: Pod "pod-a89b1be8-8bc6-419c-8739-6fff28bf869d" satisfied condition "Succeeded or Failed"
Nov 25 07:49:32.427: INFO: Trying to get logs from node k8sconformance-m02 pod pod-a89b1be8-8bc6-419c-8739-6fff28bf869d container test-container: <nil>
STEP: delete the pod
Nov 25 07:49:32.448: INFO: Waiting for pod pod-a89b1be8-8bc6-419c-8739-6fff28bf869d to disappear
Nov 25 07:49:32.453: INFO: Pod pod-a89b1be8-8bc6-419c-8739-6fff28bf869d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:49:32.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1128" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":195,"skipped":3163,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:49:32.458: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-jts2
STEP: Creating a pod to test atomic-volume-subpath
Nov 25 07:49:32.510: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jts2" in namespace "subpath-4451" to be "Succeeded or Failed"
Nov 25 07:49:32.519: INFO: Pod "pod-subpath-test-configmap-jts2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.516445ms
Nov 25 07:49:34.522: INFO: Pod "pod-subpath-test-configmap-jts2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011634251s
Nov 25 07:49:36.525: INFO: Pod "pod-subpath-test-configmap-jts2": Phase="Running", Reason="", readiness=true. Elapsed: 4.014628167s
Nov 25 07:49:38.528: INFO: Pod "pod-subpath-test-configmap-jts2": Phase="Running", Reason="", readiness=true. Elapsed: 6.017536764s
Nov 25 07:49:40.530: INFO: Pod "pod-subpath-test-configmap-jts2": Phase="Running", Reason="", readiness=true. Elapsed: 8.02032341s
Nov 25 07:49:42.533: INFO: Pod "pod-subpath-test-configmap-jts2": Phase="Running", Reason="", readiness=true. Elapsed: 10.023316803s
Nov 25 07:49:44.536: INFO: Pod "pod-subpath-test-configmap-jts2": Phase="Running", Reason="", readiness=true. Elapsed: 12.026383311s
Nov 25 07:49:46.540: INFO: Pod "pod-subpath-test-configmap-jts2": Phase="Running", Reason="", readiness=true. Elapsed: 14.029515519s
Nov 25 07:49:48.543: INFO: Pod "pod-subpath-test-configmap-jts2": Phase="Running", Reason="", readiness=true. Elapsed: 16.032520574s
Nov 25 07:49:50.546: INFO: Pod "pod-subpath-test-configmap-jts2": Phase="Running", Reason="", readiness=true. Elapsed: 18.035818352s
Nov 25 07:49:52.549: INFO: Pod "pod-subpath-test-configmap-jts2": Phase="Running", Reason="", readiness=true. Elapsed: 20.038784717s
Nov 25 07:49:54.552: INFO: Pod "pod-subpath-test-configmap-jts2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.041680085s
STEP: Saw pod success
Nov 25 07:49:54.552: INFO: Pod "pod-subpath-test-configmap-jts2" satisfied condition "Succeeded or Failed"
Nov 25 07:49:54.554: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-configmap-jts2 container test-container-subpath-configmap-jts2: <nil>
STEP: delete the pod
Nov 25 07:49:54.569: INFO: Waiting for pod pod-subpath-test-configmap-jts2 to disappear
Nov 25 07:49:54.571: INFO: Pod pod-subpath-test-configmap-jts2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jts2
Nov 25 07:49:54.571: INFO: Deleting pod "pod-subpath-test-configmap-jts2" in namespace "subpath-4451"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:49:54.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4451" for this suite.

• [SLOW TEST:22.120 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":303,"completed":196,"skipped":3174,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:49:54.578: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-8vrgb in namespace proxy-5157
I1125 07:49:54.610941      23 runners.go:190] Created replication controller with name: proxy-service-8vrgb, namespace: proxy-5157, replica count: 1
I1125 07:49:55.661358      23 runners.go:190] proxy-service-8vrgb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1125 07:49:56.661650      23 runners.go:190] proxy-service-8vrgb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1125 07:49:57.661851      23 runners.go:190] proxy-service-8vrgb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1125 07:49:58.662052      23 runners.go:190] proxy-service-8vrgb Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 25 07:49:58.664: INFO: setup took 4.066874612s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov 25 07:49:58.673: INFO: (0) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 8.858971ms)
Nov 25 07:49:58.673: INFO: (0) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 8.901588ms)
Nov 25 07:49:58.673: INFO: (0) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 8.792824ms)
Nov 25 07:49:58.675: INFO: (0) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 11.288663ms)
Nov 25 07:49:58.675: INFO: (0) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 11.254012ms)
Nov 25 07:49:58.675: INFO: (0) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 11.359829ms)
Nov 25 07:49:58.675: INFO: (0) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 11.292078ms)
Nov 25 07:49:58.676: INFO: (0) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 11.339791ms)
Nov 25 07:49:58.676: INFO: (0) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 11.363102ms)
Nov 25 07:49:58.676: INFO: (0) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 11.494957ms)
Nov 25 07:49:58.677: INFO: (0) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 12.629589ms)
Nov 25 07:49:58.677: INFO: (0) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 12.721476ms)
Nov 25 07:49:58.678: INFO: (0) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 13.9263ms)
Nov 25 07:49:58.678: INFO: (0) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 14.340657ms)
Nov 25 07:49:58.679: INFO: (0) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 15.16662ms)
Nov 25 07:49:58.680: INFO: (0) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 15.436035ms)
Nov 25 07:49:58.685: INFO: (1) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 5.638004ms)
Nov 25 07:49:58.686: INFO: (1) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 5.70501ms)
Nov 25 07:49:58.686: INFO: (1) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 5.667617ms)
Nov 25 07:49:58.686: INFO: (1) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 5.768508ms)
Nov 25 07:49:58.686: INFO: (1) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 5.81171ms)
Nov 25 07:49:58.686: INFO: (1) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 5.71244ms)
Nov 25 07:49:58.686: INFO: (1) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 5.756667ms)
Nov 25 07:49:58.686: INFO: (1) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 5.798182ms)
Nov 25 07:49:58.686: INFO: (1) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 5.898853ms)
Nov 25 07:49:58.686: INFO: (1) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 5.709688ms)
Nov 25 07:49:58.686: INFO: (1) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 5.760267ms)
Nov 25 07:49:58.686: INFO: (1) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 5.81093ms)
Nov 25 07:49:58.686: INFO: (1) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 5.885405ms)
Nov 25 07:49:58.686: INFO: (1) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 5.967939ms)
Nov 25 07:49:58.686: INFO: (1) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 6.038189ms)
Nov 25 07:49:58.687: INFO: (1) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 7.153424ms)
Nov 25 07:49:58.691: INFO: (2) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 4.128288ms)
Nov 25 07:49:58.692: INFO: (2) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 4.990587ms)
Nov 25 07:49:58.692: INFO: (2) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 4.965866ms)
Nov 25 07:49:58.692: INFO: (2) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 4.957864ms)
Nov 25 07:49:58.692: INFO: (2) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 5.091031ms)
Nov 25 07:49:58.692: INFO: (2) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 5.019647ms)
Nov 25 07:49:58.692: INFO: (2) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 5.089249ms)
Nov 25 07:49:58.693: INFO: (2) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 6.133084ms)
Nov 25 07:49:58.693: INFO: (2) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 6.075332ms)
Nov 25 07:49:58.693: INFO: (2) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 6.151163ms)
Nov 25 07:49:58.693: INFO: (2) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 6.073203ms)
Nov 25 07:49:58.693: INFO: (2) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 6.01545ms)
Nov 25 07:49:58.693: INFO: (2) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 6.051362ms)
Nov 25 07:49:58.693: INFO: (2) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 6.035079ms)
Nov 25 07:49:58.693: INFO: (2) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 6.144162ms)
Nov 25 07:49:58.693: INFO: (2) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 6.248044ms)
Nov 25 07:49:58.695: INFO: (3) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 1.921536ms)
Nov 25 07:49:58.695: INFO: (3) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 1.838252ms)
Nov 25 07:49:58.698: INFO: (3) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 4.909544ms)
Nov 25 07:49:58.699: INFO: (3) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 4.909378ms)
Nov 25 07:49:58.698: INFO: (3) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 4.970159ms)
Nov 25 07:49:58.698: INFO: (3) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 4.929695ms)
Nov 25 07:49:58.699: INFO: (3) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 5.005125ms)
Nov 25 07:49:58.699: INFO: (3) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 4.958862ms)
Nov 25 07:49:58.699: INFO: (3) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 5.024744ms)
Nov 25 07:49:58.699: INFO: (3) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 4.981332ms)
Nov 25 07:49:58.699: INFO: (3) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 5.004881ms)
Nov 25 07:49:58.699: INFO: (3) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 5.050272ms)
Nov 25 07:49:58.699: INFO: (3) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 5.054822ms)
Nov 25 07:49:58.699: INFO: (3) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 4.989843ms)
Nov 25 07:49:58.699: INFO: (3) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 5.131886ms)
Nov 25 07:49:58.699: INFO: (3) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 5.146699ms)
Nov 25 07:49:58.703: INFO: (4) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 3.776397ms)
Nov 25 07:49:58.703: INFO: (4) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 3.878955ms)
Nov 25 07:49:58.703: INFO: (4) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 3.895193ms)
Nov 25 07:49:58.703: INFO: (4) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 3.890767ms)
Nov 25 07:49:58.703: INFO: (4) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 3.874791ms)
Nov 25 07:49:58.703: INFO: (4) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 3.911279ms)
Nov 25 07:49:58.703: INFO: (4) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 4.173091ms)
Nov 25 07:49:58.703: INFO: (4) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 4.00638ms)
Nov 25 07:49:58.703: INFO: (4) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 3.951725ms)
Nov 25 07:49:58.703: INFO: (4) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 3.780112ms)
Nov 25 07:49:58.703: INFO: (4) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 4.116697ms)
Nov 25 07:49:58.704: INFO: (4) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 4.729182ms)
Nov 25 07:49:58.705: INFO: (4) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 5.914343ms)
Nov 25 07:49:58.705: INFO: (4) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 5.870481ms)
Nov 25 07:49:58.705: INFO: (4) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 5.920589ms)
Nov 25 07:49:58.705: INFO: (4) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 5.692986ms)
Nov 25 07:49:58.708: INFO: (5) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 3.146611ms)
Nov 25 07:49:58.708: INFO: (5) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 3.373342ms)
Nov 25 07:49:58.708: INFO: (5) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 3.506865ms)
Nov 25 07:49:58.708: INFO: (5) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 3.604988ms)
Nov 25 07:49:58.709: INFO: (5) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 3.60715ms)
Nov 25 07:49:58.709: INFO: (5) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 3.641332ms)
Nov 25 07:49:58.709: INFO: (5) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 3.67558ms)
Nov 25 07:49:58.709: INFO: (5) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 3.720619ms)
Nov 25 07:49:58.709: INFO: (5) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 3.704089ms)
Nov 25 07:49:58.709: INFO: (5) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 3.858625ms)
Nov 25 07:49:58.710: INFO: (5) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 5.108616ms)
Nov 25 07:49:58.710: INFO: (5) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 5.283173ms)
Nov 25 07:49:58.710: INFO: (5) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 5.260137ms)
Nov 25 07:49:58.710: INFO: (5) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 5.224203ms)
Nov 25 07:49:58.710: INFO: (5) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 5.461109ms)
Nov 25 07:49:58.710: INFO: (5) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 5.374313ms)
Nov 25 07:49:58.713: INFO: (6) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 2.362936ms)
Nov 25 07:49:58.713: INFO: (6) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 2.728513ms)
Nov 25 07:49:58.713: INFO: (6) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 2.585647ms)
Nov 25 07:49:58.713: INFO: (6) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 2.377648ms)
Nov 25 07:49:58.713: INFO: (6) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 2.004344ms)
Nov 25 07:49:58.713: INFO: (6) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 2.477588ms)
Nov 25 07:49:58.714: INFO: (6) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 3.819906ms)
Nov 25 07:49:58.714: INFO: (6) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 3.358358ms)
Nov 25 07:49:58.715: INFO: (6) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 4.495336ms)
Nov 25 07:49:58.715: INFO: (6) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 4.509515ms)
Nov 25 07:49:58.715: INFO: (6) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 4.490359ms)
Nov 25 07:49:58.715: INFO: (6) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 4.390806ms)
Nov 25 07:49:58.715: INFO: (6) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 4.445598ms)
Nov 25 07:49:58.715: INFO: (6) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 4.332883ms)
Nov 25 07:49:58.716: INFO: (6) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 5.062285ms)
Nov 25 07:49:58.716: INFO: (6) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 5.162475ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 4.780177ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 4.774912ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 4.799295ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 4.814346ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 4.825908ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 4.855584ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 4.91829ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 4.874544ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 4.931045ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 4.879389ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 4.917386ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 5.012568ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 4.956459ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 4.868945ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 4.94251ms)
Nov 25 07:49:58.721: INFO: (7) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 4.949949ms)
Nov 25 07:49:58.725: INFO: (8) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 4.155662ms)
Nov 25 07:49:58.725: INFO: (8) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 4.30082ms)
Nov 25 07:49:58.725: INFO: (8) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 4.41317ms)
Nov 25 07:49:58.725: INFO: (8) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 4.649397ms)
Nov 25 07:49:58.726: INFO: (8) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 4.75804ms)
Nov 25 07:49:58.726: INFO: (8) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 4.968387ms)
Nov 25 07:49:58.726: INFO: (8) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 5.695582ms)
Nov 25 07:49:58.727: INFO: (8) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 5.665328ms)
Nov 25 07:49:58.727: INFO: (8) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 5.789877ms)
Nov 25 07:49:58.727: INFO: (8) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 5.656577ms)
Nov 25 07:49:58.727: INFO: (8) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 5.628845ms)
Nov 25 07:49:58.727: INFO: (8) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 5.648503ms)
Nov 25 07:49:58.727: INFO: (8) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 5.762657ms)
Nov 25 07:49:58.727: INFO: (8) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 5.770176ms)
Nov 25 07:49:58.727: INFO: (8) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 5.748179ms)
Nov 25 07:49:58.727: INFO: (8) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 6.073878ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 3.746271ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 3.767802ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 3.935862ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 3.847945ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 3.025742ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 3.922057ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 4.00418ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 3.962002ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 3.87834ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 4.047115ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 3.914507ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 3.919444ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 3.900657ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 4.169543ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 4.113569ms)
Nov 25 07:49:58.731: INFO: (9) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 4.141008ms)
Nov 25 07:49:58.734: INFO: (10) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 2.022205ms)
Nov 25 07:49:58.734: INFO: (10) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 2.498215ms)
Nov 25 07:49:58.734: INFO: (10) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 2.409429ms)
Nov 25 07:49:58.736: INFO: (10) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 3.580816ms)
Nov 25 07:49:58.736: INFO: (10) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 3.809016ms)
Nov 25 07:49:58.736: INFO: (10) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 3.697224ms)
Nov 25 07:49:58.736: INFO: (10) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 4.19717ms)
Nov 25 07:49:58.736: INFO: (10) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 4.436667ms)
Nov 25 07:49:58.736: INFO: (10) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 4.590646ms)
Nov 25 07:49:58.736: INFO: (10) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 4.597621ms)
Nov 25 07:49:58.736: INFO: (10) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 4.754072ms)
Nov 25 07:49:58.736: INFO: (10) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 4.884536ms)
Nov 25 07:49:58.737: INFO: (10) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 4.93742ms)
Nov 25 07:49:58.737: INFO: (10) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 5.479811ms)
Nov 25 07:49:58.738: INFO: (10) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 6.167332ms)
Nov 25 07:49:58.738: INFO: (10) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 6.417905ms)
Nov 25 07:49:58.742: INFO: (11) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 4.409125ms)
Nov 25 07:49:58.743: INFO: (11) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 4.47594ms)
Nov 25 07:49:58.743: INFO: (11) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 4.64514ms)
Nov 25 07:49:58.743: INFO: (11) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 4.509367ms)
Nov 25 07:49:58.743: INFO: (11) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 4.492254ms)
Nov 25 07:49:58.743: INFO: (11) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 4.585871ms)
Nov 25 07:49:58.743: INFO: (11) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 4.55415ms)
Nov 25 07:49:58.743: INFO: (11) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 4.628406ms)
Nov 25 07:49:58.743: INFO: (11) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 4.630578ms)
Nov 25 07:49:58.743: INFO: (11) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 4.679138ms)
Nov 25 07:49:58.743: INFO: (11) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 4.522525ms)
Nov 25 07:49:58.743: INFO: (11) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 4.554993ms)
Nov 25 07:49:58.744: INFO: (11) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 5.987861ms)
Nov 25 07:49:58.744: INFO: (11) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 5.926126ms)
Nov 25 07:49:58.744: INFO: (11) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 6.132884ms)
Nov 25 07:49:58.744: INFO: (11) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 6.012387ms)
Nov 25 07:49:58.748: INFO: (12) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 3.14462ms)
Nov 25 07:49:58.748: INFO: (12) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 3.08588ms)
Nov 25 07:49:58.748: INFO: (12) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 3.281752ms)
Nov 25 07:49:58.748: INFO: (12) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 3.360406ms)
Nov 25 07:49:58.748: INFO: (12) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 3.234331ms)
Nov 25 07:49:58.748: INFO: (12) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 3.176367ms)
Nov 25 07:49:58.748: INFO: (12) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 3.750744ms)
Nov 25 07:49:58.748: INFO: (12) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 4.02228ms)
Nov 25 07:49:58.748: INFO: (12) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 4.232974ms)
Nov 25 07:49:58.748: INFO: (12) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 4.146755ms)
Nov 25 07:49:58.749: INFO: (12) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 4.590254ms)
Nov 25 07:49:58.749: INFO: (12) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 4.363967ms)
Nov 25 07:49:58.749: INFO: (12) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 4.416692ms)
Nov 25 07:49:58.749: INFO: (12) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 4.355607ms)
Nov 25 07:49:58.749: INFO: (12) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 5.034251ms)
Nov 25 07:49:58.749: INFO: (12) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 4.638222ms)
Nov 25 07:49:58.810: INFO: (13) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 60.616389ms)
Nov 25 07:49:58.810: INFO: (13) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 60.583297ms)
Nov 25 07:49:58.810: INFO: (13) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 60.897873ms)
Nov 25 07:49:58.810: INFO: (13) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 60.97336ms)
Nov 25 07:49:58.810: INFO: (13) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 60.978628ms)
Nov 25 07:49:58.810: INFO: (13) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 61.009019ms)
Nov 25 07:49:58.810: INFO: (13) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 61.010843ms)
Nov 25 07:49:58.811: INFO: (13) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 61.417546ms)
Nov 25 07:49:58.811: INFO: (13) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 61.404149ms)
Nov 25 07:49:58.811: INFO: (13) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 61.450208ms)
Nov 25 07:49:58.811: INFO: (13) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 61.528396ms)
Nov 25 07:49:58.811: INFO: (13) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 61.443673ms)
Nov 25 07:49:58.811: INFO: (13) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 61.544229ms)
Nov 25 07:49:58.811: INFO: (13) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 61.586748ms)
Nov 25 07:49:58.811: INFO: (13) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 61.531063ms)
Nov 25 07:49:58.811: INFO: (13) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 61.63837ms)
Nov 25 07:49:58.815: INFO: (14) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 2.942066ms)
Nov 25 07:49:58.815: INFO: (14) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 3.198834ms)
Nov 25 07:49:58.815: INFO: (14) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 3.427034ms)
Nov 25 07:49:58.815: INFO: (14) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 3.333481ms)
Nov 25 07:49:58.815: INFO: (14) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 3.096431ms)
Nov 25 07:49:58.815: INFO: (14) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 2.918513ms)
Nov 25 07:49:58.815: INFO: (14) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 3.084394ms)
Nov 25 07:49:58.815: INFO: (14) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 3.254251ms)
Nov 25 07:49:58.815: INFO: (14) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 3.674926ms)
Nov 25 07:49:58.815: INFO: (14) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 3.465873ms)
Nov 25 07:49:58.815: INFO: (14) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 3.409434ms)
Nov 25 07:49:58.815: INFO: (14) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 4.035538ms)
Nov 25 07:49:58.816: INFO: (14) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 3.94652ms)
Nov 25 07:49:58.816: INFO: (14) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 4.12394ms)
Nov 25 07:49:58.816: INFO: (14) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 4.371621ms)
Nov 25 07:49:58.816: INFO: (14) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 4.222004ms)
Nov 25 07:49:58.818: INFO: (15) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 1.829061ms)
Nov 25 07:49:58.818: INFO: (15) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 2.024827ms)
Nov 25 07:49:58.819: INFO: (15) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 3.281173ms)
Nov 25 07:49:58.819: INFO: (15) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 3.182094ms)
Nov 25 07:49:58.819: INFO: (15) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 3.180239ms)
Nov 25 07:49:58.819: INFO: (15) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 3.308764ms)
Nov 25 07:49:58.819: INFO: (15) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 3.22431ms)
Nov 25 07:49:58.819: INFO: (15) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 3.215467ms)
Nov 25 07:49:58.820: INFO: (15) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 4.238338ms)
Nov 25 07:49:58.820: INFO: (15) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 4.120115ms)
Nov 25 07:49:58.820: INFO: (15) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 4.186707ms)
Nov 25 07:49:58.820: INFO: (15) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 4.162291ms)
Nov 25 07:49:58.821: INFO: (15) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 5.479318ms)
Nov 25 07:49:58.821: INFO: (15) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 5.486944ms)
Nov 25 07:49:58.821: INFO: (15) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 5.487073ms)
Nov 25 07:49:58.821: INFO: (15) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 5.556376ms)
Nov 25 07:49:58.825: INFO: (16) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 2.902888ms)
Nov 25 07:49:58.825: INFO: (16) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 2.83688ms)
Nov 25 07:49:58.825: INFO: (16) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 2.930276ms)
Nov 25 07:49:58.825: INFO: (16) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 2.963373ms)
Nov 25 07:49:58.825: INFO: (16) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 2.919094ms)
Nov 25 07:49:58.825: INFO: (16) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 2.945469ms)
Nov 25 07:49:58.825: INFO: (16) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 2.999121ms)
Nov 25 07:49:58.825: INFO: (16) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 2.932733ms)
Nov 25 07:49:58.825: INFO: (16) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 3.200402ms)
Nov 25 07:49:58.825: INFO: (16) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 3.068979ms)
Nov 25 07:49:58.826: INFO: (16) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 4.491636ms)
Nov 25 07:49:58.826: INFO: (16) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 4.50298ms)
Nov 25 07:49:58.826: INFO: (16) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 4.63736ms)
Nov 25 07:49:58.826: INFO: (16) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 4.540531ms)
Nov 25 07:49:58.826: INFO: (16) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 4.554953ms)
Nov 25 07:49:58.826: INFO: (16) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 4.66434ms)
Nov 25 07:49:58.828: INFO: (17) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 1.880457ms)
Nov 25 07:49:58.828: INFO: (17) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 1.898734ms)
Nov 25 07:49:58.829: INFO: (17) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 2.283489ms)
Nov 25 07:49:58.830: INFO: (17) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 3.772729ms)
Nov 25 07:49:58.830: INFO: (17) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 3.851389ms)
Nov 25 07:49:58.830: INFO: (17) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 3.844078ms)
Nov 25 07:49:58.830: INFO: (17) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 3.926258ms)
Nov 25 07:49:58.830: INFO: (17) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 3.902894ms)
Nov 25 07:49:58.830: INFO: (17) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 3.864314ms)
Nov 25 07:49:58.830: INFO: (17) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 3.903156ms)
Nov 25 07:49:58.830: INFO: (17) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 4.017461ms)
Nov 25 07:49:58.830: INFO: (17) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 4.002815ms)
Nov 25 07:49:58.831: INFO: (17) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 4.084444ms)
Nov 25 07:49:58.831: INFO: (17) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 3.984875ms)
Nov 25 07:49:58.831: INFO: (17) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 4.053818ms)
Nov 25 07:49:58.831: INFO: (17) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 4.125074ms)
Nov 25 07:49:58.834: INFO: (18) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 3.35749ms)
Nov 25 07:49:58.834: INFO: (18) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 3.855626ms)
Nov 25 07:49:58.834: INFO: (18) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 3.782585ms)
Nov 25 07:49:58.834: INFO: (18) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 3.608889ms)
Nov 25 07:49:58.834: INFO: (18) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 3.783469ms)
Nov 25 07:49:58.836: INFO: (18) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 5.251914ms)
Nov 25 07:49:58.836: INFO: (18) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 5.015601ms)
Nov 25 07:49:58.836: INFO: (18) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 4.961132ms)
Nov 25 07:49:58.836: INFO: (18) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 5.215387ms)
Nov 25 07:49:58.836: INFO: (18) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 5.099327ms)
Nov 25 07:49:58.836: INFO: (18) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 4.962932ms)
Nov 25 07:49:58.837: INFO: (18) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 5.634702ms)
Nov 25 07:49:58.837: INFO: (18) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 6.168592ms)
Nov 25 07:49:58.837: INFO: (18) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 6.385408ms)
Nov 25 07:49:58.838: INFO: (18) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 6.768918ms)
Nov 25 07:49:58.838: INFO: (18) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 6.640698ms)
Nov 25 07:49:58.840: INFO: (19) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6/proxy/rewriteme">test</a> (200; 2.301555ms)
Nov 25 07:49:58.840: INFO: (19) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 2.29285ms)
Nov 25 07:49:58.840: INFO: (19) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">test<... (200; 2.447469ms)
Nov 25 07:49:58.909: INFO: (19) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 70.615435ms)
Nov 25 07:49:58.909: INFO: (19) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname2/proxy/: tls qux (200; 70.685678ms)
Nov 25 07:49:58.909: INFO: (19) /api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/http:proxy-service-8vrgb-vllq6:1080/proxy/rewriteme">... (200; 70.718918ms)
Nov 25 07:49:58.909: INFO: (19) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:460/proxy/: tls baz (200; 70.762078ms)
Nov 25 07:49:58.909: INFO: (19) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/: <a href="/api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:443/proxy/tlsrewritem... (200; 70.846926ms)
Nov 25 07:49:58.909: INFO: (19) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname2/proxy/: bar (200; 70.74191ms)
Nov 25 07:49:58.909: INFO: (19) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:162/proxy/: bar (200; 70.745792ms)
Nov 25 07:49:58.909: INFO: (19) /api/v1/namespaces/proxy-5157/pods/proxy-service-8vrgb-vllq6:160/proxy/: foo (200; 70.910462ms)
Nov 25 07:49:58.909: INFO: (19) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname2/proxy/: bar (200; 70.923939ms)
Nov 25 07:49:58.909: INFO: (19) /api/v1/namespaces/proxy-5157/services/proxy-service-8vrgb:portname1/proxy/: foo (200; 70.996926ms)
Nov 25 07:49:58.915: INFO: (19) /api/v1/namespaces/proxy-5157/pods/https:proxy-service-8vrgb-vllq6:462/proxy/: tls qux (200; 76.976744ms)
Nov 25 07:49:58.915: INFO: (19) /api/v1/namespaces/proxy-5157/services/https:proxy-service-8vrgb:tlsportname1/proxy/: tls baz (200; 77.061684ms)
Nov 25 07:49:58.923: INFO: (19) /api/v1/namespaces/proxy-5157/services/http:proxy-service-8vrgb:portname1/proxy/: foo (200; 85.131213ms)
STEP: deleting ReplicationController proxy-service-8vrgb in namespace proxy-5157, will wait for the garbage collector to delete the pods
Nov 25 07:49:59.019: INFO: Deleting ReplicationController proxy-service-8vrgb took: 42.990883ms
Nov 25 07:49:59.419: INFO: Terminating ReplicationController proxy-service-8vrgb pods took: 400.216312ms
[AfterEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:50:01.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5157" for this suite.

• [SLOW TEST:6.747 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":303,"completed":197,"skipped":3207,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:50:01.325: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Nov 25 07:50:01.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-6472 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov 25 07:50:01.433: INFO: stderr: ""
Nov 25 07:50:01.433: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Nov 25 07:50:01.433: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov 25 07:50:01.433: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6472" to be "running and ready, or succeeded"
Nov 25 07:50:01.435: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.86665ms
Nov 25 07:50:03.437: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.004718292s
Nov 25 07:50:03.437: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov 25 07:50:03.437: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Nov 25 07:50:03.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 logs logs-generator logs-generator --namespace=kubectl-6472'
Nov 25 07:50:03.514: INFO: stderr: ""
Nov 25 07:50:03.514: INFO: stdout: "I1125 07:50:02.413535       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/s4gw 204\nI1125 07:50:02.613721       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/962f 496\nI1125 07:50:02.813718       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/mrgf 461\nI1125 07:50:03.013708       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/rkg 579\nI1125 07:50:03.213703       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/rhw 202\nI1125 07:50:03.413706       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/5vk 565\n"
STEP: limiting log lines
Nov 25 07:50:03.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 logs logs-generator logs-generator --namespace=kubectl-6472 --tail=1'
Nov 25 07:50:03.588: INFO: stderr: ""
Nov 25 07:50:03.588: INFO: stdout: "I1125 07:50:03.413706       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/5vk 565\n"
Nov 25 07:50:03.588: INFO: got output "I1125 07:50:03.413706       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/5vk 565\n"
STEP: limiting log bytes
Nov 25 07:50:03.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 logs logs-generator logs-generator --namespace=kubectl-6472 --limit-bytes=1'
Nov 25 07:50:03.663: INFO: stderr: ""
Nov 25 07:50:03.663: INFO: stdout: "I"
Nov 25 07:50:03.663: INFO: got output "I"
STEP: exposing timestamps
Nov 25 07:50:03.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 logs logs-generator logs-generator --namespace=kubectl-6472 --tail=1 --timestamps'
Nov 25 07:50:03.738: INFO: stderr: ""
Nov 25 07:50:03.738: INFO: stdout: "2020-11-25T07:50:03.613867225Z I1125 07:50:03.613696       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/m8w7 205\n"
Nov 25 07:50:03.738: INFO: got output "2020-11-25T07:50:03.613867225Z I1125 07:50:03.613696       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/m8w7 205\n"
STEP: restricting to a time range
Nov 25 07:50:06.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 logs logs-generator logs-generator --namespace=kubectl-6472 --since=1s'
Nov 25 07:50:06.317: INFO: stderr: ""
Nov 25 07:50:06.317: INFO: stdout: "I1125 07:50:05.413702       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/9swq 564\nI1125 07:50:05.613584       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/884 521\nI1125 07:50:05.813695       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/tcfn 480\nI1125 07:50:06.013738       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/8qm 269\nI1125 07:50:06.213734       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/xm4v 323\n"
Nov 25 07:50:06.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 logs logs-generator logs-generator --namespace=kubectl-6472 --since=24h'
Nov 25 07:50:06.434: INFO: stderr: ""
Nov 25 07:50:06.434: INFO: stdout: "I1125 07:50:02.413535       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/s4gw 204\nI1125 07:50:02.613721       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/962f 496\nI1125 07:50:02.813718       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/mrgf 461\nI1125 07:50:03.013708       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/rkg 579\nI1125 07:50:03.213703       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/rhw 202\nI1125 07:50:03.413706       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/5vk 565\nI1125 07:50:03.613696       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/m8w7 205\nI1125 07:50:03.813719       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/nd55 346\nI1125 07:50:04.013708       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/6kx5 244\nI1125 07:50:04.213700       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/m7w 482\nI1125 07:50:04.413700       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/j29 405\nI1125 07:50:04.613705       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/4hdt 428\nI1125 07:50:04.813684       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/79k 509\nI1125 07:50:05.013727       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/t4km 424\nI1125 07:50:05.213700       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/xm2v 488\nI1125 07:50:05.413702       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/9swq 564\nI1125 07:50:05.613584       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/884 521\nI1125 07:50:05.813695       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/tcfn 480\nI1125 07:50:06.013738       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/8qm 269\nI1125 07:50:06.213734       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/xm4v 323\nI1125 07:50:06.413705       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/wwhf 344\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Nov 25 07:50:06.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 delete pod logs-generator --namespace=kubectl-6472'
Nov 25 07:50:10.660: INFO: stderr: ""
Nov 25 07:50:10.660: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:50:10.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6472" for this suite.

• [SLOW TEST:9.340 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":303,"completed":198,"skipped":3215,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:50:10.666: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 07:50:10.693: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b41fd62b-7ef5-4cbd-bde7-6e1ccb08d30d" in namespace "downward-api-5543" to be "Succeeded or Failed"
Nov 25 07:50:10.694: INFO: Pod "downwardapi-volume-b41fd62b-7ef5-4cbd-bde7-6e1ccb08d30d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.512219ms
Nov 25 07:50:12.697: INFO: Pod "downwardapi-volume-b41fd62b-7ef5-4cbd-bde7-6e1ccb08d30d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00463234s
STEP: Saw pod success
Nov 25 07:50:12.697: INFO: Pod "downwardapi-volume-b41fd62b-7ef5-4cbd-bde7-6e1ccb08d30d" satisfied condition "Succeeded or Failed"
Nov 25 07:50:12.699: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-b41fd62b-7ef5-4cbd-bde7-6e1ccb08d30d container client-container: <nil>
STEP: delete the pod
Nov 25 07:50:12.715: INFO: Waiting for pod downwardapi-volume-b41fd62b-7ef5-4cbd-bde7-6e1ccb08d30d to disappear
Nov 25 07:50:12.716: INFO: Pod downwardapi-volume-b41fd62b-7ef5-4cbd-bde7-6e1ccb08d30d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:50:12.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5543" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":303,"completed":199,"skipped":3227,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:50:12.722: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Nov 25 07:50:12.742: INFO: created test-podtemplate-1
Nov 25 07:50:12.745: INFO: created test-podtemplate-2
Nov 25 07:50:12.747: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Nov 25 07:50:12.749: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Nov 25 07:50:12.758: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:50:12.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5115" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":303,"completed":200,"skipped":3250,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:50:12.770: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Nov 25 07:50:13.079: INFO: Pod name wrapped-volume-race-9ca481a4-93c0-488a-ae8c-1bd4a4e46ced: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9ca481a4-93c0-488a-ae8c-1bd4a4e46ced in namespace emptydir-wrapper-1069, will wait for the garbage collector to delete the pods
Nov 25 07:50:29.205: INFO: Deleting ReplicationController wrapped-volume-race-9ca481a4-93c0-488a-ae8c-1bd4a4e46ced took: 5.041128ms
Nov 25 07:50:29.605: INFO: Terminating ReplicationController wrapped-volume-race-9ca481a4-93c0-488a-ae8c-1bd4a4e46ced pods took: 400.682679ms
STEP: Creating RC which spawns configmap-volume pods
Nov 25 07:50:38.718: INFO: Pod name wrapped-volume-race-e592a10a-edf2-44cb-99d4-50b237a65a10: Found 0 pods out of 5
Nov 25 07:50:43.722: INFO: Pod name wrapped-volume-race-e592a10a-edf2-44cb-99d4-50b237a65a10: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e592a10a-edf2-44cb-99d4-50b237a65a10 in namespace emptydir-wrapper-1069, will wait for the garbage collector to delete the pods
Nov 25 07:50:53.800: INFO: Deleting ReplicationController wrapped-volume-race-e592a10a-edf2-44cb-99d4-50b237a65a10 took: 4.619165ms
Nov 25 07:50:54.201: INFO: Terminating ReplicationController wrapped-volume-race-e592a10a-edf2-44cb-99d4-50b237a65a10 pods took: 400.241523ms
STEP: Creating RC which spawns configmap-volume pods
Nov 25 07:50:58.849: INFO: Pod name wrapped-volume-race-fc5df772-31bd-4a29-86bc-c79ac61eb619: Found 0 pods out of 5
Nov 25 07:51:03.853: INFO: Pod name wrapped-volume-race-fc5df772-31bd-4a29-86bc-c79ac61eb619: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fc5df772-31bd-4a29-86bc-c79ac61eb619 in namespace emptydir-wrapper-1069, will wait for the garbage collector to delete the pods
Nov 25 07:51:13.928: INFO: Deleting ReplicationController wrapped-volume-race-fc5df772-31bd-4a29-86bc-c79ac61eb619 took: 8.723734ms
Nov 25 07:51:14.328: INFO: Terminating ReplicationController wrapped-volume-race-fc5df772-31bd-4a29-86bc-c79ac61eb619 pods took: 400.191792ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:51:21.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1069" for this suite.

• [SLOW TEST:68.281 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":303,"completed":201,"skipped":3262,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:51:21.052: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:51:37.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9689" for this suite.

• [SLOW TEST:16.084 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":303,"completed":202,"skipped":3288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:51:37.136: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3493
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3493
STEP: creating replication controller externalsvc in namespace services-3493
I1125 07:51:37.224169      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3493, replica count: 2
I1125 07:51:40.274597      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Nov 25 07:51:40.289: INFO: Creating new exec pod
Nov 25 07:51:42.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-3493 execpod4cn8p -- /bin/sh -x -c nslookup clusterip-service.services-3493.svc.cluster.local'
Nov 25 07:51:42.593: INFO: stderr: "+ nslookup clusterip-service.services-3493.svc.cluster.local\n"
Nov 25 07:51:42.593: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3493.svc.cluster.local\tcanonical name = externalsvc.services-3493.svc.cluster.local.\nName:\texternalsvc.services-3493.svc.cluster.local\nAddress: 10.101.232.144\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3493, will wait for the garbage collector to delete the pods
Nov 25 07:51:42.650: INFO: Deleting ReplicationController externalsvc took: 4.167779ms
Nov 25 07:51:43.051: INFO: Terminating ReplicationController externalsvc pods took: 400.223503ms
Nov 25 07:51:47.463: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:51:47.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3493" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:10.341 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":303,"completed":203,"skipped":3312,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:51:47.477: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 25 07:51:47.522: INFO: Number of nodes with available pods: 0
Nov 25 07:51:47.522: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:51:48.528: INFO: Number of nodes with available pods: 0
Nov 25 07:51:48.528: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:51:49.527: INFO: Number of nodes with available pods: 2
Nov 25 07:51:49.527: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov 25 07:51:49.572: INFO: Number of nodes with available pods: 1
Nov 25 07:51:49.572: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:51:50.577: INFO: Number of nodes with available pods: 1
Nov 25 07:51:50.577: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:51:51.577: INFO: Number of nodes with available pods: 1
Nov 25 07:51:51.577: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:51:52.577: INFO: Number of nodes with available pods: 1
Nov 25 07:51:52.577: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:51:53.577: INFO: Number of nodes with available pods: 1
Nov 25 07:51:53.577: INFO: Node k8sconformance is running more than one daemon pod
Nov 25 07:51:54.577: INFO: Number of nodes with available pods: 2
Nov 25 07:51:54.577: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8913, will wait for the garbage collector to delete the pods
Nov 25 07:51:54.635: INFO: Deleting DaemonSet.extensions daemon-set took: 4.535528ms
Nov 25 07:51:54.735: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.248156ms
Nov 25 07:52:08.572: INFO: Number of nodes with available pods: 0
Nov 25 07:52:08.572: INFO: Number of running nodes: 0, number of available pods: 0
Nov 25 07:52:08.574: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8913/daemonsets","resourceVersion":"17124"},"items":null}

Nov 25 07:52:08.576: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8913/pods","resourceVersion":"17124"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:52:08.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8913" for this suite.

• [SLOW TEST:21.109 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":303,"completed":204,"skipped":3332,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:52:08.586: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 25 07:52:08.638: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Nov 25 07:52:08.640: INFO: starting watch
STEP: patching
STEP: updating
Nov 25 07:52:08.646: INFO: waiting for watch events with expected annotations
Nov 25 07:52:08.646: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:52:08.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5677" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":303,"completed":205,"skipped":3343,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:52:08.680: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:52:09.171: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:52:12.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Nov 25 07:52:14.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 attach --namespace=webhook-8801 to-be-attached-pod -i -c=container1'
Nov 25 07:52:14.313: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:52:14.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8801" for this suite.
STEP: Destroying namespace "webhook-8801-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.676 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":303,"completed":206,"skipped":3380,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:52:14.356: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Nov 25 07:52:14.414: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Nov 25 07:52:14.420: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 25 07:52:14.420: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Nov 25 07:52:14.425: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 25 07:52:14.425: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Nov 25 07:52:14.433: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Nov 25 07:52:14.433: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Nov 25 07:52:21.483: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:52:21.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7497" for this suite.

• [SLOW TEST:7.140 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":303,"completed":207,"skipped":3397,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:52:21.498: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov 25 07:52:25.568: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9057 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:52:25.568: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:52:25.636: INFO: Exec stderr: ""
Nov 25 07:52:25.636: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9057 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:52:25.636: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:52:25.702: INFO: Exec stderr: ""
Nov 25 07:52:25.702: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9057 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:52:25.702: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:52:25.776: INFO: Exec stderr: ""
Nov 25 07:52:25.776: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9057 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:52:25.776: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:52:25.848: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov 25 07:52:25.848: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9057 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:52:25.848: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:52:25.921: INFO: Exec stderr: ""
Nov 25 07:52:25.921: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9057 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:52:25.921: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:52:26.000: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov 25 07:52:26.000: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9057 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:52:26.000: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:52:26.068: INFO: Exec stderr: ""
Nov 25 07:52:26.068: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9057 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:52:26.068: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:52:26.137: INFO: Exec stderr: ""
Nov 25 07:52:26.137: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9057 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:52:26.137: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:52:26.208: INFO: Exec stderr: ""
Nov 25 07:52:26.208: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9057 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 25 07:52:26.208: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
Nov 25 07:52:26.284: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:52:26.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9057" for this suite.
•{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":208,"skipped":3415,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:52:26.291: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:52:26.339: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 25 07:52:31.341: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 25 07:52:31.341: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 25 07:52:31.355: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3470 /apis/apps/v1/namespaces/deployment-3470/deployments/test-cleanup-deployment 8db2f827-e60e-4853-8985-0b82d29d72d9 17415 1 2020-11-25 07:52:31 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-11-25 07:52:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033aa4a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Nov 25 07:52:31.357: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-3470 /apis/apps/v1/namespaces/deployment-3470/replicasets/test-cleanup-deployment-5d446bdd47 7f8b901d-5626-419a-a428-f4553b074510 17417 1 2020-11-25 07:52:31 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 8db2f827-e60e-4853-8985-0b82d29d72d9 0xc0032f2077 0xc0032f2078}] []  [{kube-controller-manager Update apps/v1 2020-11-25 07:52:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8db2f827-e60e-4853-8985-0b82d29d72d9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032f2108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 25 07:52:31.357: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Nov 25 07:52:31.357: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3470 /apis/apps/v1/namespaces/deployment-3470/replicasets/test-cleanup-controller 2a9ec085-7258-4f3d-b404-d4a623b11008 17416 1 2020-11-25 07:52:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 8db2f827-e60e-4853-8985-0b82d29d72d9 0xc000a09e47 0xc000a09e48}] []  [{e2e.test Update apps/v1 2020-11-25 07:52:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-25 07:52:31 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"8db2f827-e60e-4853-8985-0b82d29d72d9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0032f2008 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 25 07:52:31.363: INFO: Pod "test-cleanup-controller-qjv8h" is available:
&Pod{ObjectMeta:{test-cleanup-controller-qjv8h test-cleanup-controller- deployment-3470 /api/v1/namespaces/deployment-3470/pods/test-cleanup-controller-qjv8h fd7fa425-bbe4-4abd-9cd0-896f476ac09a 17405 0 2020-11-25 07:52:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 2a9ec085-7258-4f3d-b404-d4a623b11008 0xc0032f26e7 0xc0032f26e8}] []  [{kube-controller-manager Update v1 2020-11-25 07:52:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a9ec085-7258-4f3d-b404-d4a623b11008\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:52:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xhbkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xhbkx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xhbkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:52:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:52:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.237,StartTime:2020-11-25 07:52:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-25 07:52:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://413d223c56a236032deccebe8d5924edbe93a42bb6a703fb29a6ecebb4d725f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.237,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:52:31.363: INFO: Pod "test-cleanup-deployment-5d446bdd47-6wrg8" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-6wrg8 test-cleanup-deployment-5d446bdd47- deployment-3470 /api/v1/namespaces/deployment-3470/pods/test-cleanup-deployment-5d446bdd47-6wrg8 342489f1-97ee-42a8-a0d2-f79c9623fd88 17421 0 2020-11-25 07:52:31 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 7f8b901d-5626-419a-a428-f4553b074510 0xc0032f2897 0xc0032f2898}] []  [{kube-controller-manager Update v1 2020-11-25 07:52:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f8b901d-5626-419a-a428-f4553b074510\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xhbkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xhbkx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xhbkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:52:31.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3470" for this suite.

• [SLOW TEST:5.088 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":303,"completed":209,"skipped":3424,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:52:31.379: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:52:36.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5152" for this suite.

• [SLOW TEST:5.505 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":303,"completed":210,"skipped":3440,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:52:36.884: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-fd2ca0d7-449e-4907-862b-6d553dc2eebd
STEP: Creating a pod to test consume configMaps
Nov 25 07:52:36.913: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf676e4f-8565-49e5-8723-fc2d361d288f" in namespace "configmap-1910" to be "Succeeded or Failed"
Nov 25 07:52:36.915: INFO: Pod "pod-configmaps-cf676e4f-8565-49e5-8723-fc2d361d288f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.858868ms
Nov 25 07:52:38.918: INFO: Pod "pod-configmaps-cf676e4f-8565-49e5-8723-fc2d361d288f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004680935s
STEP: Saw pod success
Nov 25 07:52:38.918: INFO: Pod "pod-configmaps-cf676e4f-8565-49e5-8723-fc2d361d288f" satisfied condition "Succeeded or Failed"
Nov 25 07:52:38.920: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-cf676e4f-8565-49e5-8723-fc2d361d288f container configmap-volume-test: <nil>
STEP: delete the pod
Nov 25 07:52:38.959: INFO: Waiting for pod pod-configmaps-cf676e4f-8565-49e5-8723-fc2d361d288f to disappear
Nov 25 07:52:38.961: INFO: Pod pod-configmaps-cf676e4f-8565-49e5-8723-fc2d361d288f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:52:38.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1910" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":303,"completed":211,"skipped":3440,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:52:38.966: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:52:39.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-886" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":303,"completed":212,"skipped":3451,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:52:39.036: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:52:39.060: INFO: The status of Pod test-webserver-add2db92-5ccf-4014-a996-9ea1e07d964e is Pending, waiting for it to be Running (with Ready = true)
Nov 25 07:52:41.063: INFO: The status of Pod test-webserver-add2db92-5ccf-4014-a996-9ea1e07d964e is Running (Ready = false)
Nov 25 07:52:43.063: INFO: The status of Pod test-webserver-add2db92-5ccf-4014-a996-9ea1e07d964e is Running (Ready = false)
Nov 25 07:52:45.063: INFO: The status of Pod test-webserver-add2db92-5ccf-4014-a996-9ea1e07d964e is Running (Ready = false)
Nov 25 07:52:47.063: INFO: The status of Pod test-webserver-add2db92-5ccf-4014-a996-9ea1e07d964e is Running (Ready = false)
Nov 25 07:52:49.063: INFO: The status of Pod test-webserver-add2db92-5ccf-4014-a996-9ea1e07d964e is Running (Ready = false)
Nov 25 07:52:51.063: INFO: The status of Pod test-webserver-add2db92-5ccf-4014-a996-9ea1e07d964e is Running (Ready = false)
Nov 25 07:52:53.063: INFO: The status of Pod test-webserver-add2db92-5ccf-4014-a996-9ea1e07d964e is Running (Ready = false)
Nov 25 07:52:55.063: INFO: The status of Pod test-webserver-add2db92-5ccf-4014-a996-9ea1e07d964e is Running (Ready = false)
Nov 25 07:52:57.063: INFO: The status of Pod test-webserver-add2db92-5ccf-4014-a996-9ea1e07d964e is Running (Ready = true)
Nov 25 07:52:57.065: INFO: Container started at 2020-11-25 07:52:39 +0000 UTC, pod became ready at 2020-11-25 07:52:55 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:52:57.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7764" for this suite.

• [SLOW TEST:18.034 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":303,"completed":213,"skipped":3460,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:52:57.071: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 25 07:52:57.112: INFO: Waiting up to 5m0s for pod "downward-api-9fdadab9-18f7-4069-bf82-862e1de0e7a6" in namespace "downward-api-3947" to be "Succeeded or Failed"
Nov 25 07:52:57.152: INFO: Pod "downward-api-9fdadab9-18f7-4069-bf82-862e1de0e7a6": Phase="Pending", Reason="", readiness=false. Elapsed: 39.656145ms
Nov 25 07:52:59.154: INFO: Pod "downward-api-9fdadab9-18f7-4069-bf82-862e1de0e7a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042419467s
STEP: Saw pod success
Nov 25 07:52:59.155: INFO: Pod "downward-api-9fdadab9-18f7-4069-bf82-862e1de0e7a6" satisfied condition "Succeeded or Failed"
Nov 25 07:52:59.157: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-9fdadab9-18f7-4069-bf82-862e1de0e7a6 container dapi-container: <nil>
STEP: delete the pod
Nov 25 07:52:59.171: INFO: Waiting for pod downward-api-9fdadab9-18f7-4069-bf82-862e1de0e7a6 to disappear
Nov 25 07:52:59.173: INFO: Pod downward-api-9fdadab9-18f7-4069-bf82-862e1de0e7a6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:52:59.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3947" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":303,"completed":214,"skipped":3462,"failed":0}
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:52:59.178: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 25 07:52:59.198: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:53:02.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4815" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":303,"completed":215,"skipped":3469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:53:02.591: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-9b79d241-d529-4af7-999a-88441a8f984e
STEP: Creating a pod to test consume secrets
Nov 25 07:53:02.642: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-490ebc4f-f84d-471d-b821-66f9e24f6f13" in namespace "projected-4384" to be "Succeeded or Failed"
Nov 25 07:53:02.644: INFO: Pod "pod-projected-secrets-490ebc4f-f84d-471d-b821-66f9e24f6f13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.246413ms
Nov 25 07:53:04.647: INFO: Pod "pod-projected-secrets-490ebc4f-f84d-471d-b821-66f9e24f6f13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004910991s
STEP: Saw pod success
Nov 25 07:53:04.647: INFO: Pod "pod-projected-secrets-490ebc4f-f84d-471d-b821-66f9e24f6f13" satisfied condition "Succeeded or Failed"
Nov 25 07:53:04.652: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-490ebc4f-f84d-471d-b821-66f9e24f6f13 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 25 07:53:04.663: INFO: Waiting for pod pod-projected-secrets-490ebc4f-f84d-471d-b821-66f9e24f6f13 to disappear
Nov 25 07:53:04.665: INFO: Pod pod-projected-secrets-490ebc4f-f84d-471d-b821-66f9e24f6f13 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:53:04.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4384" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":216,"skipped":3495,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:53:04.675: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 25 07:53:07.216: INFO: Successfully updated pod "labelsupdate62e7ae95-757e-4c9f-adeb-476fa6b78e08"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:53:11.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4276" for this suite.

• [SLOW TEST:6.563 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":303,"completed":217,"skipped":3496,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:53:11.238: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 25 07:53:15.300: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 25 07:53:15.306: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 25 07:53:17.306: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 25 07:53:17.309: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 25 07:53:19.306: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 25 07:53:19.309: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 25 07:53:21.306: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 25 07:53:21.308: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:53:21.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3904" for this suite.

• [SLOW TEST:10.081 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":303,"completed":218,"skipped":3507,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:53:21.319: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-49210e47-c79d-4481-ad7a-d4ee6bf26275
STEP: Creating a pod to test consume secrets
Nov 25 07:53:21.396: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eb28b782-5a53-4c9a-bfb6-27faa0d28edc" in namespace "projected-3374" to be "Succeeded or Failed"
Nov 25 07:53:21.401: INFO: Pod "pod-projected-secrets-eb28b782-5a53-4c9a-bfb6-27faa0d28edc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.083291ms
Nov 25 07:53:23.403: INFO: Pod "pod-projected-secrets-eb28b782-5a53-4c9a-bfb6-27faa0d28edc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007028522s
STEP: Saw pod success
Nov 25 07:53:23.403: INFO: Pod "pod-projected-secrets-eb28b782-5a53-4c9a-bfb6-27faa0d28edc" satisfied condition "Succeeded or Failed"
Nov 25 07:53:23.405: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-eb28b782-5a53-4c9a-bfb6-27faa0d28edc container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 25 07:53:23.419: INFO: Waiting for pod pod-projected-secrets-eb28b782-5a53-4c9a-bfb6-27faa0d28edc to disappear
Nov 25 07:53:23.421: INFO: Pod pod-projected-secrets-eb28b782-5a53-4c9a-bfb6-27faa0d28edc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:53:23.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3374" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":219,"skipped":3507,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:53:23.425: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 07:53:23.453: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df307065-a3b1-4db1-af93-fb7d388f07a5" in namespace "projected-4996" to be "Succeeded or Failed"
Nov 25 07:53:23.457: INFO: Pod "downwardapi-volume-df307065-a3b1-4db1-af93-fb7d388f07a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.610132ms
Nov 25 07:53:25.460: INFO: Pod "downwardapi-volume-df307065-a3b1-4db1-af93-fb7d388f07a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007324526s
STEP: Saw pod success
Nov 25 07:53:25.460: INFO: Pod "downwardapi-volume-df307065-a3b1-4db1-af93-fb7d388f07a5" satisfied condition "Succeeded or Failed"
Nov 25 07:53:25.462: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-df307065-a3b1-4db1-af93-fb7d388f07a5 container client-container: <nil>
STEP: delete the pod
Nov 25 07:53:25.500: INFO: Waiting for pod downwardapi-volume-df307065-a3b1-4db1-af93-fb7d388f07a5 to disappear
Nov 25 07:53:25.502: INFO: Pod downwardapi-volume-df307065-a3b1-4db1-af93-fb7d388f07a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:53:25.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4996" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":303,"completed":220,"skipped":3513,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:53:25.509: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:53:25.527: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:53:26.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9384" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":303,"completed":221,"skipped":3515,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:53:26.128: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8787.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8787.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 25 07:53:30.247: INFO: DNS probes using dns-test-7c31beac-12be-47a6-838e-9cd3c517bf2a succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8787.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8787.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 25 07:53:32.281: INFO: File wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local from pod  dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 25 07:53:32.283: INFO: File jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local from pod  dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 25 07:53:32.283: INFO: Lookups using dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 failed for: [wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local]

Nov 25 07:53:37.287: INFO: File wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local from pod  dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 25 07:53:37.290: INFO: File jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local from pod  dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 25 07:53:37.290: INFO: Lookups using dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 failed for: [wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local]

Nov 25 07:53:42.287: INFO: File wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local from pod  dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 25 07:53:42.289: INFO: File jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local from pod  dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 25 07:53:42.289: INFO: Lookups using dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 failed for: [wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local]

Nov 25 07:53:47.287: INFO: File wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local from pod  dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 25 07:53:47.289: INFO: File jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local from pod  dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 25 07:53:47.289: INFO: Lookups using dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 failed for: [wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local]

Nov 25 07:53:52.287: INFO: File wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local from pod  dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 25 07:53:52.289: INFO: File jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local from pod  dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 25 07:53:52.289: INFO: Lookups using dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 failed for: [wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local]

Nov 25 07:53:57.287: INFO: File wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local from pod  dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 25 07:53:57.289: INFO: File jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local from pod  dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 25 07:53:57.289: INFO: Lookups using dns-8787/dns-test-41eb1e41-31a7-479b-8368-684782785a28 failed for: [wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local]

Nov 25 07:54:02.289: INFO: DNS probes using dns-test-41eb1e41-31a7-479b-8368-684782785a28 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8787.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8787.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8787.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8787.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 25 07:54:06.363: INFO: DNS probes using dns-test-f749b67c-a2f0-4d2f-9439-31ef96dcf342 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:54:06.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8787" for this suite.

• [SLOW TEST:40.274 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":303,"completed":222,"skipped":3544,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:54:06.402: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-7ad9d553-07b1-42e3-ab84-0a6fbbf7ca98
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-7ad9d553-07b1-42e3-ab84-0a6fbbf7ca98
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:54:10.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2249" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":223,"skipped":3550,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:54:10.552: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 25 07:54:10.914: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov 25 07:54:12.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741887650, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741887650, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741887650, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741887650, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 07:54:15.933: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:54:15.936: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:54:17.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5041" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.568 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":303,"completed":224,"skipped":3568,"failed":0}
SS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:54:17.120: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 25 07:54:17.209: INFO: starting watch
STEP: patching
STEP: updating
Nov 25 07:54:17.216: INFO: waiting for watch events with expected annotations
Nov 25 07:54:17.216: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:54:17.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2574" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":303,"completed":225,"skipped":3570,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:54:17.244: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:54:42.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6724" for this suite.

• [SLOW TEST:25.212 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":303,"completed":226,"skipped":3582,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:54:42.456: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-ba8db972-cc35-4c6b-9c89-de91efc2da7c in namespace container-probe-7246
Nov 25 07:54:44.514: INFO: Started pod liveness-ba8db972-cc35-4c6b-9c89-de91efc2da7c in namespace container-probe-7246
STEP: checking the pod's current state and verifying that restartCount is present
Nov 25 07:54:44.516: INFO: Initial restart count of pod liveness-ba8db972-cc35-4c6b-9c89-de91efc2da7c is 0
Nov 25 07:54:58.538: INFO: Restart count of pod container-probe-7246/liveness-ba8db972-cc35-4c6b-9c89-de91efc2da7c is now 1 (14.022433624s elapsed)
Nov 25 07:55:18.574: INFO: Restart count of pod container-probe-7246/liveness-ba8db972-cc35-4c6b-9c89-de91efc2da7c is now 2 (34.057834382s elapsed)
Nov 25 07:55:38.604: INFO: Restart count of pod container-probe-7246/liveness-ba8db972-cc35-4c6b-9c89-de91efc2da7c is now 3 (54.08781993s elapsed)
Nov 25 07:55:58.633: INFO: Restart count of pod container-probe-7246/liveness-ba8db972-cc35-4c6b-9c89-de91efc2da7c is now 4 (1m14.116909614s elapsed)
Nov 25 07:57:10.795: INFO: Restart count of pod container-probe-7246/liveness-ba8db972-cc35-4c6b-9c89-de91efc2da7c is now 5 (2m26.278853044s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:57:10.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7246" for this suite.

• [SLOW TEST:148.355 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":303,"completed":227,"skipped":3584,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:57:10.812: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-17082bcb-8b50-49ef-a141-071543e9e9cf
STEP: Creating a pod to test consume configMaps
Nov 25 07:57:10.839: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8597d51c-a478-49bb-b09d-2152bb769eca" in namespace "projected-477" to be "Succeeded or Failed"
Nov 25 07:57:10.843: INFO: Pod "pod-projected-configmaps-8597d51c-a478-49bb-b09d-2152bb769eca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.138798ms
Nov 25 07:57:12.846: INFO: Pod "pod-projected-configmaps-8597d51c-a478-49bb-b09d-2152bb769eca": Phase="Running", Reason="", readiness=true. Elapsed: 2.00620635s
Nov 25 07:57:14.849: INFO: Pod "pod-projected-configmaps-8597d51c-a478-49bb-b09d-2152bb769eca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009392316s
STEP: Saw pod success
Nov 25 07:57:14.849: INFO: Pod "pod-projected-configmaps-8597d51c-a478-49bb-b09d-2152bb769eca" satisfied condition "Succeeded or Failed"
Nov 25 07:57:14.851: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-8597d51c-a478-49bb-b09d-2152bb769eca container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 25 07:57:14.870: INFO: Waiting for pod pod-projected-configmaps-8597d51c-a478-49bb-b09d-2152bb769eca to disappear
Nov 25 07:57:14.871: INFO: Pod pod-projected-configmaps-8597d51c-a478-49bb-b09d-2152bb769eca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:57:14.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-477" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":228,"skipped":3602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:57:14.876: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6797
Nov 25 07:57:16.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6797 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 25 07:57:17.117: INFO: rc: 7
Nov 25 07:57:17.120: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 25 07:57:17.125: INFO: Pod kube-proxy-mode-detector still exists
Nov 25 07:57:19.125: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 25 07:57:19.127: INFO: Pod kube-proxy-mode-detector still exists
Nov 25 07:57:21.125: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 25 07:57:21.128: INFO: Pod kube-proxy-mode-detector still exists
Nov 25 07:57:23.125: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 25 07:57:23.129: INFO: Pod kube-proxy-mode-detector still exists
Nov 25 07:57:25.125: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 25 07:57:25.127: INFO: Pod kube-proxy-mode-detector still exists
Nov 25 07:57:27.125: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 25 07:57:27.127: INFO: Pod kube-proxy-mode-detector still exists
Nov 25 07:57:29.125: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 25 07:57:29.127: INFO: Pod kube-proxy-mode-detector still exists
Nov 25 07:57:31.125: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 25 07:57:31.127: INFO: Pod kube-proxy-mode-detector no longer exists
Nov 25 07:57:31.127: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6797 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-6797
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6797
I1125 07:57:31.147108      23 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6797, replica count: 3
I1125 07:57:34.198190      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 25 07:57:34.204: INFO: Creating new exec pod
Nov 25 07:57:37.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6797 execpod-affinitygczgr -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Nov 25 07:57:37.357: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Nov 25 07:57:37.357: INFO: stdout: ""
Nov 25 07:57:37.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6797 execpod-affinitygczgr -- /bin/sh -x -c nc -zv -t -w 2 10.103.97.139 80'
Nov 25 07:57:37.502: INFO: stderr: "+ nc -zv -t -w 2 10.103.97.139 80\nConnection to 10.103.97.139 80 port [tcp/http] succeeded!\n"
Nov 25 07:57:37.502: INFO: stdout: ""
Nov 25 07:57:37.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6797 execpod-affinitygczgr -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.2 31405'
Nov 25 07:57:37.638: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.2 31405\nConnection to 192.168.49.2 31405 port [tcp/31405] succeeded!\n"
Nov 25 07:57:37.638: INFO: stdout: ""
Nov 25 07:57:37.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6797 execpod-affinitygczgr -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.3 31405'
Nov 25 07:57:37.777: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.3 31405\nConnection to 192.168.49.3 31405 port [tcp/31405] succeeded!\n"
Nov 25 07:57:37.777: INFO: stdout: ""
Nov 25 07:57:37.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6797 execpod-affinitygczgr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:31405/ ; done'
Nov 25 07:57:37.988: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n"
Nov 25 07:57:37.988: INFO: stdout: "\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv\naffinity-nodeport-timeout-6j8gv"
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Received response from host: affinity-nodeport-timeout-6j8gv
Nov 25 07:57:37.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6797 execpod-affinitygczgr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.49.2:31405/'
Nov 25 07:57:38.136: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n"
Nov 25 07:57:38.136: INFO: stdout: "affinity-nodeport-timeout-6j8gv"
Nov 25 07:57:53.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6797 execpod-affinitygczgr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.49.2:31405/'
Nov 25 07:57:53.280: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n"
Nov 25 07:57:53.280: INFO: stdout: "affinity-nodeport-timeout-6j8gv"
Nov 25 07:58:08.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-6797 execpod-affinitygczgr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.49.2:31405/'
Nov 25 07:58:08.433: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.49.2:31405/\n"
Nov 25 07:58:08.433: INFO: stdout: "affinity-nodeport-timeout-2hwqs"
Nov 25 07:58:08.433: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6797, will wait for the garbage collector to delete the pods
Nov 25 07:58:08.500: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 4.293062ms
Nov 25 07:58:08.900: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 400.226363ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:58:18.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6797" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:63.749 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":229,"skipped":3633,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:58:18.626: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1125 07:58:19.756637      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 25 07:59:21.769: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:59:21.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8878" for this suite.

• [SLOW TEST:63.149 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":303,"completed":230,"skipped":3669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:59:21.776: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8075
STEP: creating service affinity-clusterip in namespace services-8075
STEP: creating replication controller affinity-clusterip in namespace services-8075
I1125 07:59:21.846096      23 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-8075, replica count: 3
I1125 07:59:24.909567      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 25 07:59:24.914: INFO: Creating new exec pod
Nov 25 07:59:27.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-8075 execpod-affinitynndqd -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Nov 25 07:59:28.182: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Nov 25 07:59:28.182: INFO: stdout: ""
Nov 25 07:59:28.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-8075 execpod-affinitynndqd -- /bin/sh -x -c nc -zv -t -w 2 10.99.44.218 80'
Nov 25 07:59:29.028: INFO: stderr: "+ nc -zv -t -w 2 10.99.44.218 80\nConnection to 10.99.44.218 80 port [tcp/http] succeeded!\n"
Nov 25 07:59:29.028: INFO: stdout: ""
Nov 25 07:59:29.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-8075 execpod-affinitynndqd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.44.218:80/ ; done'
Nov 25 07:59:29.275: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.44.218:80/\n"
Nov 25 07:59:29.275: INFO: stdout: "\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh\naffinity-clusterip-cqklh"
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.275: INFO: Received response from host: affinity-clusterip-cqklh
Nov 25 07:59:29.276: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-8075, will wait for the garbage collector to delete the pods
Nov 25 07:59:29.351: INFO: Deleting ReplicationController affinity-clusterip took: 5.233504ms
Nov 25 07:59:29.752: INFO: Terminating ReplicationController affinity-clusterip pods took: 400.252242ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:59:40.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8075" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:19.000 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":231,"skipped":3710,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:59:40.776: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 25 07:59:40.801: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 25 07:59:40.806: INFO: Waiting for terminating namespaces to be deleted...
Nov 25 07:59:40.808: INFO: 
Logging pods the apiserver thinks is on node k8sconformance before test
Nov 25 07:59:40.811: INFO: coredns-f9fd979d6-z4vhz from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:59:40.811: INFO: 	Container coredns ready: true, restart count 0
Nov 25 07:59:40.811: INFO: etcd-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:59:40.811: INFO: 	Container etcd ready: true, restart count 0
Nov 25 07:59:40.811: INFO: kindnet-n566x from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:59:40.811: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 25 07:59:40.811: INFO: kube-apiserver-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:59:40.811: INFO: 	Container kube-apiserver ready: true, restart count 0
Nov 25 07:59:40.811: INFO: kube-controller-manager-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:59:40.811: INFO: 	Container kube-controller-manager ready: true, restart count 0
Nov 25 07:59:40.811: INFO: kube-proxy-58hhs from kube-system started at 2020-11-25 06:43:58 +0000 UTC (1 container statuses recorded)
Nov 25 07:59:40.811: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 25 07:59:40.811: INFO: kube-scheduler-k8sconformance from kube-system started at 2020-11-25 06:43:48 +0000 UTC (1 container statuses recorded)
Nov 25 07:59:40.811: INFO: 	Container kube-scheduler ready: true, restart count 0
Nov 25 07:59:40.811: INFO: storage-provisioner from kube-system started at 2020-11-25 06:44:00 +0000 UTC (1 container statuses recorded)
Nov 25 07:59:40.811: INFO: 	Container storage-provisioner ready: true, restart count 0
Nov 25 07:59:40.811: INFO: sonobuoy from sonobuoy started at 2020-11-25 06:44:55 +0000 UTC (1 container statuses recorded)
Nov 25 07:59:40.812: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 25 07:59:40.812: INFO: sonobuoy-e2e-job-7010f57f836a484c from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:59:40.812: INFO: 	Container e2e ready: true, restart count 0
Nov 25 07:59:40.812: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 25 07:59:40.812: INFO: sonobuoy-systemd-logs-daemon-set-25ff39ce45304256-mnllj from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:59:40.812: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 25 07:59:40.812: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 25 07:59:40.812: INFO: 
Logging pods the apiserver thinks is on node k8sconformance-m02 before test
Nov 25 07:59:40.815: INFO: kindnet-6lgph from kube-system started at 2020-11-25 06:44:50 +0000 UTC (1 container statuses recorded)
Nov 25 07:59:40.815: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 25 07:59:40.815: INFO: kube-proxy-8kw9q from kube-system started at 2020-11-25 06:44:50 +0000 UTC (1 container statuses recorded)
Nov 25 07:59:40.815: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 25 07:59:40.815: INFO: sonobuoy-systemd-logs-daemon-set-25ff39ce45304256-nnr45 from sonobuoy started at 2020-11-25 06:45:00 +0000 UTC (2 container statuses recorded)
Nov 25 07:59:40.815: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 25 07:59:40.815: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node k8sconformance
STEP: verifying the node has the label node k8sconformance-m02
Nov 25 07:59:40.856: INFO: Pod coredns-f9fd979d6-z4vhz requesting resource cpu=100m on Node k8sconformance
Nov 25 07:59:40.856: INFO: Pod etcd-k8sconformance requesting resource cpu=0m on Node k8sconformance
Nov 25 07:59:40.856: INFO: Pod kindnet-6lgph requesting resource cpu=100m on Node k8sconformance-m02
Nov 25 07:59:40.856: INFO: Pod kindnet-n566x requesting resource cpu=100m on Node k8sconformance
Nov 25 07:59:40.856: INFO: Pod kube-apiserver-k8sconformance requesting resource cpu=250m on Node k8sconformance
Nov 25 07:59:40.856: INFO: Pod kube-controller-manager-k8sconformance requesting resource cpu=200m on Node k8sconformance
Nov 25 07:59:40.856: INFO: Pod kube-proxy-58hhs requesting resource cpu=0m on Node k8sconformance
Nov 25 07:59:40.856: INFO: Pod kube-proxy-8kw9q requesting resource cpu=0m on Node k8sconformance-m02
Nov 25 07:59:40.856: INFO: Pod kube-scheduler-k8sconformance requesting resource cpu=100m on Node k8sconformance
Nov 25 07:59:40.856: INFO: Pod storage-provisioner requesting resource cpu=0m on Node k8sconformance
Nov 25 07:59:40.856: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8sconformance
Nov 25 07:59:40.856: INFO: Pod sonobuoy-e2e-job-7010f57f836a484c requesting resource cpu=0m on Node k8sconformance
Nov 25 07:59:40.856: INFO: Pod sonobuoy-systemd-logs-daemon-set-25ff39ce45304256-mnllj requesting resource cpu=0m on Node k8sconformance
Nov 25 07:59:40.856: INFO: Pod sonobuoy-systemd-logs-daemon-set-25ff39ce45304256-nnr45 requesting resource cpu=0m on Node k8sconformance-m02
STEP: Starting Pods to consume most of the cluster CPU.
Nov 25 07:59:40.856: INFO: Creating a pod which consumes cpu=5075m on Node k8sconformance
Nov 25 07:59:40.861: INFO: Creating a pod which consumes cpu=5530m on Node k8sconformance-m02
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7be0a888-34fa-40fb-8cc3-7a7e6b94863a.164ab15189abe1f9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2409/filler-pod-7be0a888-34fa-40fb-8cc3-7a7e6b94863a to k8sconformance-m02]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-7be0a888-34fa-40fb-8cc3-7a7e6b94863a.164ab1519ed48b77], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sched-pred-2409.svc.cluster.local svc.cluster.local cluster.local corp.google.com prod.google.com prodz.google.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7be0a888-34fa-40fb-8cc3-7a7e6b94863a.164ab151b0e7aedd], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7be0a888-34fa-40fb-8cc3-7a7e6b94863a.164ab151c4e096cd], Reason = [Created], Message = [Created container filler-pod-7be0a888-34fa-40fb-8cc3-7a7e6b94863a]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-7be0a888-34fa-40fb-8cc3-7a7e6b94863a.164ab151cc82af04], Reason = [Failed], Message = [Error: failed to create containerd task: OCI runtime create failed: container_linux.go:349: starting container process caused "process_linux.go:449: container init caused \"process_linux.go:415: setting cgroup config for procHooks process caused \\\"failed to write \\\\\\\"553000\\\\\\\" to \\\\\\\"/sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod53537d1a-9dfe-4e6f-9ace-8e0cfc209001/filler-pod-7be0a888-34fa-40fb-8cc3-7a7e6b94863a/cpu.cfs_quota_us\\\\\\\": write /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod53537d1a-9dfe-4e6f-9ace-8e0cfc209001/filler-pod-7be0a888-34fa-40fb-8cc3-7a7e6b94863a/cpu.cfs_quota_us: invalid argument\\\"\"": unknown]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae50625e-6971-4c6f-9a82-30ff64ec8774.164ab15189a7045c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2409/filler-pod-ae50625e-6971-4c6f-9a82-30ff64ec8774 to k8sconformance]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-ae50625e-6971-4c6f-9a82-30ff64ec8774.164ab1519e338322], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sched-pred-2409.svc.cluster.local svc.cluster.local cluster.local corp.google.com prod.google.com prodz.google.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae50625e-6971-4c6f-9a82-30ff64ec8774.164ab151b0f32a2e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae50625e-6971-4c6f-9a82-30ff64ec8774.164ab151c39ec0f8], Reason = [Created], Message = [Created container filler-pod-ae50625e-6971-4c6f-9a82-30ff64ec8774]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-ae50625e-6971-4c6f-9a82-30ff64ec8774.164ab151cca8bb47], Reason = [Failed], Message = [Error: failed to create containerd task: OCI runtime create failed: container_linux.go:349: starting container process caused "process_linux.go:449: container init caused \"process_linux.go:415: setting cgroup config for procHooks process caused \\\"failed to write \\\\\\\"507500\\\\\\\" to \\\\\\\"/sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod5d58adf0-a8d0-4750-82c2-b9ceb5d0310c/filler-pod-ae50625e-6971-4c6f-9a82-30ff64ec8774/cpu.cfs_quota_us\\\\\\\": write /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod5d58adf0-a8d0-4750-82c2-b9ceb5d0310c/filler-pod-ae50625e-6971-4c6f-9a82-30ff64ec8774/cpu.cfs_quota_us: invalid argument\\\"\"": unknown]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.164ab1520202d4b0], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-7be0a888-34fa-40fb-8cc3-7a7e6b94863a.164ab15217819b36], Reason = [BackOff], Message = [Back-off restarting failed container]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-ae50625e-6971-4c6f-9a82-30ff64ec8774.164ab1521e55f728], Reason = [BackOff], Message = [Back-off restarting failed container]
STEP: removing the label node off the node k8sconformance
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8sconformance-m02
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:59:43.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2409" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":303,"completed":232,"skipped":3746,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:59:43.914: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-f3723237-eba8-41fa-b68b-50ce92e762d4
STEP: Creating a pod to test consume secrets
Nov 25 07:59:43.954: INFO: Waiting up to 5m0s for pod "pod-secrets-2a38fc25-63aa-452a-8c8e-c95b51d34921" in namespace "secrets-3228" to be "Succeeded or Failed"
Nov 25 07:59:43.956: INFO: Pod "pod-secrets-2a38fc25-63aa-452a-8c8e-c95b51d34921": Phase="Pending", Reason="", readiness=false. Elapsed: 1.654333ms
Nov 25 07:59:45.958: INFO: Pod "pod-secrets-2a38fc25-63aa-452a-8c8e-c95b51d34921": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003843016s
STEP: Saw pod success
Nov 25 07:59:45.958: INFO: Pod "pod-secrets-2a38fc25-63aa-452a-8c8e-c95b51d34921" satisfied condition "Succeeded or Failed"
Nov 25 07:59:45.960: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-2a38fc25-63aa-452a-8c8e-c95b51d34921 container secret-env-test: <nil>
STEP: delete the pod
Nov 25 07:59:46.018: INFO: Waiting for pod pod-secrets-2a38fc25-63aa-452a-8c8e-c95b51d34921 to disappear
Nov 25 07:59:46.020: INFO: Pod pod-secrets-2a38fc25-63aa-452a-8c8e-c95b51d34921 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:59:46.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3228" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":303,"completed":233,"skipped":3758,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:59:46.025: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Nov 25 07:59:46.060: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 25 07:59:51.064: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:59:52.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-618" for this suite.

• [SLOW TEST:6.062 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":303,"completed":234,"skipped":3764,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:59:52.087: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 07:59:52.112: INFO: Creating deployment "webserver-deployment"
Nov 25 07:59:52.115: INFO: Waiting for observed generation 1
Nov 25 07:59:54.123: INFO: Waiting for all required pods to come up
Nov 25 07:59:54.131: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Nov 25 07:59:56.140: INFO: Waiting for deployment "webserver-deployment" to complete
Nov 25 07:59:56.144: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov 25 07:59:56.150: INFO: Updating deployment webserver-deployment
Nov 25 07:59:56.150: INFO: Waiting for observed generation 2
Nov 25 07:59:58.170: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 25 07:59:58.172: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 25 07:59:58.173: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 25 07:59:58.179: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 25 07:59:58.179: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 25 07:59:58.180: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 25 07:59:58.184: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov 25 07:59:58.184: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov 25 07:59:58.188: INFO: Updating deployment webserver-deployment
Nov 25 07:59:58.188: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov 25 07:59:58.197: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 25 07:59:58.264: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 25 07:59:58.281: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2730 /apis/apps/v1/namespaces/deployment-2730/deployments/webserver-deployment 976754aa-2d8e-4b93-9954-8e06fe8586b8 19491 3 2020-11-25 07:59:52 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bb6438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2020-11-25 07:59:56 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-11-25 07:59:58 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Nov 25 07:59:58.317: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-2730 /apis/apps/v1/namespaces/deployment-2730/replicasets/webserver-deployment-795d758f88 df63a5cd-7b3b-477c-b8ec-d2166d32ad36 19477 3 2020-11-25 07:59:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 976754aa-2d8e-4b93-9954-8e06fe8586b8 0xc004bb6957 0xc004bb6958}] []  [{kube-controller-manager Update apps/v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"976754aa-2d8e-4b93-9954-8e06fe8586b8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bb6b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 25 07:59:58.317: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov 25 07:59:58.317: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-2730 /apis/apps/v1/namespaces/deployment-2730/replicasets/webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 19474 3 2020-11-25 07:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 976754aa-2d8e-4b93-9954-8e06fe8586b8 0xc004bb6c57 0xc004bb6c58}] []  [{kube-controller-manager Update apps/v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"976754aa-2d8e-4b93-9954-8e06fe8586b8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bb6cc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Nov 25 07:59:58.340: INFO: Pod "webserver-deployment-795d758f88-44gj8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-44gj8 webserver-deployment-795d758f88- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-795d758f88-44gj8 5037c69b-fc53-4bc2-b652-bf76b0b39c8c 19427 0 2020-11-25 07:59:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 df63a5cd-7b3b-477c-b8ec-d2166d32ad36 0xc004bb7807 0xc004bb7808}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df63a5cd-7b3b-477c-b8ec-d2166d32ad36\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2020-11-25 07:59:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.341: INFO: Pod "webserver-deployment-795d758f88-65bnf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-65bnf webserver-deployment-795d758f88- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-795d758f88-65bnf 2618e178-e821-42f3-9dc4-881bf72f2fd1 19521 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 df63a5cd-7b3b-477c-b8ec-d2166d32ad36 0xc004bb79b7 0xc004bb79b8}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df63a5cd-7b3b-477c-b8ec-d2166d32ad36\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.341: INFO: Pod "webserver-deployment-795d758f88-6cxss" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6cxss webserver-deployment-795d758f88- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-795d758f88-6cxss c7576f33-362e-4ab2-b575-847d08c422a5 19411 0 2020-11-25 07:59:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 df63a5cd-7b3b-477c-b8ec-d2166d32ad36 0xc004bb7ae0 0xc004bb7ae1}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df63a5cd-7b3b-477c-b8ec-d2166d32ad36\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2020-11-25 07:59:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.341: INFO: Pod "webserver-deployment-795d758f88-6tplp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6tplp webserver-deployment-795d758f88- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-795d758f88-6tplp 78aae6fa-b3ef-4d6e-8edb-00e5ffd43b52 19519 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 df63a5cd-7b3b-477c-b8ec-d2166d32ad36 0xc004bb7c97 0xc004bb7c98}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df63a5cd-7b3b-477c-b8ec-d2166d32ad36\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.341: INFO: Pod "webserver-deployment-795d758f88-78598" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-78598 webserver-deployment-795d758f88- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-795d758f88-78598 d73f3910-4f00-4b1d-8257-0146ac610c7e 19505 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 df63a5cd-7b3b-477c-b8ec-d2166d32ad36 0xc004bb7dc0 0xc004bb7dc1}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df63a5cd-7b3b-477c-b8ec-d2166d32ad36\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.341: INFO: Pod "webserver-deployment-795d758f88-9c7xr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9c7xr webserver-deployment-795d758f88- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-795d758f88-9c7xr 50f26d6b-f3a3-4148-909f-801750cb1f8c 19522 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 df63a5cd-7b3b-477c-b8ec-d2166d32ad36 0xc004bb7ef0 0xc004bb7ef1}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df63a5cd-7b3b-477c-b8ec-d2166d32ad36\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.341: INFO: Pod "webserver-deployment-795d758f88-b7ljc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-b7ljc webserver-deployment-795d758f88- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-795d758f88-b7ljc b548c177-76a2-49f2-9f87-71586328a24d 19532 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 df63a5cd-7b3b-477c-b8ec-d2166d32ad36 0xc003fda070 0xc003fda071}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df63a5cd-7b3b-477c-b8ec-d2166d32ad36\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.342: INFO: Pod "webserver-deployment-795d758f88-kclc8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kclc8 webserver-deployment-795d758f88- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-795d758f88-kclc8 cd71ead1-376b-4fd2-a301-3d5aa10cd33c 19429 0 2020-11-25 07:59:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 df63a5cd-7b3b-477c-b8ec-d2166d32ad36 0xc003fda270 0xc003fda271}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df63a5cd-7b3b-477c-b8ec-d2166d32ad36\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2020-11-25 07:59:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.342: INFO: Pod "webserver-deployment-795d758f88-n2bj4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-n2bj4 webserver-deployment-795d758f88- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-795d758f88-n2bj4 022c94fd-e54f-484d-b20b-6dd8dc0fe201 19401 0 2020-11-25 07:59:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 df63a5cd-7b3b-477c-b8ec-d2166d32ad36 0xc003fda5a7 0xc003fda5a8}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df63a5cd-7b3b-477c-b8ec-d2166d32ad36\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2020-11-25 07:59:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.342: INFO: Pod "webserver-deployment-795d758f88-pxfrq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pxfrq webserver-deployment-795d758f88- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-795d758f88-pxfrq ed708ab1-7579-411c-942f-0ea9b98497bc 19508 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 df63a5cd-7b3b-477c-b8ec-d2166d32ad36 0xc003fda867 0xc003fda868}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df63a5cd-7b3b-477c-b8ec-d2166d32ad36\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.343: INFO: Pod "webserver-deployment-795d758f88-rlgpc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rlgpc webserver-deployment-795d758f88- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-795d758f88-rlgpc 25a1ff65-b8b7-47cd-837a-f57f3cf3f2f7 19523 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 df63a5cd-7b3b-477c-b8ec-d2166d32ad36 0xc003fdaa80 0xc003fdaa81}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df63a5cd-7b3b-477c-b8ec-d2166d32ad36\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.343: INFO: Pod "webserver-deployment-795d758f88-vq8cz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vq8cz webserver-deployment-795d758f88- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-795d758f88-vq8cz a55e1405-1402-4498-8cd6-31d1d42733cd 19497 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 df63a5cd-7b3b-477c-b8ec-d2166d32ad36 0xc003fdace0 0xc003fdace1}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df63a5cd-7b3b-477c-b8ec-d2166d32ad36\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.346: INFO: Pod "webserver-deployment-795d758f88-vrqvw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vrqvw webserver-deployment-795d758f88- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-795d758f88-vrqvw 69922644-a6b3-4fdb-903a-4779527c3e34 19413 0 2020-11-25 07:59:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 df63a5cd-7b3b-477c-b8ec-d2166d32ad36 0xc003fdaf00 0xc003fdaf01}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df63a5cd-7b3b-477c-b8ec-d2166d32ad36\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2020-11-25 07:59:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.346: INFO: Pod "webserver-deployment-dd94f59b7-6mzc9" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6mzc9 webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-6mzc9 e91fa91c-d53c-42a5-8d75-bdeea4320cc2 19331 0 2020-11-25 07:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc003fdb157 0xc003fdb158}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.21,StartTime:2020-11-25 07:59:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-25 07:59:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://808408d15b92a592e038a46147ed51b7aa6ef5af62f3a42373124d3e6a58314a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.346: INFO: Pod "webserver-deployment-dd94f59b7-8spl2" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8spl2 webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-8spl2 189c366c-da17-4d6f-b5a8-3a8a2a64d4ba 19506 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc003fdb4b0 0xc003fdb4b1}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.346: INFO: Pod "webserver-deployment-dd94f59b7-9w8cc" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9w8cc webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-9w8cc a419a336-f114-4828-83dc-3ebcf291de18 19524 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc003fdb690 0xc003fdb691}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.346: INFO: Pod "webserver-deployment-dd94f59b7-9zlfr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9zlfr webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-9zlfr 3a8abb8c-da80-4e3d-984f-02aa09825c74 19528 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc003fdb7b0 0xc003fdb7b1}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2020-11-25 07:59:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.347: INFO: Pod "webserver-deployment-dd94f59b7-cqpvv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cqpvv webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-cqpvv 6eac8693-3a3f-4c53-9200-18de141320b9 19518 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc003fdb937 0xc003fdb938}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.347: INFO: Pod "webserver-deployment-dd94f59b7-dqxbr" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dqxbr webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-dqxbr e762052f-57da-407d-bd4f-de3f98aec12e 19342 0 2020-11-25 07:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc003fdba50 0xc003fdba51}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.18,StartTime:2020-11-25 07:59:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-25 07:59:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://2c6cf39754bd9010be718aa7cba66106edb9544b46b07f79b938c45b1b2bcb6b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.347: INFO: Pod "webserver-deployment-dd94f59b7-flfv2" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-flfv2 webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-flfv2 96a1601e-eb5a-47e4-8e33-20a06b2b9f2a 19354 0 2020-11-25 07:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc003fdbbe0 0xc003fdbbe1}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.66\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.66,StartTime:2020-11-25 07:59:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-25 07:59:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://f2cb84da484c75edb39d9f5e9649ad42bf08f5823894b9dbdc5b49582bbba925,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.347: INFO: Pod "webserver-deployment-dd94f59b7-fnghg" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fnghg webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-fnghg e9b5b83e-af91-46f6-8037-8b1ff77b0141 19357 0 2020-11-25 07:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc003fdbd90 0xc003fdbd91}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.69,StartTime:2020-11-25 07:59:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-25 07:59:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://6c9ad8390abc5a466f2dd3855e3729e24a5bda1681213702615cfecdc1638b60,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.347: INFO: Pod "webserver-deployment-dd94f59b7-jc6zg" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jc6zg webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-jc6zg a00f7f49-8b6c-4469-a950-7c9fd0ab3f20 19515 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc004222040 0xc004222041}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2020-11-25 07:59:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.347: INFO: Pod "webserver-deployment-dd94f59b7-lqq7z" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lqq7z webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-lqq7z 56665b9e-9886-4590-8859-d54e13bb12dd 19526 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc0042221d7 0xc0042221d8}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.348: INFO: Pod "webserver-deployment-dd94f59b7-lsnjp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lsnjp webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-lsnjp a31dafc6-788c-433b-a104-7cf913b43fbd 19498 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc0042222f0 0xc0042222f1}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.348: INFO: Pod "webserver-deployment-dd94f59b7-m4cmv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-m4cmv webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-m4cmv c8a83909-e845-4493-94f5-e28e47a032f5 19495 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc004222410 0xc004222411}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.349: INFO: Pod "webserver-deployment-dd94f59b7-mhblv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mhblv webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-mhblv 5329dbcc-f96b-4593-8a71-9bc7cc58ae2b 19336 0 2020-11-25 07:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc004222550 0xc004222551}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.20,StartTime:2020-11-25 07:59:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-25 07:59:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://13f5d501feb6bc4653ebabf3df0098f948ee82cc3110531ec66c7036ec8023f5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.349: INFO: Pod "webserver-deployment-dd94f59b7-mkbj6" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mkbj6 webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-mkbj6 118734d4-7cec-4db4-882c-71bb8483431c 19525 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc0042226f0 0xc0042226f1}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.350: INFO: Pod "webserver-deployment-dd94f59b7-ms897" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ms897 webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-ms897 3ddae7e8-c661-4625-9c52-c0c2e1be2420 19350 0 2020-11-25 07:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc004222820 0xc004222821}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.68,StartTime:2020-11-25 07:59:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-25 07:59:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://34f7801bc428302850c5f39c0a6e46b8c3fa299e30e370e579a9768e2dc19230,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.350: INFO: Pod "webserver-deployment-dd94f59b7-n79c8" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-n79c8 webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-n79c8 e75b4bc0-8ee1-45a5-97be-69f3037ec26e 19334 0 2020-11-25 07:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc0042229c0 0xc0042229c1}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.17,StartTime:2020-11-25 07:59:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-25 07:59:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://ddf558ca8f6fc50b894928138491192424f23d5dce16e9656ab4fd6703b43323,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.350: INFO: Pod "webserver-deployment-dd94f59b7-nfdd8" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nfdd8 webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-nfdd8 98e70503-78be-4c1c-8f84-3426511923d9 19339 0 2020-11-25 07:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc004222b50 0xc004222b51}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 07:59:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.67,StartTime:2020-11-25 07:59:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-25 07:59:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://7a1a06e1e954e4ba8567071ee0f62f0aeff0ecb8e974ae6aa6892d6699560794,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.351: INFO: Pod "webserver-deployment-dd94f59b7-twl4z" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-twl4z webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-twl4z 35e288be-7fa7-4e85-844d-00a163d93cc2 19520 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc004222d00 0xc004222d01}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.351: INFO: Pod "webserver-deployment-dd94f59b7-v5z6s" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-v5z6s webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-v5z6s 27aa0e37-51f4-4fc4-9087-d47f74f9a8cf 19489 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc004222e30 0xc004222e31}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 25 07:59:58.351: INFO: Pod "webserver-deployment-dd94f59b7-v8xkr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-v8xkr webserver-deployment-dd94f59b7- deployment-2730 /api/v1/namespaces/deployment-2730/pods/webserver-deployment-dd94f59b7-v8xkr 4c44927d-b268-4021-976a-507f7bd1712b 19496 0 2020-11-25 07:59:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 69d3338f-f14e-4373-b938-36d5f26e63c0 0xc004222f50 0xc004222f51}] []  [{kube-controller-manager Update v1 2020-11-25 07:59:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69d3338f-f14e-4373-b938-36d5f26e63c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7fd8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7fd8x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7fd8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 07:59:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 07:59:58.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2730" for this suite.

• [SLOW TEST:6.377 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":303,"completed":235,"skipped":3795,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 07:59:58.464: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 25 07:59:58.615: INFO: PodSpec: initContainers in spec.initContainers
Nov 25 08:00:42.647: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c4a60222-2487-465a-beca-eef931315b97", GenerateName:"", Namespace:"init-container-4135", SelfLink:"/api/v1/namespaces/init-container-4135/pods/pod-init-c4a60222-2487-465a-beca-eef931315b97", UID:"6c603f8d-ff50-4db6-917d-81137773deea", ResourceVersion:"19849", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63741887998, loc:(*time.Location)(0x77108c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"615035637"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004848640), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004848660)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004848680), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0048486a0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-wdldk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003672040), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wdldk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wdldk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wdldk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004d8e0a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8sconformance-m02", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0033e4000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004d8e120)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004d8e140)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004d8e148), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004d8e14c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0035e2020), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741887998, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741887998, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741887998, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741887998, loc:(*time.Location)(0x77108c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.49.3", PodIP:"10.244.1.32", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.32"}}, StartTime:(*v1.Time)(0xc0048486c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0033e40e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0033e4150)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://80901231c7ca5ba7a6faa83c5eb74f96743ca4e062500480f4ab9ad46ebd61a5", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004848700), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0048486e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc004d8e1cf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:00:42.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4135" for this suite.

• [SLOW TEST:44.213 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":303,"completed":236,"skipped":3841,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:00:42.678: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 25 08:00:44.719: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:00:44.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6237" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":303,"completed":237,"skipped":3871,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:00:44.737: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-a7562f59-19b3-413e-9a87-ca8abdfd8251
STEP: Creating secret with name s-test-opt-upd-28af686a-d0b6-4c76-800e-c0f83644f7bf
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a7562f59-19b3-413e-9a87-ca8abdfd8251
STEP: Updating secret s-test-opt-upd-28af686a-d0b6-4c76-800e-c0f83644f7bf
STEP: Creating secret with name s-test-opt-create-e85757b4-a8f3-42cf-8a1d-a415987eccf4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:00:48.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8507" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":238,"skipped":3877,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:00:48.828: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2854
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2854
I1125 08:00:48.875711      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2854, replica count: 2
Nov 25 08:00:51.926: INFO: Creating new exec pod
I1125 08:00:51.926425      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 25 08:00:54.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2854 execpodjrbhf -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 25 08:00:55.135: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 25 08:00:55.135: INFO: stdout: ""
Nov 25 08:00:55.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2854 execpodjrbhf -- /bin/sh -x -c nc -zv -t -w 2 10.109.99.220 80'
Nov 25 08:00:55.296: INFO: stderr: "+ nc -zv -t -w 2 10.109.99.220 80\nConnection to 10.109.99.220 80 port [tcp/http] succeeded!\n"
Nov 25 08:00:55.296: INFO: stdout: ""
Nov 25 08:00:55.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2854 execpodjrbhf -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.2 31472'
Nov 25 08:00:55.446: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.2 31472\nConnection to 192.168.49.2 31472 port [tcp/31472] succeeded!\n"
Nov 25 08:00:55.446: INFO: stdout: ""
Nov 25 08:00:55.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-2854 execpodjrbhf -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.3 31472'
Nov 25 08:00:55.591: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.3 31472\nConnection to 192.168.49.3 31472 port [tcp/31472] succeeded!\n"
Nov 25 08:00:55.591: INFO: stdout: ""
Nov 25 08:00:55.591: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:00:55.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2854" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.834 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":303,"completed":239,"skipped":3879,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:00:55.662: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Nov 25 08:00:56.197: INFO: created pod pod-service-account-defaultsa
Nov 25 08:00:56.197: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 25 08:00:56.200: INFO: created pod pod-service-account-mountsa
Nov 25 08:00:56.200: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 25 08:00:56.207: INFO: created pod pod-service-account-nomountsa
Nov 25 08:00:56.207: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 25 08:00:56.216: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 25 08:00:56.216: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 25 08:00:56.220: INFO: created pod pod-service-account-mountsa-mountspec
Nov 25 08:00:56.220: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 25 08:00:56.226: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 25 08:00:56.227: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 25 08:00:56.233: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 25 08:00:56.234: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 25 08:00:56.236: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 25 08:00:56.236: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 25 08:00:56.241: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 25 08:00:56.241: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:00:56.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1898" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":303,"completed":240,"skipped":3906,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:00:56.325: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 08:00:56.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1bf6b4b1-3351-47ef-8ec0-8db061485747" in namespace "projected-6154" to be "Succeeded or Failed"
Nov 25 08:00:56.360: INFO: Pod "downwardapi-volume-1bf6b4b1-3351-47ef-8ec0-8db061485747": Phase="Pending", Reason="", readiness=false. Elapsed: 4.237645ms
Nov 25 08:00:58.363: INFO: Pod "downwardapi-volume-1bf6b4b1-3351-47ef-8ec0-8db061485747": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007412846s
Nov 25 08:01:00.365: INFO: Pod "downwardapi-volume-1bf6b4b1-3351-47ef-8ec0-8db061485747": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010031117s
Nov 25 08:01:02.369: INFO: Pod "downwardapi-volume-1bf6b4b1-3351-47ef-8ec0-8db061485747": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013488152s
STEP: Saw pod success
Nov 25 08:01:02.369: INFO: Pod "downwardapi-volume-1bf6b4b1-3351-47ef-8ec0-8db061485747" satisfied condition "Succeeded or Failed"
Nov 25 08:01:02.371: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-1bf6b4b1-3351-47ef-8ec0-8db061485747 container client-container: <nil>
STEP: delete the pod
Nov 25 08:01:02.391: INFO: Waiting for pod downwardapi-volume-1bf6b4b1-3351-47ef-8ec0-8db061485747 to disappear
Nov 25 08:01:02.393: INFO: Pod downwardapi-volume-1bf6b4b1-3351-47ef-8ec0-8db061485747 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:01:02.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6154" for this suite.

• [SLOW TEST:6.076 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":241,"skipped":3916,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:01:02.401: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-62f2bae4-f117-4a01-8131-26d5731e08a6
STEP: Creating a pod to test consume secrets
Nov 25 08:01:02.476: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4507607f-bf00-4e12-9191-2c8b1674d6d7" in namespace "projected-8242" to be "Succeeded or Failed"
Nov 25 08:01:02.479: INFO: Pod "pod-projected-secrets-4507607f-bf00-4e12-9191-2c8b1674d6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.103073ms
Nov 25 08:01:04.482: INFO: Pod "pod-projected-secrets-4507607f-bf00-4e12-9191-2c8b1674d6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006161534s
Nov 25 08:01:06.485: INFO: Pod "pod-projected-secrets-4507607f-bf00-4e12-9191-2c8b1674d6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008792806s
Nov 25 08:01:08.488: INFO: Pod "pod-projected-secrets-4507607f-bf00-4e12-9191-2c8b1674d6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011448983s
Nov 25 08:01:10.490: INFO: Pod "pod-projected-secrets-4507607f-bf00-4e12-9191-2c8b1674d6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013963863s
Nov 25 08:01:12.496: INFO: Pod "pod-projected-secrets-4507607f-bf00-4e12-9191-2c8b1674d6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.019450578s
Nov 25 08:01:14.498: INFO: Pod "pod-projected-secrets-4507607f-bf00-4e12-9191-2c8b1674d6d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.022125117s
STEP: Saw pod success
Nov 25 08:01:14.498: INFO: Pod "pod-projected-secrets-4507607f-bf00-4e12-9191-2c8b1674d6d7" satisfied condition "Succeeded or Failed"
Nov 25 08:01:14.500: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-4507607f-bf00-4e12-9191-2c8b1674d6d7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 25 08:01:14.536: INFO: Waiting for pod pod-projected-secrets-4507607f-bf00-4e12-9191-2c8b1674d6d7 to disappear
Nov 25 08:01:14.539: INFO: Pod pod-projected-secrets-4507607f-bf00-4e12-9191-2c8b1674d6d7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:01:14.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8242" for this suite.

• [SLOW TEST:12.143 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":242,"skipped":3918,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:01:14.544: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:01:14.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-7443" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":303,"completed":243,"skipped":3935,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:01:14.613: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:01:25.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8812" for this suite.

• [SLOW TEST:11.100 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":303,"completed":244,"skipped":3966,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:01:25.713: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Nov 25 08:01:25.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 create -f -'
Nov 25 08:01:26.010: INFO: stderr: ""
Nov 25 08:01:26.010: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Nov 25 08:01:26.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 diff -f -'
Nov 25 08:01:26.476: INFO: rc: 1
Nov 25 08:01:26.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 delete -f -'
Nov 25 08:01:26.546: INFO: stderr: ""
Nov 25 08:01:26.547: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:01:26.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4044" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":303,"completed":245,"skipped":3983,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:01:26.554: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 08:03:26.590: INFO: Deleting pod "var-expansion-a761a7f0-aeea-434b-b29b-633f13624fcb" in namespace "var-expansion-8500"
Nov 25 08:03:26.594: INFO: Wait up to 5m0s for pod "var-expansion-a761a7f0-aeea-434b-b29b-633f13624fcb" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:03:32.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8500" for this suite.

• [SLOW TEST:126.070 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":303,"completed":246,"skipped":4016,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:03:32.624: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 08:03:32.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 create -f - --namespace=kubectl-830'
Nov 25 08:03:32.961: INFO: stderr: ""
Nov 25 08:03:32.961: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Nov 25 08:03:32.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 create -f - --namespace=kubectl-830'
Nov 25 08:03:33.212: INFO: stderr: ""
Nov 25 08:03:33.212: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 25 08:03:34.215: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 25 08:03:34.215: INFO: Found 1 / 1
Nov 25 08:03:34.215: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 25 08:03:34.217: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 25 08:03:34.217: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 25 08:03:34.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 describe pod agnhost-primary-hjcrh --namespace=kubectl-830'
Nov 25 08:03:34.299: INFO: stderr: ""
Nov 25 08:03:34.299: INFO: stdout: "Name:         agnhost-primary-hjcrh\nNamespace:    kubectl-830\nPriority:     0\nNode:         k8sconformance-m02/192.168.49.3\nStart Time:   Wed, 25 Nov 2020 08:03:33 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.244.1.54\nIPs:\n  IP:           10.244.1.54\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://1569a39326d5dfe64f2ce4dc41f8915ec653365c11ad9e53f3a0b70a919f148e\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 25 Nov 2020 08:03:34 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wdks2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-wdks2:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-wdks2\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason            Age              From               Message\n  ----     ------            ----             ----               -------\n  Normal   Scheduled         2s               default-scheduler  Successfully assigned kubectl-830/agnhost-primary-hjcrh to k8sconformance-m02\n  Normal   Pulled            1s               kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal   Created           1s               kubelet            Created container agnhost-primary\n  Warning  DNSConfigForming  0s (x3 over 1s)  kubelet            Search Line limits were exceeded, some search paths have been omitted, the applied search line is: kubectl-830.svc.cluster.local svc.cluster.local cluster.local corp.google.com prod.google.com prodz.google.com\n  Normal   Started           0s               kubelet            Started container agnhost-primary\n"
Nov 25 08:03:34.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 describe rc agnhost-primary --namespace=kubectl-830'
Nov 25 08:03:34.387: INFO: stderr: ""
Nov 25 08:03:34.387: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-830\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-hjcrh\n"
Nov 25 08:03:34.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 describe service agnhost-primary --namespace=kubectl-830'
Nov 25 08:03:34.481: INFO: stderr: ""
Nov 25 08:03:34.481: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-830\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.108.132.165\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.54:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 25 08:03:34.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 describe node k8sconformance'
Nov 25 08:03:34.574: INFO: stderr: ""
Nov 25 08:03:34.574: INFO: stdout: "Name:               k8sconformance\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8sconformance\n                    kubernetes.io/os=linux\n                    minikube.k8s.io/commit=640e1cb255210abb49eb7aab8001b9b425f1a161\n                    minikube.k8s.io/name=k8sconformance\n                    minikube.k8s.io/updated_at=2020_11_25T06_43_44_0700\n                    minikube.k8s.io/version=v1.15.1\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 25 Nov 2020 06:43:40 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8sconformance\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 25 Nov 2020 08:03:31 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 25 Nov 2020 07:59:20 +0000   Wed, 25 Nov 2020 06:43:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 25 Nov 2020 07:59:20 +0000   Wed, 25 Nov 2020 06:43:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 25 Nov 2020 07:59:20 +0000   Wed, 25 Nov 2020 06:43:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 25 Nov 2020 07:59:20 +0000   Wed, 25 Nov 2020 06:43:58 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.49.2\n  Hostname:    k8sconformance\nCapacity:\n  cpu:                8\n  ephemeral-storage:  510022312Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             107153064Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  510022312Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             107153064Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 71c30fdd633a4520ab1e8072fcf4c664\n  System UUID:                ef422a11-0e1b-4cb0-a67c-33e690ca4acc\n  Boot ID:                    46278987-f672-4a3d-8bc8-9b2102acd9b7\n  Kernel Version:             5.7.17-1rodete4-amd64\n  OS Image:                   Ubuntu 20.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.3.7\n  Kubelet Version:            v1.19.4\n  Kube-Proxy Version:         v1.19.4\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-f9fd979d6-z4vhz                                    100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     79m\n  kube-system                 etcd-k8sconformance                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 kindnet-n566x                                              100m (1%)     100m (1%)   50Mi (0%)        50Mi (0%)      79m\n  kube-system                 kube-apiserver-k8sconformance                              250m (3%)     0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 kube-controller-manager-k8sconformance                     200m (2%)     0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 kube-proxy-58hhs                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 kube-scheduler-k8sconformance                              100m (1%)     0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 storage-provisioner                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m\n  sonobuoy                    sonobuoy-e2e-job-7010f57f836a484c                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-25ff39ce45304256-mnllj    0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                750m (9%)   100m (1%)\n  memory             120Mi (0%)  220Mi (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Nov 25 08:03:34.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 describe namespace kubectl-830'
Nov 25 08:03:34.648: INFO: stderr: ""
Nov 25 08:03:34.648: INFO: stdout: "Name:         kubectl-830\nLabels:       e2e-framework=kubectl\n              e2e-run=7ef5a412-9337-4820-96c8-33bcf80b8254\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:03:34.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-830" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":303,"completed":247,"skipped":4037,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:03:34.654: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 08:05:34.686: INFO: Deleting pod "var-expansion-40b00aa9-01c1-4080-b19e-38a1e64021ef" in namespace "var-expansion-6646"
Nov 25 08:05:34.691: INFO: Wait up to 5m0s for pod "var-expansion-40b00aa9-01c1-4080-b19e-38a1e64021ef" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:05:38.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6646" for this suite.

• [SLOW TEST:124.047 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":303,"completed":248,"skipped":4038,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:05:38.701: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-8ade201e-5852-4750-89bd-7e7c81d3b954
STEP: Creating a pod to test consume secrets
Nov 25 08:05:38.732: INFO: Waiting up to 5m0s for pod "pod-secrets-3c62a789-944b-4cb2-a9a4-c0a47123fa78" in namespace "secrets-7689" to be "Succeeded or Failed"
Nov 25 08:05:38.737: INFO: Pod "pod-secrets-3c62a789-944b-4cb2-a9a4-c0a47123fa78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.165539ms
Nov 25 08:05:40.739: INFO: Pod "pod-secrets-3c62a789-944b-4cb2-a9a4-c0a47123fa78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006937947s
STEP: Saw pod success
Nov 25 08:05:40.739: INFO: Pod "pod-secrets-3c62a789-944b-4cb2-a9a4-c0a47123fa78" satisfied condition "Succeeded or Failed"
Nov 25 08:05:40.741: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-3c62a789-944b-4cb2-a9a4-c0a47123fa78 container secret-volume-test: <nil>
STEP: delete the pod
Nov 25 08:05:40.761: INFO: Waiting for pod pod-secrets-3c62a789-944b-4cb2-a9a4-c0a47123fa78 to disappear
Nov 25 08:05:40.763: INFO: Pod pod-secrets-3c62a789-944b-4cb2-a9a4-c0a47123fa78 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:05:40.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7689" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":303,"completed":249,"skipped":4051,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:05:40.768: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-435ac512-e7aa-4c8f-9d64-41489eb26b56
STEP: Creating a pod to test consume configMaps
Nov 25 08:05:40.800: INFO: Waiting up to 5m0s for pod "pod-configmaps-fad91af7-228b-41c3-b3b8-2ea6d5f1df65" in namespace "configmap-6785" to be "Succeeded or Failed"
Nov 25 08:05:40.802: INFO: Pod "pod-configmaps-fad91af7-228b-41c3-b3b8-2ea6d5f1df65": Phase="Pending", Reason="", readiness=false. Elapsed: 1.501694ms
Nov 25 08:05:42.804: INFO: Pod "pod-configmaps-fad91af7-228b-41c3-b3b8-2ea6d5f1df65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004423727s
STEP: Saw pod success
Nov 25 08:05:42.805: INFO: Pod "pod-configmaps-fad91af7-228b-41c3-b3b8-2ea6d5f1df65" satisfied condition "Succeeded or Failed"
Nov 25 08:05:42.806: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-fad91af7-228b-41c3-b3b8-2ea6d5f1df65 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 25 08:05:42.816: INFO: Waiting for pod pod-configmaps-fad91af7-228b-41c3-b3b8-2ea6d5f1df65 to disappear
Nov 25 08:05:42.821: INFO: Pod pod-configmaps-fad91af7-228b-41c3-b3b8-2ea6d5f1df65 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:05:42.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6785" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":303,"completed":250,"skipped":4060,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:05:42.826: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-72bb85e0-4580-4d35-a64d-5e647b4e54ab
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-72bb85e0-4580-4d35-a64d-5e647b4e54ab
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:05:46.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2956" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":251,"skipped":4070,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:05:46.888: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 08:05:46.922: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0254412-53fa-471c-802c-bcc623232b66" in namespace "projected-9323" to be "Succeeded or Failed"
Nov 25 08:05:46.928: INFO: Pod "downwardapi-volume-d0254412-53fa-471c-802c-bcc623232b66": Phase="Pending", Reason="", readiness=false. Elapsed: 5.751707ms
Nov 25 08:05:48.931: INFO: Pod "downwardapi-volume-d0254412-53fa-471c-802c-bcc623232b66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008867225s
STEP: Saw pod success
Nov 25 08:05:48.931: INFO: Pod "downwardapi-volume-d0254412-53fa-471c-802c-bcc623232b66" satisfied condition "Succeeded or Failed"
Nov 25 08:05:48.933: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-d0254412-53fa-471c-802c-bcc623232b66 container client-container: <nil>
STEP: delete the pod
Nov 25 08:05:48.943: INFO: Waiting for pod downwardapi-volume-d0254412-53fa-471c-802c-bcc623232b66 to disappear
Nov 25 08:05:48.948: INFO: Pod downwardapi-volume-d0254412-53fa-471c-802c-bcc623232b66 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:05:48.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9323" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":303,"completed":252,"skipped":4092,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:05:48.952: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 08:05:48.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 version'
Nov 25 08:05:49.055: INFO: stderr: ""
Nov 25 08:05:49.055: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-11T13:17:17Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-11T13:09:17Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:05:49.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5982" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":303,"completed":253,"skipped":4105,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:05:49.061: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-ab2b1ca4-d687-43d8-845a-de5400cfba14
STEP: Creating a pod to test consume secrets
Nov 25 08:05:49.086: INFO: Waiting up to 5m0s for pod "pod-secrets-e24042db-751d-4571-9c12-6d78c57c7526" in namespace "secrets-4474" to be "Succeeded or Failed"
Nov 25 08:05:49.088: INFO: Pod "pod-secrets-e24042db-751d-4571-9c12-6d78c57c7526": Phase="Pending", Reason="", readiness=false. Elapsed: 1.469457ms
Nov 25 08:05:51.090: INFO: Pod "pod-secrets-e24042db-751d-4571-9c12-6d78c57c7526": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004337065s
STEP: Saw pod success
Nov 25 08:05:51.090: INFO: Pod "pod-secrets-e24042db-751d-4571-9c12-6d78c57c7526" satisfied condition "Succeeded or Failed"
Nov 25 08:05:51.092: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-e24042db-751d-4571-9c12-6d78c57c7526 container secret-volume-test: <nil>
STEP: delete the pod
Nov 25 08:05:51.106: INFO: Waiting for pod pod-secrets-e24042db-751d-4571-9c12-6d78c57c7526 to disappear
Nov 25 08:05:51.110: INFO: Pod pod-secrets-e24042db-751d-4571-9c12-6d78c57c7526 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:05:51.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4474" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":254,"skipped":4115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:05:51.115: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-b2ab14d4-66df-44f6-a3f6-94c546a4f6ad
STEP: Creating a pod to test consume configMaps
Nov 25 08:05:51.153: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ce0e681-cdbe-4120-afaa-6cffca3853d2" in namespace "configmap-4739" to be "Succeeded or Failed"
Nov 25 08:05:51.157: INFO: Pod "pod-configmaps-0ce0e681-cdbe-4120-afaa-6cffca3853d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018381ms
Nov 25 08:05:53.160: INFO: Pod "pod-configmaps-0ce0e681-cdbe-4120-afaa-6cffca3853d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00682203s
STEP: Saw pod success
Nov 25 08:05:53.160: INFO: Pod "pod-configmaps-0ce0e681-cdbe-4120-afaa-6cffca3853d2" satisfied condition "Succeeded or Failed"
Nov 25 08:05:53.162: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-0ce0e681-cdbe-4120-afaa-6cffca3853d2 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 25 08:05:53.179: INFO: Waiting for pod pod-configmaps-0ce0e681-cdbe-4120-afaa-6cffca3853d2 to disappear
Nov 25 08:05:53.180: INFO: Pod pod-configmaps-0ce0e681-cdbe-4120-afaa-6cffca3853d2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:05:53.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4739" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":255,"skipped":4139,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:05:53.185: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 25 08:05:53.210: INFO: Waiting up to 5m0s for pod "pod-45864182-ad2b-42c7-8f66-f1ac77448001" in namespace "emptydir-4324" to be "Succeeded or Failed"
Nov 25 08:05:53.212: INFO: Pod "pod-45864182-ad2b-42c7-8f66-f1ac77448001": Phase="Pending", Reason="", readiness=false. Elapsed: 1.471369ms
Nov 25 08:05:55.214: INFO: Pod "pod-45864182-ad2b-42c7-8f66-f1ac77448001": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004134887s
STEP: Saw pod success
Nov 25 08:05:55.214: INFO: Pod "pod-45864182-ad2b-42c7-8f66-f1ac77448001" satisfied condition "Succeeded or Failed"
Nov 25 08:05:55.216: INFO: Trying to get logs from node k8sconformance-m02 pod pod-45864182-ad2b-42c7-8f66-f1ac77448001 container test-container: <nil>
STEP: delete the pod
Nov 25 08:05:55.231: INFO: Waiting for pod pod-45864182-ad2b-42c7-8f66-f1ac77448001 to disappear
Nov 25 08:05:55.232: INFO: Pod pod-45864182-ad2b-42c7-8f66-f1ac77448001 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:05:55.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4324" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":256,"skipped":4150,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:05:55.258: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Nov 25 08:05:55.277: INFO: namespace kubectl-7525
Nov 25 08:05:55.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 create -f - --namespace=kubectl-7525'
Nov 25 08:05:55.489: INFO: stderr: ""
Nov 25 08:05:55.489: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 25 08:05:56.492: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 25 08:05:56.492: INFO: Found 0 / 1
Nov 25 08:05:57.492: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 25 08:05:57.492: INFO: Found 1 / 1
Nov 25 08:05:57.492: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 25 08:05:57.494: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 25 08:05:57.494: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 25 08:05:57.494: INFO: wait on agnhost-primary startup in kubectl-7525 
Nov 25 08:05:57.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 logs agnhost-primary-p5bpk agnhost-primary --namespace=kubectl-7525'
Nov 25 08:05:57.566: INFO: stderr: ""
Nov 25 08:05:57.566: INFO: stdout: "Paused\n"
STEP: exposing RC
Nov 25 08:05:57.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7525'
Nov 25 08:05:57.677: INFO: stderr: ""
Nov 25 08:05:57.677: INFO: stdout: "service/rm2 exposed\n"
Nov 25 08:05:57.681: INFO: Service rm2 in namespace kubectl-7525 found.
STEP: exposing service
Nov 25 08:05:59.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7525'
Nov 25 08:05:59.765: INFO: stderr: ""
Nov 25 08:05:59.765: INFO: stdout: "service/rm3 exposed\n"
Nov 25 08:05:59.770: INFO: Service rm3 in namespace kubectl-7525 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:06:01.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7525" for this suite.

• [SLOW TEST:6.521 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":303,"completed":257,"skipped":4151,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:06:01.779: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 25 08:06:01.814: INFO: Waiting up to 5m0s for pod "pod-385a427d-aaaa-4bad-8c8b-86b910d332e5" in namespace "emptydir-8582" to be "Succeeded or Failed"
Nov 25 08:06:01.816: INFO: Pod "pod-385a427d-aaaa-4bad-8c8b-86b910d332e5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.602759ms
Nov 25 08:06:03.819: INFO: Pod "pod-385a427d-aaaa-4bad-8c8b-86b910d332e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004529558s
STEP: Saw pod success
Nov 25 08:06:03.819: INFO: Pod "pod-385a427d-aaaa-4bad-8c8b-86b910d332e5" satisfied condition "Succeeded or Failed"
Nov 25 08:06:03.821: INFO: Trying to get logs from node k8sconformance-m02 pod pod-385a427d-aaaa-4bad-8c8b-86b910d332e5 container test-container: <nil>
STEP: delete the pod
Nov 25 08:06:03.830: INFO: Waiting for pod pod-385a427d-aaaa-4bad-8c8b-86b910d332e5 to disappear
Nov 25 08:06:03.834: INFO: Pod pod-385a427d-aaaa-4bad-8c8b-86b910d332e5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:06:03.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8582" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":258,"skipped":4169,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:06:03.840: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-a02606f1-363f-4a63-b95b-c1d885670e4c
STEP: Creating a pod to test consume configMaps
Nov 25 08:06:03.865: INFO: Waiting up to 5m0s for pod "pod-configmaps-5ad19e55-99d2-4099-846b-6cd05cf4483d" in namespace "configmap-7097" to be "Succeeded or Failed"
Nov 25 08:06:03.867: INFO: Pod "pod-configmaps-5ad19e55-99d2-4099-846b-6cd05cf4483d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.458313ms
Nov 25 08:06:05.871: INFO: Pod "pod-configmaps-5ad19e55-99d2-4099-846b-6cd05cf4483d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005684645s
STEP: Saw pod success
Nov 25 08:06:05.871: INFO: Pod "pod-configmaps-5ad19e55-99d2-4099-846b-6cd05cf4483d" satisfied condition "Succeeded or Failed"
Nov 25 08:06:05.873: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-5ad19e55-99d2-4099-846b-6cd05cf4483d container configmap-volume-test: <nil>
STEP: delete the pod
Nov 25 08:06:05.904: INFO: Waiting for pod pod-configmaps-5ad19e55-99d2-4099-846b-6cd05cf4483d to disappear
Nov 25 08:06:05.906: INFO: Pod pod-configmaps-5ad19e55-99d2-4099-846b-6cd05cf4483d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:06:05.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7097" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":259,"skipped":4175,"failed":0}
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:06:05.911: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Nov 25 08:06:05.945: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:06:05.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1415" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":303,"completed":260,"skipped":4185,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:06:05.961: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Nov 25 08:06:05.980: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:06:20.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3061" for this suite.

• [SLOW TEST:14.634 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":303,"completed":261,"skipped":4202,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:06:20.596: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov 25 08:06:20.645: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 25 08:07:20.657: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Nov 25 08:07:20.673: INFO: Created pod: pod0-sched-preemption-low-priority
Nov 25 08:07:20.721: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:07:44.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2307" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:84.174 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":303,"completed":262,"skipped":4235,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:07:44.770: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:07:46.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1448" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":303,"completed":263,"skipped":4267,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:07:46.824: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:07:46.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4008" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":303,"completed":264,"skipped":4297,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:07:46.870: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov 25 08:07:46.928: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9400 /api/v1/namespaces/watch-9400/configmaps/e2e-watch-test-label-changed 4b4076b1-d461-4062-a043-39c25242b286 21229 0 2020-11-25 08:07:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-25 08:07:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 25 08:07:46.928: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9400 /api/v1/namespaces/watch-9400/configmaps/e2e-watch-test-label-changed 4b4076b1-d461-4062-a043-39c25242b286 21230 0 2020-11-25 08:07:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-25 08:07:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 25 08:07:46.928: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9400 /api/v1/namespaces/watch-9400/configmaps/e2e-watch-test-label-changed 4b4076b1-d461-4062-a043-39c25242b286 21231 0 2020-11-25 08:07:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-25 08:07:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov 25 08:07:56.967: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9400 /api/v1/namespaces/watch-9400/configmaps/e2e-watch-test-label-changed 4b4076b1-d461-4062-a043-39c25242b286 21288 0 2020-11-25 08:07:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-25 08:07:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 25 08:07:56.967: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9400 /api/v1/namespaces/watch-9400/configmaps/e2e-watch-test-label-changed 4b4076b1-d461-4062-a043-39c25242b286 21289 0 2020-11-25 08:07:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-25 08:07:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 25 08:07:56.967: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9400 /api/v1/namespaces/watch-9400/configmaps/e2e-watch-test-label-changed 4b4076b1-d461-4062-a043-39c25242b286 21290 0 2020-11-25 08:07:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-25 08:07:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:07:56.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9400" for this suite.

• [SLOW TEST:10.102 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":303,"completed":265,"skipped":4302,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:07:56.972: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1125 08:08:03.014227      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 25 08:09:05.026: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:09:05.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1071" for this suite.

• [SLOW TEST:68.061 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":303,"completed":266,"skipped":4313,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:09:05.033: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7328.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7328.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7328.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7328.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7328.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7328.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 25 08:09:09.139: INFO: DNS probes using dns-7328/dns-test-5cff9c7c-3864-4fa7-a76e-f5ea81122d58 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:09:09.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7328" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":303,"completed":267,"skipped":4323,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:09:09.160: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 08:09:09.293: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 25 08:09:12.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-3118 create -f -'
Nov 25 08:09:12.967: INFO: stderr: ""
Nov 25 08:09:12.967: INFO: stdout: "e2e-test-crd-publish-openapi-428-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 25 08:09:12.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-3118 delete e2e-test-crd-publish-openapi-428-crds test-cr'
Nov 25 08:09:13.040: INFO: stderr: ""
Nov 25 08:09:13.040: INFO: stdout: "e2e-test-crd-publish-openapi-428-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov 25 08:09:13.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-3118 apply -f -'
Nov 25 08:09:13.274: INFO: stderr: ""
Nov 25 08:09:13.274: INFO: stdout: "e2e-test-crd-publish-openapi-428-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 25 08:09:13.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 --namespace=crd-publish-openapi-3118 delete e2e-test-crd-publish-openapi-428-crds test-cr'
Nov 25 08:09:13.344: INFO: stderr: ""
Nov 25 08:09:13.344: INFO: stdout: "e2e-test-crd-publish-openapi-428-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Nov 25 08:09:13.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 explain e2e-test-crd-publish-openapi-428-crds'
Nov 25 08:09:13.624: INFO: stderr: ""
Nov 25 08:09:13.624: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-428-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:09:16.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3118" for this suite.

• [SLOW TEST:7.290 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":303,"completed":268,"skipped":4330,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:09:16.450: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:09:16.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8072" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":303,"completed":269,"skipped":4347,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:09:16.561: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:09:16.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4105" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":303,"completed":270,"skipped":4352,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:09:16.589: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 08:09:16.612: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:09:18.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-110" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":303,"completed":271,"skipped":4365,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:09:18.707: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:10:18.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7844" for this suite.

• [SLOW TEST:60.050 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":303,"completed":272,"skipped":4372,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:10:18.756: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 25 08:10:24.840: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 25 08:10:24.846: INFO: Pod pod-with-prestop-http-hook still exists
Nov 25 08:10:26.846: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 25 08:10:26.849: INFO: Pod pod-with-prestop-http-hook still exists
Nov 25 08:10:28.846: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 25 08:10:28.850: INFO: Pod pod-with-prestop-http-hook still exists
Nov 25 08:10:30.846: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 25 08:10:30.849: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:10:30.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2976" for this suite.

• [SLOW TEST:12.108 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":303,"completed":273,"skipped":4386,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:10:30.865: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Nov 25 08:10:30.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 create -f - --namespace=kubectl-7447'
Nov 25 08:10:31.674: INFO: stderr: ""
Nov 25 08:10:31.674: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 25 08:10:31.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7447'
Nov 25 08:10:31.749: INFO: stderr: ""
Nov 25 08:10:31.749: INFO: stdout: "update-demo-nautilus-76x5w update-demo-nautilus-xlzht "
Nov 25 08:10:31.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-76x5w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Nov 25 08:10:31.862: INFO: stderr: ""
Nov 25 08:10:31.862: INFO: stdout: ""
Nov 25 08:10:31.862: INFO: update-demo-nautilus-76x5w is created but not running
Nov 25 08:10:36.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7447'
Nov 25 08:10:36.932: INFO: stderr: ""
Nov 25 08:10:36.932: INFO: stdout: "update-demo-nautilus-76x5w update-demo-nautilus-xlzht "
Nov 25 08:10:36.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-76x5w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Nov 25 08:10:36.999: INFO: stderr: ""
Nov 25 08:10:36.999: INFO: stdout: "true"
Nov 25 08:10:36.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-76x5w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Nov 25 08:10:37.069: INFO: stderr: ""
Nov 25 08:10:37.069: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 25 08:10:37.069: INFO: validating pod update-demo-nautilus-76x5w
Nov 25 08:10:37.072: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 25 08:10:37.072: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 25 08:10:37.072: INFO: update-demo-nautilus-76x5w is verified up and running
Nov 25 08:10:37.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-xlzht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Nov 25 08:10:37.139: INFO: stderr: ""
Nov 25 08:10:37.139: INFO: stdout: "true"
Nov 25 08:10:37.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-xlzht -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Nov 25 08:10:37.209: INFO: stderr: ""
Nov 25 08:10:37.209: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 25 08:10:37.209: INFO: validating pod update-demo-nautilus-xlzht
Nov 25 08:10:37.213: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 25 08:10:37.213: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 25 08:10:37.213: INFO: update-demo-nautilus-xlzht is verified up and running
STEP: scaling down the replication controller
Nov 25 08:10:37.215: INFO: scanned /root for discovery docs: <nil>
Nov 25 08:10:37.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7447'
Nov 25 08:10:38.307: INFO: stderr: ""
Nov 25 08:10:38.307: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 25 08:10:38.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7447'
Nov 25 08:10:38.382: INFO: stderr: ""
Nov 25 08:10:38.382: INFO: stdout: "update-demo-nautilus-76x5w update-demo-nautilus-xlzht "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 25 08:10:43.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7447'
Nov 25 08:10:43.453: INFO: stderr: ""
Nov 25 08:10:43.453: INFO: stdout: "update-demo-nautilus-xlzht "
Nov 25 08:10:43.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-xlzht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Nov 25 08:10:43.521: INFO: stderr: ""
Nov 25 08:10:43.521: INFO: stdout: "true"
Nov 25 08:10:43.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-xlzht -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Nov 25 08:10:43.591: INFO: stderr: ""
Nov 25 08:10:43.591: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 25 08:10:43.591: INFO: validating pod update-demo-nautilus-xlzht
Nov 25 08:10:43.594: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 25 08:10:43.594: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 25 08:10:43.594: INFO: update-demo-nautilus-xlzht is verified up and running
STEP: scaling up the replication controller
Nov 25 08:10:43.596: INFO: scanned /root for discovery docs: <nil>
Nov 25 08:10:43.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7447'
Nov 25 08:10:44.688: INFO: stderr: ""
Nov 25 08:10:44.688: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 25 08:10:44.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7447'
Nov 25 08:10:44.761: INFO: stderr: ""
Nov 25 08:10:44.761: INFO: stdout: "update-demo-nautilus-f62gl update-demo-nautilus-xlzht "
Nov 25 08:10:44.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-f62gl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Nov 25 08:10:44.853: INFO: stderr: ""
Nov 25 08:10:44.853: INFO: stdout: "true"
Nov 25 08:10:44.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-f62gl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Nov 25 08:10:44.921: INFO: stderr: ""
Nov 25 08:10:44.921: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 25 08:10:44.921: INFO: validating pod update-demo-nautilus-f62gl
Nov 25 08:10:44.925: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 25 08:10:44.925: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 25 08:10:44.925: INFO: update-demo-nautilus-f62gl is verified up and running
Nov 25 08:10:44.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-xlzht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Nov 25 08:10:44.993: INFO: stderr: ""
Nov 25 08:10:44.993: INFO: stdout: "true"
Nov 25 08:10:44.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods update-demo-nautilus-xlzht -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Nov 25 08:10:45.065: INFO: stderr: ""
Nov 25 08:10:45.065: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 25 08:10:45.065: INFO: validating pod update-demo-nautilus-xlzht
Nov 25 08:10:45.110: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 25 08:10:45.110: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 25 08:10:45.110: INFO: update-demo-nautilus-xlzht is verified up and running
STEP: using delete to clean up resources
Nov 25 08:10:45.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 delete --grace-period=0 --force -f - --namespace=kubectl-7447'
Nov 25 08:10:45.230: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 25 08:10:45.230: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 25 08:10:45.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7447'
Nov 25 08:10:45.323: INFO: stderr: "No resources found in kubectl-7447 namespace.\n"
Nov 25 08:10:45.323: INFO: stdout: ""
Nov 25 08:10:45.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods -l name=update-demo --namespace=kubectl-7447 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 25 08:10:45.396: INFO: stderr: ""
Nov 25 08:10:45.396: INFO: stdout: "update-demo-nautilus-f62gl\nupdate-demo-nautilus-xlzht\n"
Nov 25 08:10:45.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7447'
Nov 25 08:10:45.968: INFO: stderr: "No resources found in kubectl-7447 namespace.\n"
Nov 25 08:10:45.968: INFO: stdout: ""
Nov 25 08:10:45.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 get pods -l name=update-demo --namespace=kubectl-7447 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 25 08:10:46.038: INFO: stderr: ""
Nov 25 08:10:46.039: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:10:46.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7447" for this suite.

• [SLOW TEST:15.180 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":303,"completed":274,"skipped":4394,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:10:46.045: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9624
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-9624
Nov 25 08:10:46.083: INFO: Found 0 stateful pods, waiting for 1
Nov 25 08:10:56.086: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 25 08:10:56.124: INFO: Deleting all statefulset in ns statefulset-9624
Nov 25 08:10:56.126: INFO: Scaling statefulset ss to 0
Nov 25 08:11:06.141: INFO: Waiting for statefulset status.replicas updated to 0
Nov 25 08:11:06.143: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:11:06.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9624" for this suite.

• [SLOW TEST:20.136 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":303,"completed":275,"skipped":4395,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:11:06.182: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1881.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1881.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 25 08:11:10.264: INFO: DNS probes using dns-1881/dns-test-673f1334-f5a7-4568-83f1-e8c59a285fa5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:11:10.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1881" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":303,"completed":276,"skipped":4415,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:11:10.280: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 08:11:10.702: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 08:11:13.721: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 08:11:13.724: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-981-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:11:14.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4711" for this suite.
STEP: Destroying namespace "webhook-4711-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":303,"completed":277,"skipped":4433,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:11:15.021: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1323
STEP: creating service affinity-nodeport-transition in namespace services-1323
STEP: creating replication controller affinity-nodeport-transition in namespace services-1323
I1125 08:11:15.121916      23 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-1323, replica count: 3
I1125 08:11:18.209991      23 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 25 08:11:18.216: INFO: Creating new exec pod
Nov 25 08:11:21.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-1323 execpod-affinity2grzf -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Nov 25 08:11:21.401: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Nov 25 08:11:21.401: INFO: stdout: ""
Nov 25 08:11:21.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-1323 execpod-affinity2grzf -- /bin/sh -x -c nc -zv -t -w 2 10.104.165.82 80'
Nov 25 08:11:21.554: INFO: stderr: "+ nc -zv -t -w 2 10.104.165.82 80\nConnection to 10.104.165.82 80 port [tcp/http] succeeded!\n"
Nov 25 08:11:21.554: INFO: stdout: ""
Nov 25 08:11:21.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-1323 execpod-affinity2grzf -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.2 32711'
Nov 25 08:11:21.703: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.2 32711\nConnection to 192.168.49.2 32711 port [tcp/32711] succeeded!\n"
Nov 25 08:11:21.703: INFO: stdout: ""
Nov 25 08:11:21.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-1323 execpod-affinity2grzf -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.3 32711'
Nov 25 08:11:21.850: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.3 32711\nConnection to 192.168.49.3 32711 port [tcp/32711] succeeded!\n"
Nov 25 08:11:21.850: INFO: stdout: ""
Nov 25 08:11:21.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-1323 execpod-affinity2grzf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:32711/ ; done'
Nov 25 08:11:22.088: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n"
Nov 25 08:11:22.088: INFO: stdout: "\naffinity-nodeport-transition-sgdcw\naffinity-nodeport-transition-sgdcw\naffinity-nodeport-transition-sgdcw\naffinity-nodeport-transition-bjgqg\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-sgdcw\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-bjgqg\naffinity-nodeport-transition-bjgqg\naffinity-nodeport-transition-bjgqg\naffinity-nodeport-transition-bjgqg\naffinity-nodeport-transition-sgdcw\naffinity-nodeport-transition-bjgqg\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8"
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-sgdcw
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-sgdcw
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-sgdcw
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-bjgqg
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-sgdcw
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-bjgqg
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-bjgqg
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-bjgqg
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-bjgqg
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-sgdcw
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-bjgqg
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.088: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 exec --namespace=services-1323 execpod-affinity2grzf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:32711/ ; done'
Nov 25 08:11:22.318: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32711/\n"
Nov 25 08:11:22.319: INFO: stdout: "\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8\naffinity-nodeport-transition-ndhm8"
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Received response from host: affinity-nodeport-transition-ndhm8
Nov 25 08:11:22.319: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1323, will wait for the garbage collector to delete the pods
Nov 25 08:11:22.538: INFO: Deleting ReplicationController affinity-nodeport-transition took: 157.622155ms
Nov 25 08:11:22.639: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.224314ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:11:30.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1323" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:15.756 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":278,"skipped":4439,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:11:30.778: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:11:30.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7571" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":303,"completed":279,"skipped":4456,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:11:30.844: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:11:32.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5822" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":280,"skipped":4457,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:11:32.890: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-c7474227-3d55-45f7-a621-e25a36d025ea
STEP: Creating a pod to test consume configMaps
Nov 25 08:11:32.922: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-474c4875-c536-48b9-ad46-313664c241e9" in namespace "projected-7104" to be "Succeeded or Failed"
Nov 25 08:11:32.926: INFO: Pod "pod-projected-configmaps-474c4875-c536-48b9-ad46-313664c241e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.677988ms
Nov 25 08:11:34.930: INFO: Pod "pod-projected-configmaps-474c4875-c536-48b9-ad46-313664c241e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007780623s
Nov 25 08:11:36.933: INFO: Pod "pod-projected-configmaps-474c4875-c536-48b9-ad46-313664c241e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010848122s
STEP: Saw pod success
Nov 25 08:11:36.933: INFO: Pod "pod-projected-configmaps-474c4875-c536-48b9-ad46-313664c241e9" satisfied condition "Succeeded or Failed"
Nov 25 08:11:36.935: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-474c4875-c536-48b9-ad46-313664c241e9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 25 08:11:36.950: INFO: Waiting for pod pod-projected-configmaps-474c4875-c536-48b9-ad46-313664c241e9 to disappear
Nov 25 08:11:36.952: INFO: Pod pod-projected-configmaps-474c4875-c536-48b9-ad46-313664c241e9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:11:36.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7104" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":281,"skipped":4474,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:11:36.958: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-669585e5-6005-4d76-b9aa-b88b01b89f75
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:11:36.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1962" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":303,"completed":282,"skipped":4482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:11:36.997: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-0d47f326-2c71-4d06-8db5-02b687dbae27
Nov 25 08:11:37.051: INFO: Pod name my-hostname-basic-0d47f326-2c71-4d06-8db5-02b687dbae27: Found 0 pods out of 1
Nov 25 08:11:42.053: INFO: Pod name my-hostname-basic-0d47f326-2c71-4d06-8db5-02b687dbae27: Found 1 pods out of 1
Nov 25 08:11:42.053: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0d47f326-2c71-4d06-8db5-02b687dbae27" are running
Nov 25 08:11:42.055: INFO: Pod "my-hostname-basic-0d47f326-2c71-4d06-8db5-02b687dbae27-t4c79" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-25 08:11:37 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-25 08:11:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-25 08:11:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-25 08:11:37 +0000 UTC Reason: Message:}])
Nov 25 08:11:42.055: INFO: Trying to dial the pod
Nov 25 08:11:47.063: INFO: Controller my-hostname-basic-0d47f326-2c71-4d06-8db5-02b687dbae27: Got expected result from replica 1 [my-hostname-basic-0d47f326-2c71-4d06-8db5-02b687dbae27-t4c79]: "my-hostname-basic-0d47f326-2c71-4d06-8db5-02b687dbae27-t4c79", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:11:47.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5985" for this suite.

• [SLOW TEST:10.072 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":303,"completed":283,"skipped":4518,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:11:47.069: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-1121
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1121 to expose endpoints map[]
Nov 25 08:11:47.101: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Nov 25 08:11:48.106: INFO: successfully validated that service multi-endpoint-test in namespace services-1121 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1121
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1121 to expose endpoints map[pod1:[100]]
Nov 25 08:11:50.146: INFO: successfully validated that service multi-endpoint-test in namespace services-1121 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1121
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1121 to expose endpoints map[pod1:[100] pod2:[101]]
Nov 25 08:11:52.163: INFO: successfully validated that service multi-endpoint-test in namespace services-1121 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-1121
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1121 to expose endpoints map[pod2:[101]]
Nov 25 08:11:52.179: INFO: successfully validated that service multi-endpoint-test in namespace services-1121 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1121
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1121 to expose endpoints map[]
Nov 25 08:11:52.275: INFO: successfully validated that service multi-endpoint-test in namespace services-1121 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:11:52.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1121" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:5.226 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":303,"completed":284,"skipped":4529,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:11:52.295: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:12:05.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4668" for this suite.

• [SLOW TEST:13.102 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":303,"completed":285,"skipped":4548,"failed":0}
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:12:05.397: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 25 08:12:09.447: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 25 08:12:09.450: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 25 08:12:11.450: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 25 08:12:11.453: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 25 08:12:13.450: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 25 08:12:13.452: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:12:13.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6641" for this suite.

• [SLOW TEST:8.061 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":303,"completed":286,"skipped":4550,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:12:13.459: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Nov 25 08:14:13.996: INFO: Successfully updated pod "var-expansion-6f2a54f8-2d02-464d-8177-b8e05de2adf8"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Nov 25 08:14:16.011: INFO: Deleting pod "var-expansion-6f2a54f8-2d02-464d-8177-b8e05de2adf8" in namespace "var-expansion-1510"
Nov 25 08:14:16.015: INFO: Wait up to 5m0s for pod "var-expansion-6f2a54f8-2d02-464d-8177-b8e05de2adf8" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:14:52.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1510" for this suite.

• [SLOW TEST:158.568 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":303,"completed":287,"skipped":4571,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:14:52.027: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov 25 08:14:52.053: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5091 /api/v1/namespaces/watch-5091/configmaps/e2e-watch-test-watch-closed de391a5c-d33b-4142-a04e-7d72e997d261 22895 0 2020-11-25 08:14:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-25 08:14:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 25 08:14:52.054: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5091 /api/v1/namespaces/watch-5091/configmaps/e2e-watch-test-watch-closed de391a5c-d33b-4142-a04e-7d72e997d261 22896 0 2020-11-25 08:14:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-25 08:14:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov 25 08:14:52.063: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5091 /api/v1/namespaces/watch-5091/configmaps/e2e-watch-test-watch-closed de391a5c-d33b-4142-a04e-7d72e997d261 22897 0 2020-11-25 08:14:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-25 08:14:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 25 08:14:52.063: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5091 /api/v1/namespaces/watch-5091/configmaps/e2e-watch-test-watch-closed de391a5c-d33b-4142-a04e-7d72e997d261 22898 0 2020-11-25 08:14:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-25 08:14:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:14:52.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5091" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":303,"completed":288,"skipped":4577,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:14:52.068: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Nov 25 08:14:54.097: INFO: &Pod{ObjectMeta:{send-events-7a821d7c-7f12-4433-9d99-c0d301f43e82  events-1882 /api/v1/namespaces/events-1882/pods/send-events-7a821d7c-7f12-4433-9d99-c0d301f43e82 aee1db7f-cacb-4938-bc99-781ccdfac2dc 22914 0 2020-11-25 08:14:52 +0000 UTC <nil> <nil> map[name:foo time:86439632] map[] [] []  [{e2e.test Update v1 2020-11-25 08:14:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 08:14:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q2jm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q2jm6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q2jm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 08:14:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 08:14:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 08:14:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 08:14:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.93,StartTime:2020-11-25 08:14:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-25 08:14:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://d1a4bacffad4543d3b8f7f0438aa1188f2408b36e88678efc4b6892ded3bc39e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Nov 25 08:14:56.101: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Nov 25 08:14:58.104: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:14:58.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1882" for this suite.

• [SLOW TEST:6.047 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":303,"completed":289,"skipped":4604,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:14:58.115: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 25 08:14:58.139: INFO: Waiting up to 5m0s for pod "pod-f23079e0-e497-4e69-9472-f364cde7ee4a" in namespace "emptydir-5321" to be "Succeeded or Failed"
Nov 25 08:14:58.141: INFO: Pod "pod-f23079e0-e497-4e69-9472-f364cde7ee4a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.599447ms
Nov 25 08:15:00.144: INFO: Pod "pod-f23079e0-e497-4e69-9472-f364cde7ee4a": Phase="Running", Reason="", readiness=true. Elapsed: 2.00443705s
Nov 25 08:15:02.146: INFO: Pod "pod-f23079e0-e497-4e69-9472-f364cde7ee4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007273568s
STEP: Saw pod success
Nov 25 08:15:02.146: INFO: Pod "pod-f23079e0-e497-4e69-9472-f364cde7ee4a" satisfied condition "Succeeded or Failed"
Nov 25 08:15:02.148: INFO: Trying to get logs from node k8sconformance-m02 pod pod-f23079e0-e497-4e69-9472-f364cde7ee4a container test-container: <nil>
STEP: delete the pod
Nov 25 08:15:02.212: INFO: Waiting for pod pod-f23079e0-e497-4e69-9472-f364cde7ee4a to disappear
Nov 25 08:15:02.214: INFO: Pod pod-f23079e0-e497-4e69-9472-f364cde7ee4a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:15:02.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5321" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":290,"skipped":4607,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:15:02.219: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 25 08:15:04.279: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:15:04.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7587" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":291,"skipped":4618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:15:04.298: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 25 08:15:04.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8798'
Nov 25 08:15:04.409: INFO: stderr: ""
Nov 25 08:15:04.409: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Nov 25 08:15:04.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 delete pods e2e-test-httpd-pod --namespace=kubectl-8798'
Nov 25 08:15:10.652: INFO: stderr: ""
Nov 25 08:15:10.652: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:15:10.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8798" for this suite.

• [SLOW TEST:6.361 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":303,"completed":292,"skipped":4666,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:15:10.660: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 08:15:10.718: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Nov 25 08:15:12.740: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:15:13.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1228" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":303,"completed":293,"skipped":4678,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:15:13.794: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 08:15:13.817: INFO: Waiting up to 5m0s for pod "busybox-user-65534-dff8c7fe-cf88-4f51-acc2-6ad8a4b0b11c" in namespace "security-context-test-517" to be "Succeeded or Failed"
Nov 25 08:15:13.821: INFO: Pod "busybox-user-65534-dff8c7fe-cf88-4f51-acc2-6ad8a4b0b11c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.785993ms
Nov 25 08:15:15.823: INFO: Pod "busybox-user-65534-dff8c7fe-cf88-4f51-acc2-6ad8a4b0b11c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005989551s
Nov 25 08:15:15.824: INFO: Pod "busybox-user-65534-dff8c7fe-cf88-4f51-acc2-6ad8a4b0b11c" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:15:15.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-517" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":294,"skipped":4685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:15:15.828: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-38d4fbe3-e500-4c6a-ab5c-88e90a5e8d44
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:15:17.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1828" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":295,"skipped":4718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:15:17.882: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:15:46.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6158" for this suite.
STEP: Destroying namespace "nsdeletetest-181" for this suite.
Nov 25 08:15:46.983: INFO: Namespace nsdeletetest-181 was already deleted
STEP: Destroying namespace "nsdeletetest-155" for this suite.

• [SLOW TEST:29.104 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":303,"completed":296,"skipped":4769,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:15:46.986: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9857
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Nov 25 08:15:47.013: INFO: Found 0 stateful pods, waiting for 3
Nov 25 08:15:57.016: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 25 08:15:57.016: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 25 08:15:57.016: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov 25 08:15:57.037: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov 25 08:16:07.102: INFO: Updating stateful set ss2
Nov 25 08:16:07.106: INFO: Waiting for Pod statefulset-9857/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Nov 25 08:16:17.167: INFO: Found 2 stateful pods, waiting for 3
Nov 25 08:16:27.170: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 25 08:16:27.170: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 25 08:16:27.170: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov 25 08:16:27.190: INFO: Updating stateful set ss2
Nov 25 08:16:27.203: INFO: Waiting for Pod statefulset-9857/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 25 08:16:37.208: INFO: Waiting for Pod statefulset-9857/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 25 08:16:47.224: INFO: Updating stateful set ss2
Nov 25 08:16:47.228: INFO: Waiting for StatefulSet statefulset-9857/ss2 to complete update
Nov 25 08:16:47.228: INFO: Waiting for Pod statefulset-9857/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 25 08:16:57.233: INFO: Waiting for StatefulSet statefulset-9857/ss2 to complete update
Nov 25 08:16:57.233: INFO: Waiting for Pod statefulset-9857/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 25 08:17:07.233: INFO: Deleting all statefulset in ns statefulset-9857
Nov 25 08:17:07.234: INFO: Scaling statefulset ss2 to 0
Nov 25 08:17:47.248: INFO: Waiting for statefulset status.replicas updated to 0
Nov 25 08:17:47.250: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:17:47.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9857" for this suite.

• [SLOW TEST:120.275 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":303,"completed":297,"skipped":4781,"failed":0}
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:17:47.262: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 25 08:17:47.313: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 25 08:17:47.319: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 25 08:17:52.324: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 25 08:17:52.324: INFO: Creating deployment "test-rolling-update-deployment"
Nov 25 08:17:52.327: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 25 08:17:52.333: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 25 08:17:54.338: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 25 08:17:54.340: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 25 08:17:54.345: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5346 /apis/apps/v1/namespaces/deployment-5346/deployments/test-rolling-update-deployment 700d1e0b-4635-4fe0-ad34-8ababc9b0003 23789 1 2020-11-25 08:17:52 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-11-25 08:17:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-25 08:17:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007487998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-25 08:17:52 +0000 UTC,LastTransitionTime:2020-11-25 08:17:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2020-11-25 08:17:53 +0000 UTC,LastTransitionTime:2020-11-25 08:17:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 25 08:17:54.347: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-5346 /apis/apps/v1/namespaces/deployment-5346/replicasets/test-rolling-update-deployment-c4cb8d6d9 b0f64349-1408-4ba3-8eda-67bbdd5e4a67 23778 1 2020-11-25 08:17:52 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 700d1e0b-4635-4fe0-ad34-8ababc9b0003 0xc007487f00 0xc007487f01}] []  [{kube-controller-manager Update apps/v1 2020-11-25 08:17:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"700d1e0b-4635-4fe0-ad34-8ababc9b0003\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007487f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 25 08:17:54.347: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 25 08:17:54.347: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5346 /apis/apps/v1/namespaces/deployment-5346/replicasets/test-rolling-update-controller 22f2cb54-52f5-4452-afda-b5e077849741 23788 2 2020-11-25 08:17:47 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 700d1e0b-4635-4fe0-ad34-8ababc9b0003 0xc007487df7 0xc007487df8}] []  [{e2e.test Update apps/v1 2020-11-25 08:17:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-25 08:17:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"700d1e0b-4635-4fe0-ad34-8ababc9b0003\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007487e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 25 08:17:54.349: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-l5tvb" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-l5tvb test-rolling-update-deployment-c4cb8d6d9- deployment-5346 /api/v1/namespaces/deployment-5346/pods/test-rolling-update-deployment-c4cb8d6d9-l5tvb d4508bd8-1900-454e-b3fa-200453180b59 23777 0 2020-11-25 08:17:52 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 b0f64349-1408-4ba3-8eda-67bbdd5e4a67 0xc004bc6440 0xc004bc6441}] []  [{kube-controller-manager Update v1 2020-11-25 08:17:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0f64349-1408-4ba3-8eda-67bbdd5e4a67\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-25 08:17:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-szpd4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-szpd4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-szpd4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 08:17:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 08:17:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 08:17:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-25 08:17:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.108,StartTime:2020-11-25 08:17:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-25 08:17:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://ba77a4da270474f4ff56cd0ec385d29c5b5f835e7ba1a2837b2e3b4fbdc04fd3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:17:54.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5346" for this suite.

• [SLOW TEST:7.093 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":303,"completed":298,"skipped":4781,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:17:54.354: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 25 08:17:54.379: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07460852-af1f-4e7c-ac48-213b66b46683" in namespace "downward-api-46" to be "Succeeded or Failed"
Nov 25 08:17:54.383: INFO: Pod "downwardapi-volume-07460852-af1f-4e7c-ac48-213b66b46683": Phase="Pending", Reason="", readiness=false. Elapsed: 3.469944ms
Nov 25 08:17:56.386: INFO: Pod "downwardapi-volume-07460852-af1f-4e7c-ac48-213b66b46683": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006374229s
STEP: Saw pod success
Nov 25 08:17:56.386: INFO: Pod "downwardapi-volume-07460852-af1f-4e7c-ac48-213b66b46683" satisfied condition "Succeeded or Failed"
Nov 25 08:17:56.388: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-07460852-af1f-4e7c-ac48-213b66b46683 container client-container: <nil>
STEP: delete the pod
Nov 25 08:17:56.437: INFO: Waiting for pod downwardapi-volume-07460852-af1f-4e7c-ac48-213b66b46683 to disappear
Nov 25 08:17:56.439: INFO: Pod downwardapi-volume-07460852-af1f-4e7c-ac48-213b66b46683 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:17:56.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-46" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":303,"completed":299,"skipped":4807,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:17:56.444: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:18:12.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-236" for this suite.

• [SLOW TEST:16.077 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":303,"completed":300,"skipped":4850,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:18:12.521: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Nov 25 08:18:12.573: INFO: created test-pod-1
Nov 25 08:18:12.575: INFO: created test-pod-2
Nov 25 08:18:12.580: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:18:12.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4494" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":303,"completed":301,"skipped":4855,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:18:12.621: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Nov 25 08:18:12.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-894937859 cluster-info'
Nov 25 08:18:12.710: INFO: stderr: ""
Nov 25 08:18:12.710: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:18:12.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-455" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":303,"completed":302,"skipped":4874,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 25 08:18:12.715: INFO: >>> kubeConfig: /tmp/kubeconfig-894937859
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 25 08:18:13.270: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 25 08:18:16.319: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 25 08:18:16.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5900" for this suite.
STEP: Destroying namespace "webhook-5900-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":303,"completed":303,"skipped":4918,"failed":0}
SSSSSSSSSSSSSNov 25 08:18:16.425: INFO: Running AfterSuite actions on all nodes
Nov 25 08:18:16.425: INFO: Running AfterSuite actions on node 1
Nov 25 08:18:16.425: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":303,"completed":303,"skipped":4931,"failed":0}

Ran 303 of 5234 Specs in 5571.927 seconds
SUCCESS! -- 303 Passed | 0 Failed | 0 Pending | 4931 Skipped
PASS

Ginkgo ran 1 suite in 1h32m53.874343862s
Test Suite Passed
